\documentclass{homework}
\author{Tomás Pérez}
\class{Advanced Linear Methods - Lecture Notes}
\date{\today}
\title{Theory \& Notes}

\graphicspath{{./media/}}

\begin{document} \maketitle

\section{Analysis of Variance and Regression}

In a wide variety of settings, it's useful to model a random variable with a pdf or a pmf which depends on some parameters to be estimated. In many situations, a random variable can be modeled not only with unknown parameters but also with known, an sometimes controllable covariates. The standard methodology for analysis of variance (ANOVA)and regression analysis rely on a linear relationship among random variables. A basic idea of ANOVA consists on partitioning variations and analysing variations in means. \\

\subsection{\textbf{Simple Linear Regression:}}

A major purpose of regression is to explore the dependence of one variable on others. In simple linear regression, the mean of a random variable, $\Y$, is modelled as a function of another observable variable $\X$, by the relationship $\E[\Y] = \alpha + \beta \X$. In general, the function that gives $\E[\Y]$ as a function of $\X$ is called the \textit{population regression function}. \\

\subsection{\textbf{Oneway Analysis of Variance:}}

In its simplest form, ANOVA is a method of estimating the means of several populations, which are often assumed to be normally distributed. The heart of ANOVA lies on the topic of statistical design. How can we get the most information on the populations with the fewest observations? Classic ANOVA had testing as its main goal, in particular testing what is known as the ANOVA null hypothesis. However, in many settings it's not sufficient to test only one hypothesis in order to make good experimental inference. \\

In the oneway analysis of variance, also known as oneway classification, we assume that the data $Y_{ij}$ are observed according to a model:

$$
Y_{ij} = \theta_i + \epsilon_{ij} \textnormal{ where } i = 1, \cdots, k \textnormal{, } j = 1, \cdots, n_i,
$$

where the $\theta_i$ are unknown parameters and where the $\epsilon_{ij}$ are error random variables. Without loss of generality, we can assume $\E \epsilon_{ij} = 0$, since if not, we can re-scale the $\epsilon_{ij}$ and absorb the leftover mean into the $\theta_i$'s. Then 

$$
\E[Y_{ij}] = \theta_i \textnormal{ where } j = 1, \cdots, n_i.
$$

Therefore the $\{\theta_i\}_{i=1}^{k}$ are the means of the $\{Y_{ij}\}_{i=1\textnormal{, } j=1}^{k\textnormal{, } n_i}$. The first ones are the \textit{treatment means} (since the index usually correspond to different treatments or to levels of a particular treatment, such as dossage levels of a particular drug)\footnote{There is an alternative model, called the \textit{overparameterized model}, which can be written as 

\begin{equation}
Y_{ij} = \mu + \tau_i + \epsilon_{ij} \textnormal{ where } i = 1, \cdots, k \textnormal{, } j = 1, \cdots, n_i,
\label{std linear mod}
\end{equation}

where, again, $\E \epsilon_{ij} = 0$. In this setting, then 

$$
\E[Y_{ij}] = \mu+\theta_i \textnormal{ where } j = 1, \cdots, n_i.
$$

In this formulation, we think of $\mu$ as a grand mean, that is the common mean level of the treatments. The parameters $\tau_i$ then denote the unique effect due to the $i$-th treatment. However, the $\tau_i$ and $\mu$ parameters cannot be estimated independently due to problems with \textit{identifiability}. 
}. \\

\begin{df}

A parameter $\theta$ for a family of distributions $\{f(x|\theta): \theta \in \Theta\}$ is \textbf{identifiable} if distinct values of $\theta$ correspond to distinct pdfs or pmfs. This is, if $\theta_1 \neq \theta_2\comma f(x|\theta_1)  \neq f(x|\theta_2) \blank \forall x$. \\

\end{df}

Identifiability is a property of the model, not of an estimator or estimation procedure. If the model is not identifiable, there is difficulty in doing inference, eg. if $f(x|\theta_1) = f(x|\theta_2)$, then observations from both distributions will look exactly the same and we would have no way of knowing whichever value was correct. In particular, the likelihood of both values would be equal. Problems con identifiability can usually be solved by redefining the model. In a general overparameterized model, there are $k+1$ values parameters $(\mu, \tau_{1}, \cdots, \tau_k) \in \bm{\Theta} = \Theta_{\mu} \times \bigtimes\limits_{i=1}^k \Theta_{\tau_{i}}$ but only $k$ means, $\{\E[Y_{ij}]\}_{i=1}^{k}$. Without any further restriction on the parameters, more than one set of values $(\mu, \tau_{1}, \cdots, \tau_k)$ will lead to the same distribution. Therefore, it's common in this model to add the restriction that $\sum_{i=1}^{k} \tau_k = 0$, which effectively reduces the number of parameters to $k$ and makes the model identifiable. The restriction also has the effect of giving the $\{\tau_i\}_{i=1}^k$ an interpretation as deviations from an overall mean level. In oneway ANOVA, it's more common to use the \textit{cell means model}, given by \eqref{std linear mod}. \\

\paragraph{\textbf{Model and distribution assumptions}}

Under model \eqref{std linear mod}, the minimum assumptions are

\begin{itemize}
    \item $\E (\epsilon_{ij}) = 0$
    \item $\var (\epsilon_{ij}) < \infty$, for all $i,j$. 
\end{itemize}

However to do any confidence interval estimation or testing, we need distributional assumptions. Here are the classical ANOVA assumptions: \\

\paragraph{\textbf{Oneway ANOVA assumptions}}

Random variables $Y_{ij}$ are observed according to model \eqref{std linear mod}, where

\begin{itemize}
    \item $\E (\epsilon_{ij}) = 0$, $\var (\epsilon_{ij}) < \infty$, for all $i,j$ and $\cov(\epsilon_{ij}, \epsilon_{mn}) \propto \delta_{im}\delta_{jn}$. 
    \item The $\epsilon_{ij}$ are independent and normally distributed.
    \item $\sigma_i^2 = \sigma^2$ for all $i$, this is called \textit{homoscedasticity}. \\
\end{itemize}

Note that without assumption (II) we could only do point estimation and possibly look estimators that minimize variance within a class, but we could not do interval estimation nor testing. Instead, if we assume some other error distribution other than the normal, intervals and tests can be quite difficult to derive. The homoscedastic condition is also quite important. \\

\paragraph{\textbf{Classic ANOVA Hypothesis}:}

The classic ANOVA test is a test of the null hypothesis

$$
H_0 : \theta_1 = \theta_2 = \cdots = \theta_k,
$$

a hypothesis which may be uninteresting in many settings. The alternative hypothesis is 

$$
H_1 : \theta_i \neq \theta_j \textnormal{ for some } i,j .
$$

Equivalently, we can specify $H_1$ as $H_1: \textnormal{ not } H_0$. Either way, note that if $H_0$ is rejected, we can conclude only that there is some difference amongst the $\theta_i$s, but we cannot infer as to where the difference might be. One problem with ANOVA hypothesis is that the interpretation of hypothesis is not easy. What would be more useful than concluding that some $\theta_i$s are different is a statistical description of the  $\theta_i$s. Such a description can be readily found by breaking down the ANOVA hypothesis into smaller, more easily describable pieces. One way to implement this is by using an intersection-union test\footnote{Let

$$
\prob_{\theta}({\bf X} \in R) = \left\{ \begin{array}{cc}
    \textnormal{probability of a Type I error } & \textnormal{ if } \theta \in \Theta_0 \\
    \textnormal{one minus the probability of a Type II error } & \textnormal{ if } \theta \in \Theta_0^c  
\end{array} \right.
$$

The previous function is none other than the power function of a hypothesis test with rejection region $R$. Consider the following theorem: 

\begin{theo}
Consider testing $H_0: \theta \in \cup_{j=1}^k \Theta_j$. For each $j = 1, \cdots, k$, let $R_j$ be the rejection region of an $\alpha$-level test of $H_{0j}$. Suppose that for some $i = 1, \cdots, k$, there exists a sequence of parameters points, $\theta_l \in \Theta_i$, $l = 1,2, \cdots$, such that 

\begin{itemize}
    \item $\lim_{l \rightarrow \infty} \prob_{\theta_l}(\X \in R_i) = \alpha$,
    \item for each $j=1, \cdots, k$ and $j \neq i, \lim_{l \in \infty} \prob_{\theta_l}(\X \in R_i) = 1$.
\end{itemize}

Then the intersection-union test with a rejection region $R = \cap_{j=1}^k R_j$ is a size $\alpha$-test. 
\end{theo}

\begin{proof}
Let $\alpha_{\gamma}$ be the size of the $\gamma$-th null hypothesis, $H_{0\gamma}$ with rejection region $R_\gamma$. Then the intersection-union test with rejection region $R = \cap_{\gamma \in \Gamma}$ has a $\Tilde{\alpha}$-level, with $\Tilde{\alpha} = \sup_{\gamma \in \Gamma} \alpha_\gamma$. \\

In effect, let $\theta \in \Theta_0$ be arbitrary. Then, $\exists \gamma \in \Gamma \blank | \blank \theta \in \Theta_\gamma $ such that 

$$
\prob_{\theta}({\bf X} \in R) \leq P_{\theta}({\bf X} \in R_{\gamma}) \leq \alpha_{\gamma} \leq \alpha,
$$

thus the IUT is an $\alpha$-level test. 

Then, in our context, R is an $\alpha$-level test. By definition, this implies 

$$
\sup_{\theta \in \Theta_0} \prob_{\theta} (\X \in R) \leq \alpha.
$$

But, because all parameter points $\theta_l$ satisfy $\theta_l \in \Theta_i \subset \Theta_0$, then

\begin{align*}
    \sup_{\theta \in \Theta_0} \prob_{\theta} (\X \in R) &\geq \lim_{l \rightarrow \infty} \prob_{\theta_l}(\X \in R) \\
    & = \lim_{l \rightarrow \infty} \prob_{\theta_l}\bigg(\X \in \cap_{j=1}^k R_j\bigg) \\
    &\geq \lim_{l \rightarrow \infty}
    \sum_{j=1}^{k}  \prob_{\theta_l}(\X \in R_j) - (k-1) \textnormal{ according to Bonferroni's inequality.} \\
    &= (k-1)  + \alpha - (k-1) = \alpha
\end{align*}
\end{proof}}, which is the best suited method as the ANOVA null can be thought as the inteersection of univariate hypotheses, expressed in terms of \textit{contrasts}. 

\begin{df}

Let ${\bf t} =\{t_i\}_{i=1}^k$ be a set of variables, either parameters or statistics, and let ${\bf a} =\{a_i\}_{i=1}^k$ be known constants. Then the function 

$$
\sum_{i=1}^k a_i t_i 
$$

is a linear combination of them. If, furthermore, $\sum_{i} a_i = 0$, it is called a \textit{contrast}. \\
\end{df}

Constrasts are important because they can be used to compare treatment means, eg, if we have means $\{\theta_i\}_{i=1}^k$ and constants ${\bf a} = (1, -1, 0, \cdots, 0)$, then 

$$
\sum_{i=1}^{k} a_i \theta_i = \theta_1 - \theta_2,
$$

which is a contrast that compares $\theta_1$ to $\theta_2$. The power of the union approach relies on its increased understanding. The individual null hypotheses, of which the ANOVA null hypothesis is the intersection, are readily seen. 

\begin{theo}
Let $\theta = \{\theta_i\}_{i=1}^{k}$ be arbitrary parameters. Then, 

$$
\theta_1 = \theta_2 = \cdots = \theta_k \Leftrightarrow \sum_{i=1}^{k} a_i \theta_i = 0 \blank \forall {\bf a} \in \mathcal{A},
$$

where $\mathcal{A}$ is the set of constants satisfying $\mathcal{A} = \{{\bf a} = \{a_i\}_{i=1}^{k} | \sum_{i} a_i = 0\}$. \\
\end{theo}

\begin{proof}
If $\theta_1 = \theta_2 = \cdots = \theta_k$ then 

$$
\sum_{i=1}^{k} a_i \theta_i  = \sum_{i=1}^{k} a_i \theta = \theta \sum_{i=1}^{k} a_i = 0,
$$

since $\sum_{i} a_i = 0$, which proves the left-to-right implication. To prove the right-to-left implication, consider the following basis for $\mathcal{A}$

$$
\mathcal{B}(\mathcal{A}) = \{a_i\}_{i=1}^{k-1} \blank | \blank (a_i)_j = \left\{ \begin{array}{cc}
    1 & \textnormal{ if } j = i  \\
    -1 & \textnormal{ if } j = i+1 \\
    0 & \textnormal{ otherwise}.
\end{array} \right. 
$$

Since it's a basis, we can form contrasts with these ${\bf a_i}$s: 

$$
a_1 \Rightarrow \theta_1 = \theta_2 \land a_2 \Rightarrow \theta_2 = \theta_3, \cdots a_{k-1} \Rightarrow \theta_{k-1} = \theta_k
$$

which, taken together, prove the theorem. 
\end{proof}

Therefore, the ANOVA null can then be expressed as a hypothesis about contrasts. That is, the null hypothesis is true iff the hypothesis 

$$
H_0: \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ for all } {\bf a} \textnormal{ such that } \sum_{i=1}^{k} a_i = 0,
$$

is true. Moreover, if $H_0$ is false, we now know that there must be at least one nonzero contrasts. That is, the ANOVA alternative, $H_1$: not all $\theta_i$s equal, is equivalent to the alternative 

$$
H_1: \blank \sum_{i=1}^{k} a_i \theta_i \neq 0 \textnormal{ for all } {\bf a} \textnormal{ such that } \sum_{i=1}^{k} a_i = 0. \\
$$

\blank \\

\paragraph{\textbf{Inferences regarding Linear combinations of Means}}

Linear combinations, in particular of contrasts, play an extremely important role in the analysis of variance. In the previous section, we proved that the ANOVA null can be decoupled into statements about constrasts. This is also true in general, more elaborate statistical hypothesis tests. We start first by discussing a single linear combination. \\

Working under oneway ANOVA assumptions, we have that

$$
Y_{ij} \sim \N(\theta_i, \sigma^2), \begin{array}{c}
     i = 1, \cdots, k  \\
     j = 1, \cdots, n_i 
\end{array},
$$

therefore 

$$
\Bar{Y}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} Y_{ij} \sim \N\bigg(\theta_i, \frac{\sigma^2}{n_i}\bigg), \blank i = 1, \cdots, k.
$$

For any constants ${\bf a} \in \R^{k}$, we have ${\bf a}^{\textnormal{T}} \bar{{\bf Y}}$ is also normal with 

\begin{align*}
    \E\bigg({\bf a}^{\textnormal{T}} \bar{{\bf Y}}\bigg) &= \sum_{i=1}^{k} a_i \theta_i, & \var \bigg({\bf a}^{\textnormal{T}} \bar{{\bf Y}}\bigg) &= \sigma^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}. 
\end{align*}

Furthermore, 

$$
\frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{\sigma^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \sim \N(0, 1).
$$

In practice, we'd like to make inferences about the $\theta_i$s without knowledge of $\sigma$. Therefore, we need to replace $\sigma$ with an estimate, the sample variance $S_i^2$ of the $i-$th population, defined as follows,

$$
S_i^2 = \frac{1}{n_i - 1} \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2\textnormal{, } i=1, \cdots,k,  
$$

which is an estimate for $\sigma^2$ with a \chidis, $\frac{(n_i-1)S^2_i}{\sigma^2} \sim \chi^2_{n_i-1}$. Furthermore, under the ANOVA assumptions, since each one of the $S_i^2$ estimates the same $\sigma^2$, we can improve the estimate by combining them into a \textit{pooled variance estimator} $\hat{S}^2_p$, of $\sigma^2$, defined as

$$
\hat{S}^2_p = \frac{1}{N-k} \sum_{i=1}^{k} (n_i-1)S_i^2 = \frac{1}{N-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y}_i)^2.
$$

Note that $N-k =\sum_{i} (n_i - 1)$. Now, since the $S_i^2$ are independent, then $\frac{(N-k)S^2_p}{\sigma^2} \sim \chi^2_{N-k}$. Also, $S_p^2$ is independent of each $\bar{Y}_i$, then

$$
\frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \sim t_{N-k},
$$

a Student's \tdis with $N-k$ degrees of freedom. \\

Now, consider the null hypothesis, which we'd like to test

$$
H_0 : \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ versus } H_1 : \blank \sum_{i=1}^{k} a_i \theta_i \neq 0,
$$

at $\alpha$-level, therefore the rejection region $R$ for the null hypothesis is 

$$
\bigg|\frac{\sum_{i=1}^{k} a_i \bar{Y}_i}{{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}}}\bigg| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

Furthermore, this can be inverted by the means of a pivot, the $\frac{\alpha}{2}-$th level quantile of the \tdis with $N-k$ degrees of freedom. With probability $1-\alpha$, we can assert 
\begin{align*}
    \sum_{i=1}^{k} a_i \bar{Y}_i - t_{N-k, \blank \frac{\alpha}{2}} \sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}} &\leq \sum_{i=1}^{k} a_i \theta_i \\
    &\leq \sum_{i=1}^{k} a_i \bar{Y}_i + t_{N-k, \blank \frac{\alpha}{2}} \sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}.
\end{align*}

\begin{tcolorbox}[title=Example of ANOVA contrasts]

Special values of ${\bf a}$ will yield particular tests or confidence intervals. For example, to compare treatment 1 and treatment 2 in a clinical exam trial, we can take ${\bf a} = (1, -1, 0, \cdots, 0)$. Then, using the previous results, to test $H_0: \blank \theta_1=\theta_2$ versus $H_1: \blank \theta_1 \neq \theta_2$, we would reject if 

$$
\left|{\frac{\bar{Y}_1 - \bar{Y}_2}{{\sqrt{S_p^2 \bigg(\frac{1}{n_1}+\frac{1}{n_2}\bigg)}}}}\right| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

Note that the difference between this test and a two-sample $t$-test is that here information from treatments $3,\cdots,k$, as well as treatments 1 and 2, is used to estimate $\sigma^2$. \\

Alternatively, to compare treatment 1 to the average of treatments $2$ and 3 (eg. treatment 1 might be a control, 2 and 3 might be experimental treatments, and we are looking for some overall effect), we would take ${\bf a} = (1, -\frac{1}{2}, -\frac{1}{2}, 0, \cdots, 0)$ and reject $H_0: \blank \theta_1 = \frac{1}{2}(\theta_2+\theta_3)$ if 

$$
\left|{\frac{\bar{Y}_1 - \frac{1}{2} \bar{Y}_2 - \frac{1}{2} \bar{Y}_3}{{\sqrt{S_p^2 \bigg(\frac{1}{n_1}+\frac{1}{4n_2}+\frac{1}{4n_3}\bigg)}}}}\right| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

\blank \\

In general, we have a framework for testing and estimating linear combinations of random variables in the ANOVA. By judiciously choosing our linear combination we can learn much about the treatment means. For example, if we look at the contrasts $\theta_1-\theta_2$, $\theta_2-\theta_3$ and $\theta_1-\theta_3$, we can learn something about the ordering of the $\theta_i$s. Some care must be taken when drawing formal conclusions from combinations of contrasts. For example, consider the hypotheses 

$$
H_0: \blank \theta_{1} = \frac{1}{2}(\theta_{2}+\theta_{3}) \textnormal{ versus } H_0: \blank \theta_{1} < \frac{1}{2}(\theta_{2}+\theta_{3}),
$$

and 

$$
H_0 : \blank \theta_2 = \theta_3 \textnormal{ versus } H_0 : \blank \theta_2 < \theta_3.
$$

If we reject both null hypotheses, we can conclude $\theta_3$ is greater than both $\theta_1$ and $\theta_2$, although we can draw no formal conclusion about the ordering of $\theta_1$ and $\theta_2$ from these two tests. 
\end{tcolorbox}

\blank \\

\paragraph{\textbf{The ANOVA \textit{F}-test}}

In the previous section we saw how to deal with single linear combinations of random variables, an in particular, contrasts in the ANOVA, and how the ANOVA null hypothesis is equivalent to a set of hypotheses about contrasts. Using this equivalence, together with the union-intersection method, it's possible to derive a test of the ANOVA hypothesis. \\

The ANOVA hypothesis test can be written as 

\begin{align*}
    H_0: \blank \forall {\bf a} \in \mathcal{A}, \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ versus } H_1: \blank \exists {\bf a} \in \mathcal{A}, \blank \sum_{i=1}^{k} a_i \theta_i \neq 0,
\end{align*}

where $\mathcal{A} = \{{\bf a} \in \R^{k} \blank | \blank \sum_{i=1}^{k} a_i = 0\}$. To see this more clearly as a union-intersection test, let's define the set $\Theta_a$ for each ${\bf a}$ as 

$$
\Theta_{\bf a} = \{\theta=(\theta_1, \cdots, \theta_k) \blank | \blank \sum_{i=1}^{k}a_i \theta_i = 0\},
$$

then we have 

\begin{align*}
    \theta \in \{\theta: \blank \theta_1 = \theta_2 = \cdots =  \theta_k \} \Leftrightarrow \theta \in \Theta_{\bf a} \blank \forall {\bf a} \in \mathcal{A} \Leftrightarrow \theta \in \cap_{{\bf a} \in \mathcal{A}} \Theta_{\bf a},
\end{align*}

showing that the ANOVA null can be written as an intersection. Now, recalling the union-intersection methodology, we will reject $H_0 : \theta \in \cap_{{\bf a} \in \mathcal{A}} \Theta_{\bf a}$ (and, hence the ANOVA null) if we can reject 

$$
H_{0_{\bf a}} : \theta \in \Theta_{\bf a} \textnormal{ versus } H_{1_{\bf a}} : \theta \notin \Theta_{\bf a}, \blank  \forall a \in \mathcal{A}.
$$

We test $H_{0_{\bf a}}$ with a $t$-statistic, 

$$
T_{\bf a} = \left| \frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \right|,
$$

therefore the rejection region is $T_{\bf a} > k$, in which case we'll reject $H_{\bf a}$, for some constant $k \in \R$. From the union-intersection methodology, it follows that if we could reject for any ${\bf a}$, we could reject for the ${\bf a}$ that maximizes $T_{\bf a}$. Thus, the union-intersection test of the ANOVA null is to reject $H_0$ if $\sup_{\bf a} T_{\bf a} > k$, where $k$ is chosen so that $P_{H_0} (\sup_{\bf a} T_{\bf a} > k) = \alpha$. It turns out that the calculation of $\sup_{\bf a} T_{\bf a} $ is not all straightforward. \\

\clearpage

\section{Simple Linear Regression}

In ANOVA, we studied how one factor, a random variable, influenced the means of a response variable. We now turn to understand the functional dependence of one variable on another. In particular, in the case of linear regression, we have a relationship of the form

\begin{equation}
{\bf Y}_i = \alpha + \beta {\bf X}_i + \epsilon_i,
\label{linear regression 1}
\end{equation}

where 

\begin{itemize}
    \item both ${\bf X}_i$ and ${\bf Y}_i$ are random variables, we call ${\bf X}_i$ as the \textit{predictor} variable while ${\bf Y}_i$ is the \textit{response} variable, 
    \item $\alpha$, $\beta$ are the intercept and the slope of the regression, and are assumed to be fixed and unknown parameters, 
    \item and where $\epsilon_i$ is a random variable, which can be assume to have $\E(\epsilon_i) = 0$. \\
\end{itemize}

Due to the imposed condition on the errors, we have 

\begin{equation}
\E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta x_i,
\label{linear regression 2}
\end{equation}

which is called the \textit{population regression function}. The main purpose of regression is to predict ${\bf Y}_i$ from knowledge of ${\bf X}_i$, using the population regression function. In this context, "regression" refers to a conditional expectation of ${\bf Y}$ given ${\bf X} = x$ and is used, in a statistical context, so signify a relationship between variables. When referring to a "linear regression", this implies that the conditional expectation of ${\bf Y}$ given ${\bf X}=x$ is a linear function of $x$. Note that in \eqref{linear regression 2}, it doesn't matter whether $x_i$ is fixed and known or it is a realization of the observable random variable ${\bf X}_i$, since in either case \eqref{linear regression 2} has the same interpretation. This isn't the case when the concern is to make inferences using the joint distribution of ${\bf X}_i$ and ${\bf Y}_i$. \\

The term \textit{linear regression} refers to a specification that is \textbf{linear in the parameters} ie. the specifications 

\begin{align*}
    \E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta x_i^2 \textnormal{ and } \E (\log {\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta \frac{1}{x_i},
\end{align*}

both specify linear regressions. The first one specifices a linear relationship between ${\bf Y}_i$ and $x_i^2$, whilst the second one between $\log{\bf Y}_i$ and $\frac{1}{x_i}$. In contrast, the specification $\E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta^2 x_i$ is not a linear regression. \\

In a simple linear regression problem, we observe data consisting of $n$ pairs of observation $\{(x_i, y_i)\}_{i=1}^n$, where there may be different underlying models for these data. The different models entail different assumptions about whether $x$ or $y$ or both are observed values of random variables ${\bf X}$ and ${\bf Y}$, respectively. In each model we'll be interested in investigating a linear relationship between $x$ and $y$. These $n$ data points will not fall exactly on a straight line but, rather, we're interested in summarizing the sample information by fitting a line to the observed data points. Given a data set $\{(x_i, y_i)\}_{i=1}^n$, we can define the following quantities of interest

\begin{align}
    \textnormal{The sample means } \begin{array}{c}
        \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i    \\
        \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i,   
    \end{array} & \textnormal{ and the sums of squares} \begin{array}{c}
       S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2  \\
       S_{yy} = \sum_{i=1}^{n} (y_i - \bar{y})^2   \\
       S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
    \end{array}.
\end{align}

Then the most common estimates of $\alpha$ and $\beta$ are 

\begin{align*}
    \hat{\beta} = \frac{S_{xy}}{S_{xx}} \textnormal{ and } \hat{\alpha} = \bar{y} - \hat{\beta} \bar{x}.
\end{align*}

\subsection{Least squares: A mathematical solution}

Our first derivation of estimates for $\alpha$ and $\beta$ makes no statistical assumptions about the observations  $\{(x_i, y_i)\}_{i=1}^n$. Simply consider a scatterplot of the dataset, for any line $y=c+dx$, the \textbf{residual sum of squares} (RSS) is defined to be 

\begin{equation}
    \textnormal{RSS } = \sum_{i=1}^{n} \left(y_i - (c+dx_i)^2\right), 
\end{equation}

which measures the vertical distance from each data point to the line $c+dx$, then summing the squares of these distances. The \textbf{least squares estimates} of $\alpha$ and $\beta$, 
$\hat{\alpha}$ and $\hat{\beta}$, are defined to be those values such that the line $\hat{\alpha} + \hat{\beta} x$ minimizes the RSS. That is, the least squares estimates satisfy

\begin{equation}
    \min_{c, \blank d \blank \in \R} \sum_{i=1}^{n} (y_i - (c+dx_i))^2 = \sum_{i=1}^{n} (y_i - (\hat{\alpha}+\hat{\beta}x_i))^2.
\end{equation}

The analytic minimization can be done in the following way. For any fixed value $d \in \R$, the value of $c$ which gives the minimum value can be found by writing 

$$
\sum_{i=1}^{n} (y_i - (c+dx_i))^2 = \sum_{i=1}^{n} ((y_i - dx_i) -c)^2.
$$

Then the minimizing value of $c$ is 

\begin{equation}
    c = \frac{1}{n} \sum_{i=1}^{n} (y_i - dx_i) = \bar{y} - d \bar{x}.
    \label{least squares 2}
\end{equation}

Thus, for a given value of $d$, the minimum value of RSS is 

\begin{equation}
    \sum_{i=1}^{n} \bigg((y_i - dx_i) - (\bar{y} - d \bar{x})\bigg)^2 = \sum_{i=1}^{n} \bigg((y_i - \bar{y}) - d(x_i - \bar{x})\bigg)^2 = S_{yy} - 2dS_{xy} + d^2 S_{xx}.
\end{equation}

The value of $d$ which gives the overall minimum value of RSS is obtained by setting the derivative of this quadratic function of $d$ equal to $0$. The minimizing value is then 

\begin{align}
    \frac{d}{d d}  S_{yy} - 2dS_{xy} + d^2 S_{xx} &= 0 \Rightarrow -2 S_{xy} + 2d S_{xx} = 0 \\
    \Rightarrow d = \frac{S_{xy}}{S_{xx}}.
    \label{least squares 1}
\end{align}

This value is, indeed, a minimum since the coefficient of $d^2$ is positive. Therefore, \eqref{least squares 2} and \eqref{least squares 1}, are indeed the values of $c$ and $d$ that minimize the residual sum of the squares. \\

The RSS is only one of the many reasonable ways of measuring the distance from the line $c+dx$ to the data points. For example, rather than using vertical distances we could use horizontal distances. This is equivalent to graphing the $y$ variable on the horizontal axis and the $x$ variabble on the vertical axis and using vertical distances as we did above. Using \eqref{least squares 2} and \eqref{least squares 1}, interchanging the roles of $x$ and $y$, we find the least squares lines is $\hat{x} = \hat{\alpha}' + \hat{\beta}'y$ where 

\begin{align*}
   \hat{\beta}' = \frac{S_{xy}}{S_{yy}} \textnormal{ and } \hat{\alpha}' = \bar{x} - \hat{\beta}'\bar{y},
\end{align*}

so that when reexpressing the line so that $y$ is a function of $x$, we obtain $\hat{y} = -\frac{\hat{\alpha}'}{\hat{\beta}'} + \frac{1}{\hat{\beta}' x}$. \\

It should be noted that least squares, as a method, is not statistical inference, since there's no way to construct confidence intervals nor testing hypothesis. In this context, the estimates are simply solutions of a minimization problem. It will turn out that, in some statistical models, these solutions have optimality properties. \\

\subsection{Best Linear Unbiased Estimators: A statistical solution}

In this section, we'll show that the estimates obtained in \eqref{least squares 2} and \eqref{least squares 1}, are optimal in the class of linear unbiased estimates under a general statistical model. Assume that the values $\{x_i\}_{i=1}^{n}$ are known, fixed values. The values $\{y_i\}_{i=1}^{n}$ are observed values of uncorrelated random variables $\{{\bf Y}_i\}_{i=1}^{n}$. The linear relationship assumed is given by \eqref{linear regression 2}, where we'll also asume $\var({\bf Y}_i) = \sigma^2, \blank \forall i \in \mathds{N}_{[1,n]}$. This is called the homoscedastic condition. The previous two assumptions are the only requirements for our model to work, eg. we need not any assumption on the probability distribution on the response variables. \\

To derive estimators for the parameters $\alpha$ and $\beta$, we restrict our attention to the class of linear estimators, ie. estimators which can be written as 

$$
\exists \{d_i\}_{i=1}^{n} \subset \mathds{R} \blank | \blank \sum_{i=1}^{n} d_i {\bf Y}_i.
$$

In particular, we'll restrict ourselves only to a subclass of linear estimators, the unbiased linear estimators. \\

An unbiased estimator of the slope $\beta$ must satisfy then 

$$
\exists \{d_i\}_{i=1}^{n} \subset \mathds{R} \blank | \blank  \E \bigg[\sum_{i=1}^{n} d_i {\bf Y}_i\bigg] = \hat{\beta},
$$

therefore 

\begin{align*}
    \hat{\beta} &= \E \bigg[\sum_{i=1}^{n} d_i {\bf Y}_i\bigg]  = \sum_{i=1}^{n} d_i \E({\bf Y}_i) = \sum_{i=1}^{n} d_i (\alpha + \beta x_i) \\
    &= \alpha \bigg(\sum_{i=1}^{n} d_i\bigg) + \beta \bigg(\sum_{i=1}^{n} d_i x_i\bigg).
\end{align*}

This equality is true for all $\alpha$ and $\beta$ if and only if 

\begin{align}
    \sum_{i=1}^{n} d_i = 0 \textnormal{ and } \sum_{i=1}^{n} d_i x_i = 1.
    \label{blue conditions}
\end{align}

Then, $\{d_i\}_{i=1}^{n} \in \R$ must satisfy \eqref{blue conditions} in order for the estimator $\hat{\beta}$ to be an unbiased estimator of $\beta$. We say a given estimator is the "best" if it has the smallest variance among all unbiased estimators. Similarly, an estimator is the \textbf{best linear unbiased estimator} (BLUE) if it is the linear unbiased estimator with the smallest variance. Since the  $\{{\bf Y}_i\}_{i=1}^{n}$ are uncorrelated with equal variance $\sigma^2$, the variance of any linear estimator if given by 

\begin{align*}
    \var \bigg[\sum_{i=1}^{n} d_i {\bf Y}_i\bigg]  = \sum_{i=1}^{n} d_i^2 \var({\bf Y}_i) = \sum_{i=1}^{n} d_i^2 \sigma^2 = \sigma^2 \sum_{i=1}^{n} d_i^2. 
\end{align*}

The BLUE of $\beta$ is, therefore, defined by a set of constants  $\{d_i\}_{i=1}^{n} \in \R$ which satisfy \eqref{blue conditions} and have the minimum value of $\sum_{i=1}^{n} d_i^2$, with $\sigma^2$ being an overall multiplicative constant. Let 

$$
k = n, \blank v_i = x_i, \blank c_i = 1 \textnormal{ and } a_i = d_i,
$$

which imply $\bar{v}_c = \bar{x}$. If $d_i$ is of the form 

\begin{equation}
    d_i = Kc_i ( v_i - \bar{v}_c) = K (x_i - \bar{x}) \textnormal{ with } i = 1, \cdots, n,
    \label{gram schmidt}
\end{equation}


then the set $\{d_i\}_{i=1}^{n} \in \R$ maximizes 

\begin{equation}
    \frac{\bigg(\sum_{i=1}^{n} d_i x_i\bigg)^2}{\sum_{i=1} d_i^2} \blank \forall \{d_i\}_{i=1}^{n} \in \R \blank | \blank \sum_{i} d_i = 0.
    \label{maximize functional}
\end{equation}

Furthermore, since 

$$
\{\{d_i\}_{i=1}^{n} \in \R: \blank \sum_{i}\ d_i \textnormal{ and } \sum_{i=1} d_i x_i = 1 \} \subset \{\{d_i\}_{i=1}^{n} \in \R : \blank \sum_{i} d_i = 0\},
$$

if the $d_i$s of the form \eqref{gram schmidt}, satisfy the \eqref{blue conditions} as well, then they certainly maximize 
\eqref{maximize functional} among the set of all real finite-sequences with the sum of its elements being zero: $\{\{d_i\}_{i=1}^{n} \in \R : \blank \sum_{i} d_i = 0\}$. Now, using \eqref{gram schmidt}, we have 

\begin{equation}
    \sum_{i=1}^{n} d_i x_i = \sum_{i=1}^{n} K(x_i - \bar{x}) x_i = K S_{xx}.
\end{equation}

Now, \eqref{blue conditions}'s second constraint is satisfied if $K = \frac{1}{K_{xx}}$. Therefore, we can force the first constraint by setting the $d_i$s as 

\begin{equation}
  \{d_i\}_{i=1}^{n} \in \R \Rightarrow d_i = \frac{x_i - \bar{x}}{S_{xx}} \blank \forall i = 1, \cdots, n,
  \label{contrasts estimators}
\end{equation}

which then produce a maximum. Note that for all finite subsets of real numbers which satisfy \eqref{blue conditions}, 

$$
\frac{\bigg(\sum_{i=1}^{n} d_i x_i\bigg)^2}{\sum_{i=1} d_i^2} = \frac{1}{\sum_{i=1} d_i^2}.
$$

Thus, for all $\{\{d_i\}_{i=1}^{n} \in \R : \blank \sum_{i} d_i = 0\}$ which satisfy \eqref{blue conditions}, maximization of \eqref{maximize functional} is equivalent to minimization of $\sum_{i=1}^{n} d_i^2$. Hence, we can conclude that the $d_i$s defined in \eqref{contrasts estimators}, give the minimum value of $\sum_{i=1}^{n} d_i^2$ among all $d_i$ which satisfy \eqref{blue conditions}, with the linear unbiased estimator defined by these $d_i$s, namely,

\begin{equation}
    \hat{\beta} = \sum_{i=1}^{n} \frac{x_i - \bar{x}}{S_{xx}} y_i = \frac{S_{xy}}{S_{yy}}.
\end{equation}

The variance of $\beta$ is 

\begin{equation}
    \var \beta = \sigma^2 \sum_{i=1}^{n} d_i^2 = \frac{\sigma^2}{S_{xx}} = \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2},
\end{equation}

note that, since the values $\{x_i\}_{i=1}^{n}$ are chosen by the experimenter, they can be chosen to make $S_{xx}$ large and the variance of the estimator small. That is, the experiment can be designed to make the estimator more precise. For example, if all the $\{x_i\}_{i=1}^{n}$ must be chosen so that they lie in the $\R_{[a,b]}$ and if $n$ is even, the choice of the $\{x_i\}_{i=1}^{n}$ which makes $S_{xx}$ as large as posible is to take half of the $x_i$s equal to $a$ and half equal to $b$. This would be the best design in that it would give the most precise estimate of the slope $\beta$. In practice, however, this design is seldom used since an experimenter is hardly ever certain of the model and this two-point design only gives information about the value of $\E ({\bf Y}_i\blank|\blank{\bf X}_i)$ at only two values. \\

A similar analysis will show that $\hat{\alpha}$ is the BLUE of the intercept $\alpha$, which must be written in terms of
$\{\{d_i\}_{i=1}^{n} \in \R$ which must satisfy 

\begin{align}
    \sum_{i=1}^{n} d_i = 1 \textnormal{ and } \sum_{i=1}^{n} d_i x_i = 0.
\end{align}

The fact that least squares estimators are BLUEs holds in other linear models as well. This result is called the Gauss-Markov theorem. \\

\subsection{Gauss-Markov Theorem}
Let 

\begin{equation}
    {\bf Y} = {\bf X} \bm{\beta} + \bm{\epsilon}, \textnormal{ where } \begin{array}{c}
         {\bf Y}, \bm{\epsilon} \in \R^{n}, \\
         \bm{\beta} \in \mathds{R}^{K}, \\
         {\bf X} \in \R^{n \times K}
    \end{array},
\end{equation}

where the $\beta_j$ are non-random but unobservable parameters, the $X_{ij}$ are non-random and observable (the explanatory variables), the $\epsilon_i$ are random (the errors), and thus the $Y_i$ are also random. Note that, should we desire to include a constant in the model above, one can choose to introduce the constant as a $\beta_{K+1}$ variable with a newly introduced last column of ${\bf X}$ being unity ie. ${\bf X}_{i(K+1)} = 1, \blank \forall i$\footnote{Note that although the ${\bf Y}_i$, as sample responses, are observable, the theorem holds only under the condition of knowing $\X_{ij}$, but not the $\Y_{ij}$.}. The Gauss-Markov assumptions concern a set of error random variables ${\bm \epsilon}$ with 

\begin{itemize}
    \item mean zero: $\E({\epsilon}_i) = 0$, 
    \item homoscedastic-compatible, having the same finite variance $\var({\epsilon}_i) = \sigma^2 < \infty, \blank \forall i$,
    \item and with distinct error terms being uncorrelated $\cov({\epsilon}_i, {\epsilon}_j) \propto \delta_{ij}$.
\end{itemize}

Then a linear estimator of $\beta_j$ is a linear combination 

$$
\hat{\beta_j} = \sum_{k = 1}^{n} c_{kj} {Y}_k = {\bf c}^{\textnormal{T}} {\bf Y},
$$

in which the coefficients $c_{ij}$ are not allowed to depend on the underlying coefficients $\beta_j$, since those are not observable, but are allowed to depend on the values $X_{ij}$, since these data are observable\footnote{Note that the dependence on the coefficients on each $X_{ij}$ is typically nonlinear, the estimator is linear in each $Y_i$, and hence in each random $\epsilon$.}. The estimator is said to be unbiased if and only if 
$$
\E(\hat{\beta_j}) = {\beta_j},
$$

regardless of the values of ${X}_{ij}$. Now, let $\sum_{j=1}^{K} \lambda_{j} \beta_{j}$ be some linear combination of the coefficients. Then the mean squared error of the corresponding estimation is 

$$
\E\bigg[\bigg(\sum_{j=1}^{K} \lambda_j (\hat{\beta_j} - {\beta_j})\bigg)^2\bigg].
$$

In other words, it's the expectation of the square of the weighted sum (across parameters) of the differences between the estimators and the corresponding parameters to be estimated. Note that since we're considering the case in which all the parameter estimates are unbiased, this mean squared error is the same as the variance of the linear combination. Then, the best linear unbiased estimator BLUE of the vector ${\bm \beta}$, of parameters $\beta_j$, is the one with the smallest mean squared error for every vector $\lambda \in \mathds{R}$ of linear combination parameters. This is equivalent to the condition that

$$
\var(\Tilde{{\bm \beta}}) - \var({{\bm \beta}}),
$$

is a positive semi-definite matrix for every other linear unbiased estimator $\Tilde{{\bm \beta}}$. Then, the \textbf{ordinary least squares estimator (OLS)} is then a matrix function 

$$
\hat{{\bm \beta}} = ({\bf X}^{\textnormal{T}} {\bf X})^{-1} {\bf X}^{\textnormal{T}} {\bf Y},
$$

which minimizes the sum of squares of residuals 

$$
\sum_{i=1}^{n} (Y_{i} - \hat{Y}_i) = \sum_{i=1}^{n} \bigg(Y_i - \sum_{j=1}^{K} \hat{\beta}_j X_{ij} \bigg)^2.
$$

The theorem now states that the OLS estimator is a BLUE. The main point behind the proof is tha the least-squares estimator is uncorrelated with every linear unbiased estimator of zero ie. a linear combination of the $Y_i$s whose coefficients do not depend upon the unobservable $\bm \beta$ but whose expected value is always zero.

\subsubsection{Proof of the Gauss-Markov theorem} TODO

\subsection{Models and Distribution Assumptions}

In this section, we'll treat two models for the paired data $\{(x_i, y_i)\}_{i=1}^n$ that are called simple linear regression models. Up until now, the least squares estimates were not obtained via statistical considerations but via a minimization problem. Therefore, we cannot make any statistical inference on the results. In order to do exactly that, we present two complete probabilistic models. \\

\paragraph{\textbf{Conditional Normal Model}} 

The conditional normal model is the most common simple linear regression model and the most straightforward to analyze. The observed data are the $n$ pairs $\{(x_i, y_i)\}_{i=1}^n$. The values of the predictor variable, $\{x_i\}_{i=1}^n$ are assumed to be known, fixed constants, chosen by the experimenter. The values of the response variable $\{y_i\}_{i=1}^n$ are observed values of random variables, $\{{\bf Y}_i\}_{i=1}^n$, which are assumed to be independent. Furthermore, we assume the distribution of the ${\bf Y}_i$s to be normal, specifically

\begin{equation}
    {\bf Y}_i \sim \N(\alpha+\beta x_i, \sigma^2), \blank i = 1, \cdots, n.
\end{equation}

Thus the population regression function is a linear function of $x$, that is $\E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta x_i, $ and all the ${\bf Y}_i$s have the same variance, $\sigma^2$. The (complete) conditional normal model is then

\begin{equation}
    {\bf Y} = \bm \alpha J_n + \bm \beta {\bf X} + \bm \epsilon,
    \label{conditional normal model}
\end{equation}

where $(J_n)_k = 1, \blank 1 \leq k \leq n$, where the errors ${\bm \epsilon}$ are assumed to be $\N(0,\sigma^2)$ random variables. \\

Since the $\seqY$ are independent random variables, the joint pdf is the product of the marginal pdfs, ie.

\begin{align}
    f(\Y | \alpha, \beta, \sigma^2) &= f(y_1, \cdots, y_n | \alpha, \beta, \sigma^2) \\
    &= \prod_{j=1}^{n} f(y_j | \alpha, \beta, \sigma^2) \\
    &= \prod_{j=1}^{n} \frac{1}{\sqrt{2\pi}\sigma} \exp\bigg[-\frac{(y_j -(\alpha + \beta x_j))^2}{2\sigma^2}\bigg] \\
    &=  \frac{1}{{(2\pi)}^{\frac{n}{2}}\sigma^{n}} \exp\bigg[-\frac{\bigg(\sum_{j=1}^{n}(y_j -(\alpha + \beta x_j)\bigg)^2}{2\sigma^2}\bigg]. \\
    \label{conditional normal pdf}
\end{align}

\paragraph{\textbf{Biviariate Normal Model}} 

In all the previous models, the values of the predictor variable, $\{x_i\}_{i=1}^n$ have been fixed, known constants. In some situations it's necessary to consider models in which the predictor variable, as well as the response variable, is random ie. sometimes these $\{x_i\}_{i=1}^n$ values are actually observed values of random variables $\{{\bf X}_i\}_{i=1}^n$. One such model is the \textit{bivariate normal model}. \\

In the bivariate normal model, the data $\{(x_i, y_i)\}_{i=1}^n$ are the observed values of the bivariate random vectors $\{({\bf X}_i, {\bf Y}_i)\}_{i=1}^n$. The random vectors are independent and the joint distribution of $({\bf X}_i, {\bf Y}_i)$ is assumed to be bivariate normal. Specifically, it is assumed that 

$$
({\bf X}_i, {\bf Y}_i) \sim \N(\mu_{\bf X}, \mu_{\bf Y}, \sigma^2_{\bf X}, \sigma^2_{\bf y}, \rho),
$$

the joint pdf and various properties of the data is $\{({\bf X}_i, {\bf Y}_i)\}_{i=1}^n$ is the product of the bivariate pdfs. In a simple linear regression analysis, we are still thinking of the $x_i$s as the predictor variable and $y$ as the response variable. That is, we are most interested in predicting the value of $y$ having observed the value of $x$. This naturally leads to basing inference on the conditional distribution of $\Y$ given $\X = x$, which for the case of a bivariate normal model, the conditional distribution distribution of $\Y$ given $\X = x$ is normal. The population regression function is now a true conditional expectation, given by

$$
\E(\Y|\X = x) = \mu_{\Y} + \rho \frac{\sigma_{\Y}}{\sigma_\X} (x-\mu_\X) = \bigg[\mu_\Y - \rho \frac{\sigma_{\Y}}{\sigma_\X} \mu_\X \bigg] + \bigg[ \rho \frac{\sigma_{\Y}}{\sigma_\X}\bigg]x.
$$

Therefore the bivariate normal model implies that the population regression is a linear function of $x$, where $\beta = \rho \frac{\sigma_{\Y}}{\sigma_\X}$ and where $\alpha = \mu_\Y - \rho \frac{\sigma_{\Y}}{\sigma_\X} \mu_\X$. We need not assume this as in the previous models. Also, as in the conidtional normal model, the conditional variance of the response variable $\Y$ doesn't depend on $x$,

$$
\var(\Y|\X = x) = \sigma^2_\Y (1-\rho^2).
$$

For the bivariate normal model, the linear regression analysis is almost always carried out using the conditional distribution of $\{{\bf Y}_i\}_{i=1}^n$ 
given $\{{\bf X}_i\}_{i=1}^n \blank |\blank \X_i = x_i $, rather than the unconditional distribution of $\{({\bf X}_i, {\bf Y}_i)\}_{i=1}^n$. But then we are in the same situation as the one described by the conditional normal model. The fact that the $\{x_i\}_{i=1}^{n}$ are observed values of random variables is immaterial if we condition on these values and, in general, in simple linear regression we do not use the fact of bivariate normality except to define the conditional distribution. \\

\paragraph{\textbf{Estimation and Testing with Normal Errors}}

Consider the conditional normal model, where the regression is given by \eqref{conditional normal model}. First, we find the maximum likelihood estimates for the three parameters, $\alpha, \beta$ and $\sigma^2$. To that end, we use the joint pdf given by 
\eqref{conditional normal pdf}. The log-likelihood functions is 

\begin{equation}
\log L(\alpha,\beta, \sigma^2 \blank | \blank {\bf x},{\bf y}) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log \sigma^2 - \frac{\sum_{j=1}^{n}(y_j - \alpha - \beta x_j)^2}{2\sigma^2}.
\label{MLE linear regression}
\end{equation}

For any fixed value of $\sigma^2$, $\log L$ is maximized as a function of $\alpha$ and $\beta$ by those values, the estimators $\hat{\alpha}$ and $\hat{\beta}$, that minimize 

$$
\sum_{j=1}^{n}(y_j - \alpha - \beta x_j)^2,
$$

which is precisely the RSS. Then the minimizing values are 


\begin{align*}
   \hat{\beta} = \frac{S_{xy}}{S_{xx}} \textnormal{ and } \hat{\alpha} = \bar{y} - \hat{\beta}\bar{x}.
\end{align*}

Thus, the least squares estimators of $\alpha$ and $\beta$ are also the Maximum Likelihood Estimators. The values $\hat{\beta}$ and $\hat{\alpha}$ are the maximizing values for any fixed value of $\sigma^2$. Now, substituting in the log likelihood, to find the MLE of $\sigma^2$ we need to maximize \eqref{MLE linear regression}, as follows:

\begin{align*}
    \frac{\partial}{\partial \sigma^2} & \log L(\alpha,\beta, \sigma^2 \blank | \blank {\bf x},{\bf y}) = \frac{\partial}{\partial \sigma^2} \bigg[-\frac{n}{2}\log(2\pi) - \frac{n}{2}\log \sigma^2 - \frac{\sum_{j=1}^{n}(y_j - \alpha - \beta x_j)^2}{2\sigma^2}\bigg]_{\sigma^2 = \hat{\sigma}^2} = 0 \\
    &\Rightarrow -\frac{n}{2} \frac{1}{\hat{\sigma}^2} + \frac{\sum_{j=1}^{n}(y_j - \alpha - \beta x_j)^2}{(\hat{\sigma}^2)^2} = -\frac{n}{2} + \frac{\sum_{j=1}^{n}(y_j - \alpha - \beta x_j)^2}{\hat{\sigma}^2} = 0 \\
    &\Rightarrow \hat{\sigma}^2 = \frac{1}{n} \sum_{j=1}^{n}(y_j - \alpha - \beta x_j)^2,
\end{align*}

ie. the MLE of $\sigma^2$, under the conditional normal model, is the RSS evaluated at the least squares lines, divided by the sample size. It turns out that, though the estimators $\hat{\beta}$ and $\hat{\alpha}$ are unbiased, $\hat{\sigma}^2$ is not. There is, however, an important result regarding the calculation of its expected values, as follow

\begin{lemma}
Let $\seqY$ be uncorrelated homoscedastic random variables with $\var \Y_i = \sigma^2$. Let $\{c_i\}_{i=1}^{n} \subset \R$, $\{d_i\}_{i=1}^{n} \subset \R$, then

$$
\cov\bigg(\sum_{i=1}^{n} c_i \Y_i, \sum_{i=1}^{n} d_i \Y_i\bigg) = \sigma^2 \sum_{i=1}^{n} c_i d_i.
$$
\end{lemma}

Note that the lemma asks not for normality nor independence of the set $\seqY$ of random variables. \\

Now, we wish to find $\sigma^2$'s bias. From the definition of the linear model, we have $
\epsilon_i = \Y_i - \alpha - \beta x_i, $ then the \textit{residuals from the regression} as 
$\hat \epsilon_i = \Y_i - \hat \alpha - \hat \beta x_i$
and thus 

$$
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} \hat{\epsilon}_i^2 = \frac{\textnormal{RSS}}{n}.
$$

It can be calculated that $\E \hat{\epsilon}_i = 0$ \footnote{Indeed, using the residuals from the regression's definition we have 

\begin{align*}
    \E \hat{\epsilon}_i &= \E[\Y_i - \hat \alpha - \hat \beta x_i] \\
    &= \E[\Y_i] - \E[\hat \alpha] - \E[\hat \beta x_i] \\
    &= \alpha + \beta x_i - \alpha - \beta x_i = 0,
\end{align*}

since both $\hat{\alpha}$ and $\hat{\beta}$ are unbiased. We can also readily compute $\hat{\epsilon}_i$'s variance as

\begin{align*}
    \var \hat \epsilon_i &= \var[\Y_i - \hat \alpha - \hat \beta x_i] = \E[\Y_i - \hat \alpha - \hat \beta x_i]^2 - 0 \\
    &= \E\bigg[(\Y_i - \alpha - \beta x_i) - (\hat \alpha - \alpha) - (\hat \beta - \beta) x_i \bigg]^2 \\
    &= \E(\Y_i - \alpha - \beta x_i)^2  + \E(\hat \alpha - \alpha)^2 + \E(\hat \beta - \beta)^2 x_i^2 \\ 
    & \blank \blank \blank + 2\E[(\Y_i - \alpha - \beta x_i) (\hat \alpha - \alpha)] + 2\E[(\hat \alpha - \alpha)(\hat \beta -  \beta)] + 2 \E[(\Y_i - \alpha - \beta x_i)(\hat \beta -  \beta)] \\
    &= \var \Y_i + \var \hat \alpha + \var \hat \beta - 2\cov (\Y_i, \hat \alpha) - 2x_i \cov(\Y_i, \hat \beta) + 2x_i \cov(\hat \alpha, \hat \beta) 
\end{align*}
}
with its variance given by 

\begin{equation}
    \var \hat{\epsilon_i} = \E\epsilon^2_i = \sigma^2 \bigg[\frac{n-2}{n} + \frac{1}{S_{xx}} \bigg(\frac{1}{n} \sum_{j=1}^{n} x_j^2 + x_i^2 - 2(x_i - \bar{x})^2 - 2x_i \bar{x}\bigg) \bigg].
\end{equation}

Thus, 

\begin{align*}
    \E \hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^{n} \E \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^{n} \sigma^2 \bigg[\frac{n-2}{n} + \frac{1}{S_{xx}} \bigg(\frac{1}{n} \sum_{j=1}^{n} x_j^2 + x_i^2 - 2(x_i - \bar{x})^2 - 2x_i \bar{x}\bigg) \bigg]. \\
    &= \frac{n-2}{n} {\frac{\cancel n}{\cancel n}} + \frac{1}{nS_{xx}} \bigg[\sum_{j=1}^{n} x_j^2 + \sum_{i=1}^{n} x_i^2 - 2S_{xx} - 2\frac{1}{n} \bigg(\sum_{i=1}^{n} x_i\bigg)^2 \bigg] \sigma^2\\
    &= \frac{n-2}{n} \sigma^2 + \frac{2}{nS_{xx}} \sigma^2 \bigg[\summation x_i^2 - S_{xx} - \frac{1}{n} \bigg(\summation x_i\bigg)^2 \bigg] \textnormal{ but since } \begin{array}{c}
         \summation x_i \bar{x} = \frac{1}{n} (\summation x_i)^2  \\
         \summation x_i^2 - \frac{1}{n} (\summation x_i)^2 = S_{xx}
    \end{array} \\
    &= \frac{n-2}{n} \sigma^2 + \frac{2}{nS_{xx}} \sigma^2 \cancel{\bigg[S_{xx} - S_{xx} \bigg]} \Rightarrow  \E \hat{\sigma}^2 = \frac{n-2}{n} \sigma^2.
\end{align*}

Therefore the MLE $\hat{\sigma}^2$ is a biased estimator of $\sigma^2$. However, we can define a new unbiased estimator

$$
\hat{S}^2 = \frac{n}{n-2} \hat{\sigma}^2 = \frac{1}{n-2} \summation (y_i - \hat{\alpha} - \hat{\beta} x_i)^2 = \frac{1}{n-2} \summation \hat{\epsilon}_i^2.
$$

\clearpage

To develop estimation and testing procedures, based on these estimators, we need to know their sampling distributions, which are given by the following theorem

\begin{theo}
\label{linear model distributions}\footnote{\begin{proof}
First, we are to show that the $\hat{\alpha}$ and $\hat{\beta}$ estimators have the indicated normal distributions. To that end, we note that both estimators are linear functions of the independent normal variables $\seqY$. Therefore, they both have normal distributions. Remember as well, that if $\hat{\beta} = \summation d_i \Y_i$, with $d_i = \frac{x_i - \bar{x}}{S_{xx}} \textnormal{ for } i = 1, \cdots, n$, then 

$$
\E \hat{\beta} = \beta \textnormal{ and } \var \hat{\beta} = \frac{\sigma^2}{S_{xx}},
$$

ie. $\hat{\beta}$ is an unbiased estimators with minimal variance (since it's the best linear unbiased estimator). Furthermore, the estimator $\hat{\alpha} = \bar{\Y} - \hat{\beta} \bar{x}$ can be expressed as $\hat{\alpha} = \summation c_i \Y_i$ with $c_i = \frac{1}{n} - \frac{(x_i - \bar{x})\bar{x}}{S_{xx}}$, then 

\begin{align*}
    \E \hat{\alpha} &= \summation c_i \E \Y_i = \summation \bigg(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{S_{xx}}\bigg) (\alpha + \beta x_i) = \alpha + \beta \bar{x} - \frac{\bar x \summation(x_i - \bar x)}{\summation (x_i - \bar{x})^2} \alpha - \frac{\bar x \summation(x_i - \bar x) x_i}{\summation (x_i - \bar{x})^2} \beta \\
    &= \alpha + \beta \bar{x} - \frac{\bar x(\summation x_i -  \summation \bar x)}{\summation (x_i - \bar{x})^2} \alpha - \frac{\bar x (\summation x_i^2 -  \summation x_i\bar x) }{\summation (x_i - \bar{x})^2} \beta \\
    &= \alpha + {\beta \bar{x}} - \frac{\bar x \cancel{(n \bar x -  n\bar x)}}{\summation (x_i - \bar{x})^2} \alpha - \frac{\beta \bar x}{S_{xx}} {\summation x_i^2 - n\bar{x}^2} = \alpha + {\beta \bar{x}} - \frac{\beta \bar{x}}{S_{xx}} \summation(x_i^2 -2n \bar{x}^2 + n \bar{x}^2) \\
    &= \alpha + {\beta \bar{x}} - \frac{\beta \bar{x}}{S_{xx}} \underbrace{\summation(x_i - \bar x)^2}_{S_{xx}} = \alpha,
\end{align*}

with the variance being 

\begin{align*}
    \var \hat \alpha = \sigma^2 \summation c_i^2 &= \sigma^2 \summation \bigg[\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{S_{xx}}\bigg]^2 = \sigma^2 \bigg[ \summation \frac{1}{n^2} + \summation \frac{(x_i-\bar{x})^2\bar{x}^2}{S_{xx}^2} - \cancel{2 \summation \frac{1}{n} \frac{(x_i-\bar{x})\bar{x}}{S_{xx}}} \bigg]\\
    &= \sigma^2 \bigg( \frac{1}{n} + \frac{\bar{x}^2}{S_{xx}} \bigg) = \frac{\sigma^2 \summation x_i^2}{S_{xx}}
\end{align*}

We can also compute the covariance between both estimators
 
\begin{align*}
    \cov(\hat{\alpha}, \hat{\beta}) = \cov \bigg(\summation c_i \Y_i, \summation d_i \Y_i \bigg) = \sigma^2 \summation c_i d_i = \sigma^2 \summation \bigg[\bigg(\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{S_{xx}}\bigg) \frac{x_i - \bar{x}}{S_{xx}}\bigg] \\
    = \sigma^2 \summation \frac{1}{n} \frac{x_i - \bar{x}}{S_{xx}} - \sigma^2 \summation \frac{(x_i - \bar{x})^2 \bar{x}}{S_{xx}^2} = -\frac{\sigma^2 \bar{x}}{S_{xx}}
\end{align*}

Next, we must show that $\hat{\alpha}$ and $\hat{\beta}$ are independent of $\hat{S}^2$. From the definition of the $\hat{\epsilon}_i$, we can write $
\hat{\epsilon}_i = \sum_{j=1}^{n} [\delta_{ij} - (c_j + d_j x_i)] \Y_i$. Now, given that $\hat{\alpha} = \summation c_i \Y_i$ and $\hat{\beta} = \summation d_i \Y_i$, we can compute $\cov(\hat{\epsilon}_i, \hat{\alpha})$ and $\cov(\hat{\epsilon}_i, \hat{\beta})$, as follows 

\begin{align*}
    \begin{array}{c}
        \cov(\hat{\epsilon}_i, \hat{\alpha}) = \sigma^2 \sum_{j=1}^{n} c_j [\delta_{ij} - (c_j + d_j x_i)] = \sigma^2[c_i - \sum_{j=1}^{n} c_j (c_j + d_j x_i)] = \sigma^2 [c_i - \sum_{j=1}^{n} c_j^2 - x_i \sum_{j=1}^{n} c_j d_j] \\
         \blank  \\
         = \sigma^2 [\frac{1}{n}-\frac{(x_i-\bar{x})\bar{x}}{S_{xx}} - \frac{1}{n} - \frac{\bar{x}^2}{S_xx} - x_i \frac{\bar{x}}{S_{xx}}] = 0 \\
         \blank  \\
        \cov(\hat{\epsilon}_i, \hat{\beta}) = \sigma^2 \sum_{j=1}^{n} d_j [\delta_{ij} - (c_j + d_j x_i)] = \sigma^2[d_i - \sum_{j=1}^{n} d_j (c_j + d_j x_i)]  = \sigma^2 [d_i - \sum_{j=1}^{n} c_j d_j - x_i \sum_{j=1}^{n} d_j^2]  \\
         \blank  \\
        = \sigma^2 [\frac{(x_i-\bar{x})}{S_{xx}} - \frac{\bar{x}}{S_{xx}} - \frac{x_i}{S_{xx}}] = 0.
    \end{array}
\end{align*}

Then, it follows that, under normal sampling, $\hat{S}^2 = \summation \frac{\hat{\epsilon}_i^2}{n-2}$ is indeed independent from $\hat{\alpha}$ and $\hat{\beta}$. Proving that $\frac{(n-2)\hat{S}^2}{\sigma^2} \sim \chi^2_{n-2}$, we'd write $(n-2)\hat{S}^2$ as the sum of $n-2$ independent random variables, each of which has as $\chi^2_1$-distribution, ie. we must find constants $\{a_{ij}\}_{i=1, \blank j = 1}^{i = 1, j = n-2}$ which satisfy 

$$
\summation \hat{\epsilon}_i^2 = \sum_{j=1}^{n-2}\bigg(\summation a_{ij} \Y_i \bigg)^2,
$$

where 

$$
\summation a_{ij} = 0, \blank \forall j = 1, \cdots, n-2 \textnormal{ and } \summation a_{ij} a_{ij'} \propto \delta_{jj'}.
$$

Then, by Cochran's theorem, it would follow that $\frac{(n-2)\hat{S}^2}{\sigma^2} \sim \chi^2_{n-2}$. The details are somewhat involved because of the general nature of the $\{x_i\}_{i=1}^{n}$. 

\end{proof}}
Under the conditional normal regression model, the sampling distribution of the unbiased estimators, $\hat{\alpha}, \hat{\beta}$ and $\hat{S}^2$ are 

\begin{align*}
    \hat{\alpha} \sim \N\bigg(\alpha, \frac{\sigma^2}{nS_{xx}} \summation x_i^2\bigg), \blank  & \hat{\beta} \sim \N\bigg(\beta, \frac{\sigma^2}{S_{xx}}\bigg) \textnormal{ with } \cov(\hat{\alpha}, \hat{\beta}) = -\frac{\sigma^2 \bar{x}}{S_{xx}}.
\end{align*}

Furthermore, $(\hat{\alpha}, \hat{\beta})$ and $\hat{S}^2$ are independent, with the latter estimator's distribution being 

$$
\frac{(n-2)\hat{S}^2}{n} \sim \chi_{n-2}^2. 
$$
\end{theo}

\blank \textnormal{}

The RSS from the linear regression contains information about the worth of a polynomial fit of a higher order, over and above a linear fit. Since, in this model, we assume that the population regression is linear, the variation in this higher-order fit is just random variation. Robson (1959) gives a general recursion formula for finding coefficients for such higher-order polynomial fits. Alternatively, Cochran's theorem can be used to establish that $\summation \frac{\hat{\epsilon}_i^2}{\sigma} \sim \chi^2_{n-2} $. \\

Inferences regarding the two parameters, $\alpha \textnormal{ and } \beta$, are usually based on the following two Student's \tdis, which follow immediately from the normal and \chidis and from Theorem \ref{linear model distributions}, 

\begin{align}
    \frac{\hat{\alpha}-\alpha}{S \sqrt{\frac{\summation x_i^2}{nS_{xx}}}} \sim t_{n-2} \textnormal{ and } \frac{\hat{\beta}-\beta}{\frac{S}{\sqrt{S_{xx}}}} \sim t_{n-2}.
    \label{parameter distributions}
\end{align}

The joint distribution of these two $t$-statistics is called a \textit{bivariate Student's \tdis}. This is so, since the joint distribution of $\hat \alpha \textnormal{ and } \hat \beta$ is bivariate normal and the same variance estimate $S$ is used in both univariate $t$-statistics. This joint distribution would be used if we wanted to do simultaneous inference regarding $\alpha \textnormal{ and } \beta$. \\

Usually, there is more interest in $\beta \textnormal{ than in } \alpha$ since, the parameter $\alpha$ is the expected value of $\Y$ at $x = 0$, $\E(\Y | \X = 0)$ whereas $\beta$ is the rate of change of $\E(\Y | \X = x)$ as a function of $x$. That is, $\beta$ is the amount that $\E(\Y | \X = x)$ changes if $x$ is changed by one unit. Thus, this parameter relates to the entire range of $x$ values and contains the information about whatever linear relationship exists between $\Y$ and $x$. \\

Note that if $\beta = 0$, then $\E(\Y | \X = x) = \alpha$, thus $\Y \sim \N(\alpha, \sigma^2)$ and is independent of $x$. The corresponding $\beta = 0$-hypothesis test can be written as a particular type of ANOVA test, one which tests whether all the treatments are equal. In the ANOVA, the null hypothesis states that the treatments $(x)$ are unrelated to the response in any way, while in linear regression the null hypothesis $\beta = 0$ states that the treatments are unrelated to the response in a linear way. 

To test 

$$
H_0 : \blank \beta = 0 \textnormal{ versus } H_1 : \blank \beta \neq 0,
$$

we reject $H_0$ at $\alpha$-level if 

\begin{align*}
\bigg|\frac{\hat{\beta} - 0}{\frac{S}{\sqrt{S_{xx}}}}\bigg| > t_{n-2, \blank \frac{\alpha}{2}} \textnormal{ or equivalently, if } \frac{\hat{\beta}^2}{\frac{S^2}{S_{xx}}} > F_{1, \blank n-2, \blank \alpha}.
\end{align*}

Recalling $\hat{\beta}$'s definition and that RSS = $\summation \hat{\epsilon}_i^2$ we have

$$
\frac{\hat{\beta}^2}{\frac{S^2}{S_{xx}}} = \frac{\frac{S_{xy}^2}{S_{xx}}}{\frac{\textnormal{RSS}}{n-2}} = \frac{\textnormal{Regression sum of squares}}{\textnormal{Residual sum of squares/df}}.
$$

Said information can be succinctly stated in an ANOVA table.

\begin{table}
    \centering
    \caption{ANOVA Table for simple linear regression}
    \rowcolors{5}{}{gray!10}
    \begin{tabular}{*5c}
        \toprule
        Source of Variation & df  & Sum of squares & Mean square & F-statistic \\    
        Regression (slope) & 1 & Reg. SS = $\frac{S_{xy}^2}{S_{xx}}$     & MS(Reg) = Reg. SS & $F = \frac{\textnormal {MS(Reg)}}{\textnormal {MS(Resid)}}$ \\
        Residual & $n-2$ & RSS = $\summation \hat{\epsilon}_i^2$ & MS(Resid) = $\frac{\textnormal{RSS}}{n-2}$  \\
        \hline
        Total & $n-1$ & SST = $\summation (y_i - \bar{y})^2$ & & \\
        \midrule
        \bottomrule
    \end{tabular}
\end{table}

Equation \eqref{parameter distributions} can be used to construct $100(1-\alpha)\%$-confidence intervals for $\beta$.

\begin{equation}
    \hat{\beta} - t_{n-2, \blank \frac{\alpha}{2}} \frac{S}{\sqrt{S_{xx}}}< \beta <\hat{\beta} + t_{n-2, \blank \frac{\alpha}{2}} \frac{S}{\sqrt{S_{xx}}}.
\end{equation}

Also, an $\alpha$-level test of $H_0: \blank \beta = \beta_0$ versus $H_1: \blank \beta \neq \beta_0$ rejects $H_0$ if 

\begin{equation}
    \bigg|\frac{\hat{\beta} - \beta_0}{\frac{S}{S_{xx}}}\bigg| > t_{n-2, \blank \frac{\alpha}{2}}.
\end{equation}

As mentioned before, it is indeed common to test these two hypothesis, to determine if there is some linear relationship between the predictor and response variables. However, this test is more general, since any value of $\beta_0$ can be specified. The regression ANOVA can only test the ANOVA null hypothesis $H_0: \beta = 0$\footnote{Note as well that the total sum of squares is the sum of the regression sum of squares and the residual sum of squares, ie. 

$$
\summation (y_i - \bar{y})^2 = \summation (\hat{y}_i - \bar{y})^2 +  \summation (y_i - \hat{y}_i)^2, \textnormal{ where } \hat{y}_i = \hat \alpha + \hat \beta x_i. 
$$

Notice the similarity of these sums of squares too those in ANOVA, with the total sum of squares being the same. Indeed, since $y_i - \bar{y}_i = (y_i - \hat y_i) + (\hat y_i - \bar y_i)$, we just have to show that the cross term is zero by itself, 

\begin{align*}
    \summation (y_i - \hat y_i) (\hat y_i - \bar y_i) &= \summation \bigg[y_i - (\hat \alpha + \hat \beta x_i)\bigg]\bigg[(\hat \alpha + \hat \beta x_i) - \bar{y}\bigg] \\
    &= \summation \bigg[(\hat{y}_i - \bar{y}) - \hat{\beta} (x_i - \bar{x})\bigg]\bigg[\hat{\beta}(x_i - \bar{x})\bigg] \textnormal{ where we used } \hat{\alpha} = \bar{y} - \hat{\beta} \bar{x} \\
    &= \hat{\beta} \summation (\hat{y}_i - \bar{y})(x_i - \bar{x}) - \hat{\beta}^2 \cancel{\summation (x_i - \bar{x})^2} \\
    &= \hat{\beta} \summation[\hat{y}_i x_i - \hat{y}_i\bar{x} - \bar{y} x_i + \bar{x}\bar{y}] = \hat{\beta} \summation [\hat{y}_i x_i - \bar{x}y_i] -\cancel{n\bar{y}\bar{x}} + \cancel{n \bar{x}\bar{y}} = 0.
\end{align*}

Note as well that 

$$
\summation (\hat{y}_i - \bar{y})^2 = \hat{\beta}^2 \summation (x_i - \bar{x})^2 = \frac{S_{xy}^2}{S_{xx}}.
$$}. \\

The RSS measures deviation of the fitted line from the observed values, and the regression sum of squares, analogous to the ANOVA treatment sum of squares, measures the deviation of predicted values ("treatment means") from the grand mean. Also, as in the ANOVA, the sum of squares identity is valid because of the disappearance of the cross term. Also, note as well that 

$$
\summation (\hat{y}_i - \bar{y})^2 = \hat{\beta}^2 \summation (x_i - \bar{x})^2 = \frac{S_{xy}^2}{S_{xx}},
$$

which is easier to use for computing and provides the link with the $t$-test. But, $\summation (\hat{y}_i - \bar{y})^2$ is the more easily interpreted expression. \\

A statistic that is used to quantify how well the fitted line describes the data is the \textit{coefficient of determination} $r^2$. It is defined as the ratio of the regression sum of squares to the total sum of squares, which can be written in the various forms 

\begin{align*}
    r^2 = \frac{\textnormal{Regression sum of squares}}{\textnormal{Total sum of squares}} = \frac{\summation (\hat{y}_i - \bar{y})^2}{\summation (y_i - \bar{y})^2} = \frac{S_{xy}^2}{S_{xx} S_{yy}}.
\end{align*}

The coefficient of determination measures the proportion of the total variation in the response variable $\{y_i\}_{i=1}^{N}$, measured by $S_{yy}$, that is explained by the fitted line, measured by the regression sum of squares). By definition, $r \in \R_{[0,1]}$. If the data $\{y_i\}_{i=1}^{N}$ fall exactly on the fitted line, then $y_i = \hat{y}_i, \blank \forall i$ and then $r^2 = 1$. If the data are not close to the fitted line, then the residual sum of squares will be large and $r^2 \rightarrow 0$. The coefficient of determination can also be derived as the square of the sample correlation coefficient of the $n$ pairs $\{(x_i, y_i)\}_{i=1}^{N}$ with the $n$ pairs $\{(y_i, \hat{y}_i)\}_{i=1}^{N}$. \\

\clearpage

\section{Regression Models}

In this section we are interested in generalizations to the simple linear regression models, such as models with errors in the predictor (regression with errors in variables EIV), logistc regression, general linear models and we conclude by analyzing the linear regression's robustness. \\

\subsection{Regression with Errors in Variables}

The Regression with errors in variables (EIV), also known as measurement error mode, is fundamentally different from the simple linear regression. \\

Consider again a linear model of the form 

\begin{equation}
    \Y_i = \alpha + \beta x_i + \epsilon_i,
\end{equation}

where now it is not assumed that the $\{x_i\}_{i=1}^{N}$ are known, but rather they are sampled from some random vectors. 

\subsubsection{Functional and Structural Relationships}

There are two different types of relationship that can be specified in the EIV model, one which specifies a functional linear relationship and one describing a structural linear relationship, which lead to different estimators with different properties. \\

\paragraph{\textbf{Linear functional relationship model}}

In this model, the parameters of main interest are $\alpha$ and $\beta$, and statistics is performed on these parameters by using the joint distribution of the $\{(\X_i, \Y_i)\}_{i=1}^{N}$. In particular, the data-set $\{(x_i, y_i)\}_{i=1}^{N}$ is observed via sampling from a set of random variables pairs, $\{(\X_i, \Y_i)\}_{i=1}^{N}$, whose means satisfy the \textit{functional} linear relationship

\begin{equation}
    \E \Y_i = \alpha + \beta \E \X_i.
\end{equation}

Or in terms of the mean vectors of vector random variables, 

\begin{equation}
    \begin{array}{c}
         \E \X = \bm{\zeta} \\
         \E \Y = \bm{\eta}
    \end{array} \Rightarrow \bm{\eta} = \alpha J_n + \beta \bm{\zeta}.
    \label{functional EIV-model}
\end{equation}

The $ \bm{\zeta}_i, \bm{\eta}$ are sometimes called latent variables. Note that \eqref{functional EIV-model} makes no distinction between $\X$ and $\Y$\footnote{Note that in the special case of $\bm{\delta} = 0$, the simple linear regression model is returned, since there are no measurement errors. If instead, $\alpha = 0$, then 

\begin{equation*}
     \Y_i \sim \N(\eta_i, \sigma^2_\epsilon) \textnormal{ and } \X_i \sim \N(\zeta_i, \sigma^2_\delta) 
\end{equation*}}. If we are interested in a regression, however, there should be a reason for choosing $\Y$ as the response and $\X$ as the predictor. Keeping this specification in mind, we can define formally define the EIV model as the observation of independent pairs $\{(x_i, y_i)\}_{i=1}^{N}$, according to 

\begin{equation}
    \begin{array}{c}
         \Y = \alpha J_n + \beta \bm{\zeta} + \bm{\epsilon}  \textnormal{ with } \epsilon_i \sim \N(0, \sigma^2_\epsilon), \\
         \\
         \X = \bm{\zeta} + \bm{\delta} \textnormal{ with } \delta_i \sim \N(0, \sigma^2_\delta)
    \end{array}
\end{equation}

In this model we make inference on the parameters of interest using the joint distribution $\{(\X_i, \Y_i)\}_{i=1}^{N}$, conditional on $\bm{\zeta}$. 

\blank \\

\paragraph{\textbf{Linear structural relationship model}}

This model can be thought of as an extension of the functional relationship model, extended with a hierarchy. As in the functional relationship model, we have random vectors $\X$ and $\Y$, which satisfy $ \E \X = \bm{\zeta}, \E \Y = \bm{\eta}$ as well. The functional relationship $\bm{\eta} = \alpha J_n + \beta \bm{\zeta}$ is also assumed, but now the $\bm{\zeta}-$parameters are themselves a random sample from a common population. Thus, conditional on $\bm{\zeta}$, we observe pairs $\{(\X_i, \Y_i)\}_{i=1}^{N}$ according to 

\begin{equation}
    \begin{array}{c}
         \Y = \alpha J_n + \beta \bm{\zeta} + \bm{\epsilon}  \textnormal{ with } \epsilon_i \sim \N(0, \sigma^2_\epsilon), \\
         \\
         \X = \bm{\zeta} + \bm{\delta} \textnormal{ with } \delta_i \sim \N(0, \sigma^2_\delta),
    \end{array} \textnormal{ and } \bm{\zeta} \sim \textnormal{iid } \N(\zeta, \sigma_\zeta^2).
    \label{structural EIV model}
\end{equation}

As before, the $\bm{\epsilon}$ and $\bm{\delta}$-vectors are independent and they are also independent from $\bm{\zeta}$. As in the functional model, the parameters of interest are $\alpha$ and $\beta$ as well but the inference on these parameters is made using the joint distribution $\{(\X_i, \Y_i)\}_{i=1}^{N}$, unconditional on $\bm{\zeta}$ (ie. $\bm{\zeta}$ must be integrated out according to the  distribution in \eqref{structural EIV model}. 

\end{document}
