\documentclass{homework}
\author{Tomás Pérez}
\class{Advanced Linear Methods - Lecture Notes}
\date{\today}
\title{Theory \& Notes}

\graphicspath{{./media/}}

\begin{document} \maketitle

\section{Analysis of Variance and Regression}

In a wide variety of settings, it's useful to model a random variable with a pdf or a pmf which depends on some parameters to be estimated. In many situations, a random variable can be modeled not only with unknown parameters but also with known, an sometimes controllable covariates. The standard methodology for analysis of variance (ANOVA)and regression analysis rely on a linear relationship among random variables. A basic idea of ANOVA consists on partitioning variations and analysing variations in means. \\

\subsection{\textbf{Simple Linear Regression:}}

A major purpose of regression is to explore the dependence of one variable on others. In simple linear regression, the mean of a random variable, $\Y$, is modelled as a function of another observable variable $\X$, by the relationship $\E[\Y] = \alpha + \beta \X$. In general, the function that gives $\E[\Y]$ as a function of $\X$ is called the \textit{population regression function}. \\

\subsection{\textbf{Oneway Analysis of Variance:}}

In its simplest form, ANOVA is a method of estimating the means of several populations, which are often assumed to be normally distributed. The heart of ANOVA lies on the topic of statistical design. How can we get the most information on the populations with the fewest observations? Classic ANOVA had testing as its main goal, in particular testing what is known as the ANOVA null hypothesis. However, in many settings it's not sufficient to test only one hypothesis in order to make good experimental inference. \\

In the oneway analysis of variance, also known as oneway classification, we assume that the data $Y_{ij}$ are observed according to a model:

$$
Y_{ij} = \theta_i + \epsilon_{ij} \textnormal{ where } i = 1, \cdots, k \textnormal{, } j = 1, \cdots, n_i,
$$

where the $\theta_i$ are unknown parameters and where the $\epsilon_{ij}$ are error random variables. Without loss of generality, we can assume $\E \epsilon_{ij} = 0$, since if not, we can re-scale the $\epsilon_{ij}$ and absorb the leftover mean into the $\theta_i$'s. Then 

$$
\E[Y_{ij}] = \theta_i \textnormal{ where } j = 1, \cdots, n_i.
$$

Therefore the $\{\theta_i\}_{i=1}^{k}$ are the means of the $\{Y_{ij}\}_{i=1\textnormal{, } j=1}^{k\textnormal{, } n_i}$. The first ones are the \textit{treatment means} (since the index usually correspond to different treatments or to levels of a particular treatment, such as dossage levels of a particular drug)\footnote{There is an alternative model, called the \textit{overparameterized model}, which can be written as 

\begin{equation}
Y_{ij} = \mu + \tau_i + \epsilon_{ij} \textnormal{ where } i = 1, \cdots, k \textnormal{, } j = 1, \cdots, n_i,
\label{std linear mod}
\end{equation}

where, again, $\E \epsilon_{ij} = 0$. In this setting, then 

$$
\E[Y_{ij}] = \mu+\theta_i \textnormal{ where } j = 1, \cdots, n_i.
$$

In this formulation, we think of $\mu$ as a grand mean, that is the common mean level of the treatments. The parameters $\tau_i$ then denote the unique effect due to the $i$-th treatment. However, the $\tau_i$ and $\mu$ parameters cannot be estimated independently due to problems with \textit{identifiability}. 
}. \\

\begin{df}

A parameter $\theta$ for a family of distributions $\{f(x|\theta): \theta \in \Theta\}$ is \textbf{identifiable} if distinct values of $\theta$ correspond to distinct pdfs or pmfs. This is, if $\theta_1 \neq \theta_2\comma f(x|\theta_1)  \neq f(x|\theta_2) \blank \forall x$. \\

\end{df}

Identifiability is a property of the model, not of an estimator or estimation procedure. If the model is not identifiable, there is difficulty in doing inference, eg. if $f(x|\theta_1) = f(x|\theta_2)$, then observations from both distributions will look exactly the same and we would have no way of knowing whichever value was correct. In particular, the likelihood of both values would be equal. Problems con identifiability can usually be solved by redefining the model. In a general overparameterized model, there are $k+1$ values parameters $(\mu, \tau_{1}, \cdots, \tau_k) \in \bm{\Theta} = \Theta_{\mu} \times \bigtimes\limits_{i=1}^k \Theta_{\tau_{i}}$ but only $k$ means, $\{\E[Y_{ij}]\}_{i=1}^{k}$. Without any further restriction on the parameters, more than one set of values $(\mu, \tau_{1}, \cdots, \tau_k)$ will lead to the same distribution. Therefore, it's common in this model to add the restriction that $\sum_{i=1}^{k} \tau_k = 0$, which effectively reduces the number of parameters to $k$ and makes the model identifiable. The restriction also has the effect of giving the $\{\tau_i\}_{i=1}^k$ an interpretation as deviations from an overall mean level. In oneway ANOVA, it's more common to use the \textit{cell means model}, given by \eqref{std linear mod}. \\

\subsection{Model and distribution assumptions}

Under model \eqref{std linear mod}, the minimum assumptions are

\begin{itemize}
    \item $\E (\epsilon_{ij}) = 0$
    \item $\var (\epsilon_{ij}) < \infty$, for all $i,j$. 
\end{itemize}

However to do any confidence interval estimation or testing, we need distributional assumptions. Here are the classical ANOVA assumptions: \\

\paragraph{\textbf{Oneway ANOVA assumptions}}

Random variables $Y_{ij}$ are observed according to model \eqref{std linear mod}, where

\begin{itemize}
    \item $\E (\epsilon_{ij}) = 0$, $\var (\epsilon_{ij}) < \infty$, for all $i,j$ and $\cov(\epsilon_{ij}, \epsilon_{mn}) \propto \delta_{im}\delta_{jn}$. 
    \item The $\epsilon_{ij}$ are independent and normally distributed.
    \item $\sigma_i^2 = \sigma^2$ for all $i$, this is called \textit{homoscedasticity}. \\
\end{itemize}

Note that without assumption (II) we could only do point estimation and possibly look estimators that minimize variance within a class, but we could not do interval estimation nor testing. Instead, if we assume some other error distribution other than the normal, intervals and tests can be quite difficult to derive. The homoscedastic condition is also quite important. \\

\paragraph{\textbf{Classic ANOVA Hypothesis}:}

The classic ANOVA test is a test of the null hypothesis

$$
H_0 : \theta_1 = \theta_2 = \cdots = \theta_k,
$$

a hypothesis which may be uninteresting in many settings. The alternative hypothesis is 

$$
H_1 : \theta_i \neq \theta_j \textnormal{ for some } i,j .
$$

Equivalently, we can specify $H_1$ as $H_1: \textnormal{ not } H_0$. Either way, note that if $H_0$ is rejected, we can conclude only that there is some difference amongst the $\theta_i$s, but we cannot infer as to where the difference might be. One problem with ANOVA hypothesis is that the interpretation of hypothesis is not easy. What would be more useful than concluding that some $\theta_i$s are different is a statistical description of the  $\theta_i$s. Such a description can be readily found by breaking down the ANOVA hypothesis into smaller, more easily describable pieces. One way to implement this is by using an intersection-union test\footnote{Let

$$
\prob_{\theta}({\bf X} \in R) = \left\{ \begin{array}{cc}
    \textnormal{probability of a Type I error } & \textnormal{ if } \theta \in \Theta_0 \\
    \textnormal{one minus the probability of a Type II error } & \textnormal{ if } \theta \in \Theta_0^c  
\end{array} \right.
$$

The previous function is none other than the power function of a hypothesis test with rejection region $R$. Consider the following theorem: 

\begin{theo}
Consider testing $H_0: \theta \in \cup_{j=1}^k \Theta_j$. For each $j = 1, \cdots, k$, let $R_j$ be the rejection region of an $\alpha$-level test of $H_{0j}$. Suppose that for some $i = 1, \cdots, k$, there exists a sequence of parameters points, $\theta_l \in \Theta_i$, $l = 1,2, \cdots$, such that 

\begin{itemize}
    \item $\lim_{l \rightarrow \infty} \prob_{\theta_l}(\X \in R_i) = \alpha$,
    \item for each $j=1, \cdots, k$ and $j \neq i, \lim_{l \in \infty} \prob_{\theta_l}(\X \in R_i) = 1$.
\end{itemize}

Then the intersection-union test with a rejection region $R = \cap_{j=1}^k R_j$ is a size $\alpha$-test. 
\end{theo}

\begin{proof}
Let $\alpha_{\gamma}$ be the size of the $\gamma$-th null hypothesis, $H_{0\gamma}$ with rejection region $R_\gamma$. Then the intersection-union test with rejection region $R = \cap_{\gamma \in \Gamma}$ has a $\Tilde{\alpha}$-level, with $\Tilde{\alpha} = \sup_{\gamma \in \Gamma} \alpha_\gamma$. \\

In effect, let $\theta \in \Theta_0$ be arbitrary. Then, $\exists \gamma \in \Gamma \blank | \blank \theta \in \Theta_\gamma $ such that 

$$
\prob_{\theta}({\bf X} \in R) \leq P_{\theta}({\bf X} \in R_{\gamma}) \leq \alpha_{\gamma} \leq \alpha,
$$

thus the IUT is an $\alpha$-level test. 

Then, in our context, R is an $\alpha$-level test. By definition, this implies 

$$
\sup_{\theta \in \Theta_0} \prob_{\theta} (\X \in R) \leq \alpha.
$$

But, because all parameter points $\theta_l$ satisfy $\theta_l \in \Theta_i \subset \Theta_0$, then

\begin{align*}
    \sup_{\theta \in \Theta_0} \prob_{\theta} (\X \in R) &\geq \lim_{l \rightarrow \infty} \prob_{\theta_l}(\X \in R) \\
    & = \lim_{l \rightarrow \infty} \prob_{\theta_l}\bigg(\X \in \cap_{j=1}^k R_j\bigg) \\
    &\geq \lim_{l \rightarrow \infty}
    \sum_{j=1}^{k}  \prob_{\theta_l}(\X \in R_j) - (k-1) \textnormal{ according to Bonferroni's inequality.} \\
    &= (k-1)  + \alpha - (k-1) = \alpha
\end{align*}
\end{proof}}, which is the best suited method as the ANOVA null can be thought as the inteersection of univariate hypotheses, expressed in terms of \textit{contrasts}. 

\begin{df}

Let ${\bf t} =\{t_i\}_{i=1}^k$ be a set of variables, either parameters or statistics, and let ${\bf a} =\{a_i\}_{i=1}^k$ be known constants. Then the function 

$$
\sum_{i=1}^k a_i t_i 
$$

is a linear combination of them. If, furthermore, $\sum_{i} a_i = 0$, it is called a \textit{contrast}. \\
\end{df}

Constrasts are important because they can be used to compare treatment means, eg, if we have means $\{\theta_i\}_{i=1}^k$ and constants ${\bf a} = (1, -1, 0, \cdots, 0)$, then 

$$
\sum_{i=1}^{k} a_i \theta_i = \theta_1 - \theta_2,
$$

which is a contrast that compares $\theta_1$ to $\theta_2$. The power of the union approach relies on its increased understanding. The individual null hypotheses, of which the ANOVA null hypothesis is the intersection, are readily seen. 

\begin{theo}
Let $\theta = \{\theta_i\}_{i=1}^{k}$ be arbitrary parameters. Then, 

$$
\theta_1 = \theta_2 = \cdots = \theta_k \Leftrightarrow \sum_{i=1}^{k} a_i \theta_i = 0 \blank \forall {\bf a} \in \mathcal{A},
$$

where $\mathcal{A}$ is the set of constants satisfying $\mathcal{A} = \{{\bf a} = \{a_i\}_{i=1}^{k} | \sum_{i} a_i = 0\}$. \\
\end{theo}

\begin{proof}
If $\theta_1 = \theta_2 = \cdots = \theta_k$ then 

$$
\sum_{i=1}^{k} a_i \theta_i  = \sum_{i=1}^{k} a_i \theta = \theta \sum_{i=1}^{k} a_i = 0,
$$

since $\sum_{i} a_i = 0$, which proves the left-to-right implication. To prove the right-to-left implication, consider the following basis for $\mathcal{A}$

$$
\mathcal{B}(\mathcal{A}) = \{a_i\}_{i=1}^{k-1} \blank | \blank (a_i)_j = \left\{ \begin{array}{cc}
    1 & \textnormal{ if } j = i  \\
    -1 & \textnormal{ if } j = i+1 \\
    0 & \textnormal{ otherwise}.
\end{array} \right. 
$$

Since it's a basis, we can form contrasts with these ${\bf a_i}$s: 

$$
a_1 \Rightarrow \theta_1 = \theta_2 \land a_2 \Rightarrow \theta_2 = \theta_3, \cdots a_{k-1} \Rightarrow \theta_{k-1} = \theta_k
$$

which, taken together, prove the theorem. 
\end{proof}

Therefore, the ANOVA null can then be expressed as a hypothesis about contrasts. That is, the null hypothesis is true iff the hypothesis 

$$
H_0: \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ for all } {\bf a} \textnormal{ such that } \sum_{i=1}^{k} a_i = 0,
$$

is true. Moreover, if $H_0$ is false, we now know that there must be at least one nonzero contrasts. That is, the ANOVA alternative, $H_1$: not all $\theta_i$s equal, is equivalent to the alternative 

$$
H_1: \blank \sum_{i=1}^{k} a_i \theta_i \neq 0 \textnormal{ for all } {\bf a} \textnormal{ such that } \sum_{i=1}^{k} a_i = 0. \\
$$

\blank \\

\paragraph{\textbf{Inferences regarding Linear combinations of Means}}

Linear combinations, in particular of contrasts, play an extremely important role in the analysis of variance. In the previous section, we proved that the ANOVA null can be decoupled into statements about constrasts. This is also true in general, more elaborate statistical hypothesis tests. We start first by discussing a single linear combination. \\

Working under oneway ANOVA assumptions, we have that

$$
Y_{ij} \sim \N(\theta_i, \sigma^2), \begin{array}{c}
     i = 1, \cdots, k  \\
     j = 1, \cdots, n_i 
\end{array},
$$

therefore 

$$
\Bar{Y}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} Y_{ij} \sim \N\bigg(\theta_i, \frac{\sigma^2}{n_i}\bigg), \blank i = 1, \cdots, k.
$$

For any constants ${\bf a} \in \R^{k}$, we have ${\bf a}^{\textnormal{T}} \bar{{\bf Y}}$ is also normal with 

\begin{align*}
    \E\bigg({\bf a}^{\textnormal{T}} \bar{{\bf Y}}\bigg) &= \sum_{i=1}^{k} a_i \theta_i, & \var \bigg({\bf a}^{\textnormal{T}} \bar{{\bf Y}}\bigg) &= \sigma^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}. 
\end{align*}

Furthermore, 

$$
\frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{\sigma^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \sim \N(0, 1).
$$

In practice, we'd like to make inferences about the $\theta_i$s without knowledge of $\sigma$. Therefore, we need to replace $\sigma$ with an estimate, the sample variance $S_i^2$ of the $i-$th population, defined as follows,

$$
S_i^2 = \frac{1}{n_i - 1} \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2\textnormal{, } i=1, \cdots,k,  
$$

which is an estimate for $\sigma^2$ with a \chidis, $\frac{(n_i-1)S^2_i}{\sigma^2} \sim \chi^2_{n_i-1}$. Furthermore, under the ANOVA assumptions, since each one of the $S_i^2$ estimates the same $\sigma^2$, we can improve the estimate by combining them into a \textit{pooled variance estimator} $\hat{S}^2_p$, of $\sigma^2$, defined as

$$
\hat{S}^2_p = \frac{1}{N-k} \sum_{i=1}^{k} (n_i-1)S_i^2 = \frac{1}{N-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y}_i)^2.
$$

Note that $N-k =\sum_{i} (n_i - 1)$. Now, since the $S_i^2$ are independent, then $\frac{(N-k)S^2_p}{\sigma^2} \sim \chi^2_{N-k}$. Also, $S_p^2$ is independent of each $\bar{Y}_i$, then

$$
\frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \sim t_{N-k},
$$

a Student's \tdis with $N-k$ degrees of freedom. \\

Now, consider the null hypothesis, which we'd like to test

$$
H_0 : \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ versus } H_1 : \blank \sum_{i=1}^{k} a_i \theta_i \neq 0,
$$

at $\alpha$-level, therefore the rejection region $R$ for the null hypothesis is 

$$
\bigg|\frac{\sum_{i=1}^{k} a_i \bar{Y}_i}{{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}}}\bigg| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

Furthermore, this can be inverted by the means of a pivot, the $\frac{\alpha}{2}-$th level quantile of the \tdis with $N-k$ degrees of freedom. With probability $1-\alpha$, we can assert 
\begin{align*}
    \sum_{i=1}^{k} a_i \bar{Y}_i - t_{N-k, \blank \frac{\alpha}{2}} \sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}} &\leq \sum_{i=1}^{k} a_i \theta_i \\
    &\leq \sum_{i=1}^{k} a_i \bar{Y}_i + t_{N-k, \blank \frac{\alpha}{2}} \sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}.
\end{align*}

\begin{tcolorbox}[title=Example of ANOVA contrasts]

Special values of ${\bf a}$ will yield particular tests or confidence intervals. For example, to compare treatment 1 and treatment 2 in a clinical exam trial, we can take ${\bf a} = (1, -1, 0, \cdots, 0)$. Then, using the previous results, to test $H_0: \blank \theta_1=\theta_2$ versus $H_1: \blank \theta_1 \neq \theta_2$, we would reject if 

$$
\left|{\frac{\bar{Y}_1 - \bar{Y}_2}{{\sqrt{S_p^2 \bigg(\frac{1}{n_1}+\frac{1}{n_2}\bigg)}}}}\right| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

Note that the difference between this test and a two-sample $t$-test is that here information from treatments $3,\cdots,k$, as well as treatments 1 and 2, is used to estimate $\sigma^2$. \\

Alternatively, to compare treatment 1 to the average of treatments $2$ and 3 (eg. treatment 1 might be a control, 2 and 3 might be experimental treatments, and we are looking for some overall effect), we would take ${\bf a} = (1, -\frac{1}{2}, -\frac{1}{2}, 0, \cdots, 0)$ and reject $H_0: \blank \theta_1 = \frac{1}{2}(\theta_2+\theta_3)$ if 

$$
\left|{\frac{\bar{Y}_1 - \frac{1}{2} \bar{Y}_2 - \frac{1}{2} \bar{Y}_3}{{\sqrt{S_p^2 \bigg(\frac{1}{n_1}+\frac{1}{4n_2}+\frac{1}{4n_3}\bigg)}}}}\right| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

\blank \\

In general, we have a framework for testing and estimating linear combinations of random variables in the ANOVA. By judiciously choosing our linear combination we can learn much about the treatment means. For example, if we look at the contrasts $\theta_1-\theta_2$, $\theta_2-\theta_3$ and $\theta_1-\theta_3$, we can learn something about the ordering of the $\theta_i$s. Some care must be taken when drawing formal conclusions 
\end{tcolorbox}

\blank \\

\end{document}
