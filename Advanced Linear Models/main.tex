\documentclass{homework}
\author{Tomás Pérez}
\class{Advanced Linear Methods - Lecture Notes}
\date{\today}
\title{Theory \& Notes}

\graphicspath{{./media/}}

\begin{document} \maketitle

\section{Analysis of Variance and Regression}

In a wide variety of settings, it's useful to model a random variable with a pdf or a pmf which depends on some parameters to be estimated. In many situations, a random variable can be modeled not only with unknown parameters but also with known, an sometimes controllable covariates. The standard methodology for analysis of variance (ANOVA)and regression analysis rely on a linear relationship among random variables. A basic idea of ANOVA consists on partitioning variations and analysing variations in means. \\

\subsection{\textbf{Simple Linear Regression:}}

A major purpose of regression is to explore the dependence of one variable on others. In simple linear regression, the mean of a random variable, $\Y$, is modelled as a function of another observable variable $\X$, by the relationship $\E[\Y] = \alpha + \beta \X$. In general, the function that gives $\E[\Y]$ as a function of $\X$ is called the \textit{population regression function}. \\

\subsection{\textbf{Oneway Analysis of Variance:}}

In its simplest form, ANOVA is a method of estimating the means of several populations, which are often assumed to be normally distributed. The heart of ANOVA lies on the topic of statistical design. How can we get the most information on the populations with the fewest observations? Classic ANOVA had testing as its main goal, in particular testing what is known as the ANOVA null hypothesis. However, in many settings it's not sufficient to test only one hypothesis in order to make good experimental inference. \\

In the oneway analysis of variance, also known as oneway classification, we assume that the data $Y_{ij}$ are observed according to a model:

$$
Y_{ij} = \theta_i + \epsilon_{ij} \textnormal{ where } i = 1, \cdots, k \textnormal{, } j = 1, \cdots, n_i,
$$

where the $\theta_i$ are unknown parameters and where the $\epsilon_{ij}$ are error random variables. Without loss of generality, we can assume $\E \epsilon_{ij} = 0$, since if not, we can re-scale the $\epsilon_{ij}$ and absorb the leftover mean into the $\theta_i$'s. Then 

$$
\E[Y_{ij}] = \theta_i \textnormal{ where } j = 1, \cdots, n_i.
$$

Therefore the $\{\theta_i\}_{i=1}^{k}$ are the means of the $\{Y_{ij}\}_{i=1\textnormal{, } j=1}^{k\textnormal{, } n_i}$. The first ones are the \textit{treatment means} (since the index usually correspond to different treatments or to levels of a particular treatment, such as dossage levels of a particular drug)\footnote{There is an alternative model, called the \textit{overparameterized model}, which can be written as 

\begin{equation}
Y_{ij} = \mu + \tau_i + \epsilon_{ij} \textnormal{ where } i = 1, \cdots, k \textnormal{, } j = 1, \cdots, n_i,
\label{std linear mod}
\end{equation}

where, again, $\E \epsilon_{ij} = 0$. In this setting, then 

$$
\E[Y_{ij}] = \mu+\theta_i \textnormal{ where } j = 1, \cdots, n_i.
$$

In this formulation, we think of $\mu$ as a grand mean, that is the common mean level of the treatments. The parameters $\tau_i$ then denote the unique effect due to the $i$-th treatment. However, the $\tau_i$ and $\mu$ parameters cannot be estimated independently due to problems with \textit{identifiability}. 
}. \\

\begin{df}

A parameter $\theta$ for a family of distributions $\{f(x|\theta): \theta \in \Theta\}$ is \textbf{identifiable} if distinct values of $\theta$ correspond to distinct pdfs or pmfs. This is, if $\theta_1 \neq \theta_2\comma f(x|\theta_1)  \neq f(x|\theta_2) \blank \forall x$. \\

\end{df}

Identifiability is a property of the model, not of an estimator or estimation procedure. If the model is not identifiable, there is difficulty in doing inference, eg. if $f(x|\theta_1) = f(x|\theta_2)$, then observations from both distributions will look exactly the same and we would have no way of knowing whichever value was correct. In particular, the likelihood of both values would be equal. Problems con identifiability can usually be solved by redefining the model. In a general overparameterized model, there are $k+1$ values parameters $(\mu, \tau_{1}, \cdots, \tau_k) \in \bm{\Theta} = \Theta_{\mu} \times \bigtimes\limits_{i=1}^k \Theta_{\tau_{i}}$ but only $k$ means, $\{\E[Y_{ij}]\}_{i=1}^{k}$. Without any further restriction on the parameters, more than one set of values $(\mu, \tau_{1}, \cdots, \tau_k)$ will lead to the same distribution. Therefore, it's common in this model to add the restriction that $\sum_{i=1}^{k} \tau_k = 0$, which effectively reduces the number of parameters to $k$ and makes the model identifiable. The restriction also has the effect of giving the $\{\tau_i\}_{i=1}^k$ an interpretation as deviations from an overall mean level. In oneway ANOVA, it's more common to use the \textit{cell means model}, given by \eqref{std linear mod}. \\

\paragraph{\textbf{Model and distribution assumptions}}

Under model \eqref{std linear mod}, the minimum assumptions are

\begin{itemize}
    \item $\E (\epsilon_{ij}) = 0$
    \item $\var (\epsilon_{ij}) < \infty$, for all $i,j$. 
\end{itemize}

However to do any confidence interval estimation or testing, we need distributional assumptions. Here are the classical ANOVA assumptions: \\

\paragraph{\textbf{Oneway ANOVA assumptions}}

Random variables $Y_{ij}$ are observed according to model \eqref{std linear mod}, where

\begin{itemize}
    \item $\E (\epsilon_{ij}) = 0$, $\var (\epsilon_{ij}) < \infty$, for all $i,j$ and $\cov(\epsilon_{ij}, \epsilon_{mn}) \propto \delta_{im}\delta_{jn}$. 
    \item The $\epsilon_{ij}$ are independent and normally distributed.
    \item $\sigma_i^2 = \sigma^2$ for all $i$, this is called \textit{homoscedasticity}. \\
\end{itemize}

Note that without assumption (II) we could only do point estimation and possibly look estimators that minimize variance within a class, but we could not do interval estimation nor testing. Instead, if we assume some other error distribution other than the normal, intervals and tests can be quite difficult to derive. The homoscedastic condition is also quite important. \\

\paragraph{\textbf{Classic ANOVA Hypothesis}:}

The classic ANOVA test is a test of the null hypothesis

$$
H_0 : \theta_1 = \theta_2 = \cdots = \theta_k,
$$

a hypothesis which may be uninteresting in many settings. The alternative hypothesis is 

$$
H_1 : \theta_i \neq \theta_j \textnormal{ for some } i,j .
$$

Equivalently, we can specify $H_1$ as $H_1: \textnormal{ not } H_0$. Either way, note that if $H_0$ is rejected, we can conclude only that there is some difference amongst the $\theta_i$s, but we cannot infer as to where the difference might be. One problem with ANOVA hypothesis is that the interpretation of hypothesis is not easy. What would be more useful than concluding that some $\theta_i$s are different is a statistical description of the  $\theta_i$s. Such a description can be readily found by breaking down the ANOVA hypothesis into smaller, more easily describable pieces. One way to implement this is by using an intersection-union test\footnote{Let

$$
\prob_{\theta}({\bf X} \in R) = \left\{ \begin{array}{cc}
    \textnormal{probability of a Type I error } & \textnormal{ if } \theta \in \Theta_0 \\
    \textnormal{one minus the probability of a Type II error } & \textnormal{ if } \theta \in \Theta_0^c  
\end{array} \right.
$$

The previous function is none other than the power function of a hypothesis test with rejection region $R$. Consider the following theorem: 

\begin{theo}
Consider testing $H_0: \theta \in \cup_{j=1}^k \Theta_j$. For each $j = 1, \cdots, k$, let $R_j$ be the rejection region of an $\alpha$-level test of $H_{0j}$. Suppose that for some $i = 1, \cdots, k$, there exists a sequence of parameters points, $\theta_l \in \Theta_i$, $l = 1,2, \cdots$, such that 

\begin{itemize}
    \item $\lim_{l \rightarrow \infty} \prob_{\theta_l}(\X \in R_i) = \alpha$,
    \item for each $j=1, \cdots, k$ and $j \neq i, \lim_{l \in \infty} \prob_{\theta_l}(\X \in R_i) = 1$.
\end{itemize}

Then the intersection-union test with a rejection region $R = \cap_{j=1}^k R_j$ is a size $\alpha$-test. 
\end{theo}

\begin{proof}
Let $\alpha_{\gamma}$ be the size of the $\gamma$-th null hypothesis, $H_{0\gamma}$ with rejection region $R_\gamma$. Then the intersection-union test with rejection region $R = \cap_{\gamma \in \Gamma}$ has a $\Tilde{\alpha}$-level, with $\Tilde{\alpha} = \sup_{\gamma \in \Gamma} \alpha_\gamma$. \\

In effect, let $\theta \in \Theta_0$ be arbitrary. Then, $\exists \gamma \in \Gamma \blank | \blank \theta \in \Theta_\gamma $ such that 

$$
\prob_{\theta}({\bf X} \in R) \leq P_{\theta}({\bf X} \in R_{\gamma}) \leq \alpha_{\gamma} \leq \alpha,
$$

thus the IUT is an $\alpha$-level test. 

Then, in our context, R is an $\alpha$-level test. By definition, this implies 

$$
\sup_{\theta \in \Theta_0} \prob_{\theta} (\X \in R) \leq \alpha.
$$

But, because all parameter points $\theta_l$ satisfy $\theta_l \in \Theta_i \subset \Theta_0$, then

\begin{align*}
    \sup_{\theta \in \Theta_0} \prob_{\theta} (\X \in R) &\geq \lim_{l \rightarrow \infty} \prob_{\theta_l}(\X \in R) \\
    & = \lim_{l \rightarrow \infty} \prob_{\theta_l}\bigg(\X \in \cap_{j=1}^k R_j\bigg) \\
    &\geq \lim_{l \rightarrow \infty}
    \sum_{j=1}^{k}  \prob_{\theta_l}(\X \in R_j) - (k-1) \textnormal{ according to Bonferroni's inequality.} \\
    &= (k-1)  + \alpha - (k-1) = \alpha
\end{align*}
\end{proof}}, which is the best suited method as the ANOVA null can be thought as the inteersection of univariate hypotheses, expressed in terms of \textit{contrasts}. 

\begin{df}

Let ${\bf t} =\{t_i\}_{i=1}^k$ be a set of variables, either parameters or statistics, and let ${\bf a} =\{a_i\}_{i=1}^k$ be known constants. Then the function 

$$
\sum_{i=1}^k a_i t_i 
$$

is a linear combination of them. If, furthermore, $\sum_{i} a_i = 0$, it is called a \textit{contrast}. \\
\end{df}

Constrasts are important because they can be used to compare treatment means, eg, if we have means $\{\theta_i\}_{i=1}^k$ and constants ${\bf a} = (1, -1, 0, \cdots, 0)$, then 

$$
\sum_{i=1}^{k} a_i \theta_i = \theta_1 - \theta_2,
$$

which is a contrast that compares $\theta_1$ to $\theta_2$. The power of the union approach relies on its increased understanding. The individual null hypotheses, of which the ANOVA null hypothesis is the intersection, are readily seen. 

\begin{theo}
Let $\theta = \{\theta_i\}_{i=1}^{k}$ be arbitrary parameters. Then, 

$$
\theta_1 = \theta_2 = \cdots = \theta_k \Leftrightarrow \sum_{i=1}^{k} a_i \theta_i = 0 \blank \forall {\bf a} \in \mathcal{A},
$$

where $\mathcal{A}$ is the set of constants satisfying $\mathcal{A} = \{{\bf a} = \{a_i\}_{i=1}^{k} | \sum_{i} a_i = 0\}$. \\
\end{theo}

\begin{proof}
If $\theta_1 = \theta_2 = \cdots = \theta_k$ then 

$$
\sum_{i=1}^{k} a_i \theta_i  = \sum_{i=1}^{k} a_i \theta = \theta \sum_{i=1}^{k} a_i = 0,
$$

since $\sum_{i} a_i = 0$, which proves the left-to-right implication. To prove the right-to-left implication, consider the following basis for $\mathcal{A}$

$$
\mathcal{B}(\mathcal{A}) = \{a_i\}_{i=1}^{k-1} \blank | \blank (a_i)_j = \left\{ \begin{array}{cc}
    1 & \textnormal{ if } j = i  \\
    -1 & \textnormal{ if } j = i+1 \\
    0 & \textnormal{ otherwise}.
\end{array} \right. 
$$

Since it's a basis, we can form contrasts with these ${\bf a_i}$s: 

$$
a_1 \Rightarrow \theta_1 = \theta_2 \land a_2 \Rightarrow \theta_2 = \theta_3, \cdots a_{k-1} \Rightarrow \theta_{k-1} = \theta_k
$$

which, taken together, prove the theorem. 
\end{proof}

Therefore, the ANOVA null can then be expressed as a hypothesis about contrasts. That is, the null hypothesis is true iff the hypothesis 

$$
H_0: \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ for all } {\bf a} \textnormal{ such that } \sum_{i=1}^{k} a_i = 0,
$$

is true. Moreover, if $H_0$ is false, we now know that there must be at least one nonzero contrasts. That is, the ANOVA alternative, $H_1$: not all $\theta_i$s equal, is equivalent to the alternative 

$$
H_1: \blank \sum_{i=1}^{k} a_i \theta_i \neq 0 \textnormal{ for all } {\bf a} \textnormal{ such that } \sum_{i=1}^{k} a_i = 0. \\
$$

\blank \\

\paragraph{\textbf{Inferences regarding Linear combinations of Means}}

Linear combinations, in particular of contrasts, play an extremely important role in the analysis of variance. In the previous section, we proved that the ANOVA null can be decoupled into statements about constrasts. This is also true in general, more elaborate statistical hypothesis tests. We start first by discussing a single linear combination. \\

Working under oneway ANOVA assumptions, we have that

$$
Y_{ij} \sim \N(\theta_i, \sigma^2), \begin{array}{c}
     i = 1, \cdots, k  \\
     j = 1, \cdots, n_i 
\end{array},
$$

therefore 

$$
\Bar{Y}_i = \frac{1}{n_i}\sum_{j=1}^{n_i} Y_{ij} \sim \N\bigg(\theta_i, \frac{\sigma^2}{n_i}\bigg), \blank i = 1, \cdots, k.
$$

For any constants ${\bf a} \in \R^{k}$, we have ${\bf a}^{\textnormal{T}} \bar{{\bf Y}}$ is also normal with 

\begin{align*}
    \E\bigg({\bf a}^{\textnormal{T}} \bar{{\bf Y}}\bigg) &= \sum_{i=1}^{k} a_i \theta_i, & \var \bigg({\bf a}^{\textnormal{T}} \bar{{\bf Y}}\bigg) &= \sigma^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}. 
\end{align*}

Furthermore, 

$$
\frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{\sigma^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \sim \N(0, 1).
$$

In practice, we'd like to make inferences about the $\theta_i$s without knowledge of $\sigma$. Therefore, we need to replace $\sigma$ with an estimate, the sample variance $S_i^2$ of the $i-$th population, defined as follows,

$$
S_i^2 = \frac{1}{n_i - 1} \sum_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2\textnormal{, } i=1, \cdots,k,  
$$

which is an estimate for $\sigma^2$ with a \chidis, $\frac{(n_i-1)S^2_i}{\sigma^2} \sim \chi^2_{n_i-1}$. Furthermore, under the ANOVA assumptions, since each one of the $S_i^2$ estimates the same $\sigma^2$, we can improve the estimate by combining them into a \textit{pooled variance estimator} $\hat{S}^2_p$, of $\sigma^2$, defined as

$$
\hat{S}^2_p = \frac{1}{N-k} \sum_{i=1}^{k} (n_i-1)S_i^2 = \frac{1}{N-k} \sum_{i=1}^{k} \sum_{j=1}^{n_i} (Y_{ij}-\bar{Y}_i)^2.
$$

Note that $N-k =\sum_{i} (n_i - 1)$. Now, since the $S_i^2$ are independent, then $\frac{(N-k)S^2_p}{\sigma^2} \sim \chi^2_{N-k}$. Also, $S_p^2$ is independent of each $\bar{Y}_i$, then

$$
\frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \sim t_{N-k},
$$

a Student's \tdis with $N-k$ degrees of freedom. \\

Now, consider the null hypothesis, which we'd like to test

$$
H_0 : \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ versus } H_1 : \blank \sum_{i=1}^{k} a_i \theta_i \neq 0,
$$

at $\alpha$-level, therefore the rejection region $R$ for the null hypothesis is 

$$
\bigg|\frac{\sum_{i=1}^{k} a_i \bar{Y}_i}{{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}}}\bigg| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

Furthermore, this can be inverted by the means of a pivot, the $\frac{\alpha}{2}-$th level quantile of the \tdis with $N-k$ degrees of freedom. With probability $1-\alpha$, we can assert 
\begin{align*}
    \sum_{i=1}^{k} a_i \bar{Y}_i - t_{N-k, \blank \frac{\alpha}{2}} \sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}} &\leq \sum_{i=1}^{k} a_i \theta_i \\
    &\leq \sum_{i=1}^{k} a_i \bar{Y}_i + t_{N-k, \blank \frac{\alpha}{2}} \sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}.
\end{align*}

\begin{tcolorbox}[title=Example of ANOVA contrasts]

Special values of ${\bf a}$ will yield particular tests or confidence intervals. For example, to compare treatment 1 and treatment 2 in a clinical exam trial, we can take ${\bf a} = (1, -1, 0, \cdots, 0)$. Then, using the previous results, to test $H_0: \blank \theta_1=\theta_2$ versus $H_1: \blank \theta_1 \neq \theta_2$, we would reject if 

$$
\left|{\frac{\bar{Y}_1 - \bar{Y}_2}{{\sqrt{S_p^2 \bigg(\frac{1}{n_1}+\frac{1}{n_2}\bigg)}}}}\right| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

Note that the difference between this test and a two-sample $t$-test is that here information from treatments $3,\cdots,k$, as well as treatments 1 and 2, is used to estimate $\sigma^2$. \\

Alternatively, to compare treatment 1 to the average of treatments $2$ and 3 (eg. treatment 1 might be a control, 2 and 3 might be experimental treatments, and we are looking for some overall effect), we would take ${\bf a} = (1, -\frac{1}{2}, -\frac{1}{2}, 0, \cdots, 0)$ and reject $H_0: \blank \theta_1 = \frac{1}{2}(\theta_2+\theta_3)$ if 

$$
\left|{\frac{\bar{Y}_1 - \frac{1}{2} \bar{Y}_2 - \frac{1}{2} \bar{Y}_3}{{\sqrt{S_p^2 \bigg(\frac{1}{n_1}+\frac{1}{4n_2}+\frac{1}{4n_3}\bigg)}}}}\right| > t_{N-k, \blank \frac{\alpha}{2}}.
$$

\blank \\

In general, we have a framework for testing and estimating linear combinations of random variables in the ANOVA. By judiciously choosing our linear combination we can learn much about the treatment means. For example, if we look at the contrasts $\theta_1-\theta_2$, $\theta_2-\theta_3$ and $\theta_1-\theta_3$, we can learn something about the ordering of the $\theta_i$s. Some care must be taken when drawing formal conclusions from combinations of contrasts. For example, consider the hypotheses 

$$
H_0: \blank \theta_{1} = \frac{1}{2}(\theta_{2}+\theta_{3}) \textnormal{ versus } H_0: \blank \theta_{1} < \frac{1}{2}(\theta_{2}+\theta_{3}),
$$

and 

$$
H_0 : \blank \theta_2 = \theta_3 \textnormal{ versus } H_0 : \blank \theta_2 < \theta_3.
$$

If we reject both null hypotheses, we can conclude $\theta_3$ is greater than both $\theta_1$ and $\theta_2$, although we can draw no formal conclusion about the ordering of $\theta_1$ and $\theta_2$ from these two tests. 
\end{tcolorbox}

\blank \\

\paragraph{\textbf{The ANOVA \textit{F}-test}}

In the previous section we saw how to deal with single linear combinations of random variables, an in particular, contrasts in the ANOVA, and how the ANOVA null hypothesis is equivalent to a set of hypotheses about contrasts. Using this equivalence, together with the union-intersection method, it's possible to derive a test of the ANOVA hypothesis. \\

The ANOVA hypothesis test can be written as 

\begin{align*}
    H_0: \blank \forall {\bf a} \in \mathcal{A}, \blank \sum_{i=1}^{k} a_i \theta_i = 0 \textnormal{ versus } H_1: \blank \exists {\bf a} \in \mathcal{A}, \blank \sum_{i=1}^{k} a_i \theta_i \neq 0,
\end{align*}

where $\mathcal{A} = \{{\bf a} \in \R^{k} \blank | \blank \sum_{i=1}^{k} a_i = 0\}$. To see this more clearly as a union-intersection test, let's define the set $\Theta_a$ for each ${\bf a}$ as 

$$
\Theta_{\bf a} = \{\theta=(\theta_1, \cdots, \theta_k) \blank | \blank \sum_{i=1}^{k}a_i \theta_i = 0\},
$$

then we have 

\begin{align*}
    \theta \in \{\theta: \blank \theta_1 = \theta_2 = \cdots =  \theta_k \} \Leftrightarrow \theta \in \Theta_{\bf a} \blank \forall {\bf a} \in \mathcal{A} \Leftrightarrow \theta \in \cap_{{\bf a} \in \mathcal{A}} \Theta_{\bf a},
\end{align*}

showing that the ANOVA null can be written as an intersection. Now, recalling the union-intersection methodology, we will reject $H_0 : \theta \in \cap_{{\bf a} \in \mathcal{A}} \Theta_{\bf a}$ (and, hence the ANOVA null) if we can reject 

$$
H_{0_{\bf a}} : \theta \in \Theta_{\bf a} \textnormal{ versus } H_{1_{\bf a}} : \theta \notin \Theta_{\bf a}, \blank  \forall a \in \mathcal{A}.
$$

We test $H_{0_{\bf a}}$ with a $t$-statistic, 

$$
T_{\bf a} = \left| \frac{\sum_{i=1}^k a_i \bar{Y}_i - \sum_{i=1}^k a_i \theta_i}{\sqrt{S_p^2 \sum_{i=1}^{k} \frac{a_i^2}{n_i}}} \right|,
$$

therefore the rejection region is $T_{\bf a} > k$, in which case we'll reject $H_{\bf a}$, for some constant $k \in \R$. From the union-intersection methodology, it follows that if we could reject for any ${\bf a}$, we could reject for the ${\bf a}$ that maximizes $T_{\bf a}$. Thus, the union-intersection test of the ANOVA null is to reject $H_0$ if $\sup_{\bf a} T_{\bf a} > k$, where $k$ is chosen so that $P_{H_0} (\sup_{\bf a} T_{\bf a} > k) = \alpha$. It turns out that the calculation of $\sup_{\bf a} T_{\bf a} $ is not all straightforward. \\

\clearpage

\section{Simple Linear Regression}

In ANOVA, we studied how one factor, a random variable, influenced the means of a response variable. We now turn to understand the functional dependence of one variable on another. In particular, in the case of linear regression, we have a relationship of the form

\begin{equation}
{\bf Y}_i = \alpha + \beta {\bf X}_i + \epsilon_i,
\label{linear regression 1}
\end{equation}

where 

\begin{itemize}
    \item both ${\bf X}_i$ and ${\bf Y}_i$ are random variables, we call ${\bf X}_i$ as the \textit{predictor} variable while ${\bf Yequation}_i$ is the \textit{response} variable, 
    \item $\alpha$, $\beta$ are the intercept and the slope of the regression, and are assumed to be fixed and unknown parameters, 
    \item and where $\epsilon_i$ is a random variable, which can be assume to have $\E(\epsilon_i) = 0$. \\
\end{itemize}

Due to the imposed condition on the errors, we have 

\begin{equation}
\E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta x_i,
\label{linear regression 2}
\end{equation}

which is called the \textit{population regression function}. The main purpose of regression is to predict ${\bf Y}_i$ from knowledge of ${\bf X}_i$, using the population regression function. In this context, "regression" refers to a conditional expectation of ${\bf Y}$ given ${\bf X} = x$ and is used, in a statistical context, so signify a relationship between variables. When referring to a "linear regression", this implies that the conditional expectation of ${\bf Y}$ given ${\bf X}=x$ is a linear function of $x$. Note that in \eqref{linear regression 2}, it doesn't matter whether $x_i$ is fixed and known or it is a realization of the observable random variable ${\bf X}_i$, since in either case \eqref{linear regression 2} has the same interpretation. This isn't the case when the concern is to make inferences using the joint distribution of ${\bf X}_i$ and ${\bf Y}_i$. \\

The term \textit{linear regression} refers to a specification that is \textbf{linear in the parameters} ie. the specifications 

\begin{align*}
    \E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta x_i^2 \textnormal{ and } \E (\log {\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta \frac{1}{x_i},
\end{align*}

both specify linear regressions. The first one specifices a linear relationship between ${\bf Y}_i$ and $x_i^2$, whilst the second one between $\log{\bf Y}_i$ and $\frac{1}{x_i}$. In contrast, the specification $\E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta^2 x_i$ is not a linear regression. \\

In a simple linear regression problem, we observe data consisting of $n$ pairs of observation $\{(x_i, y_i)\}_{i=1}^n$, where there may be different underlying models for these data. The different models entail different assumptions about whether $x$ or $y$ or both are observed values of random variables ${\bf X}$ and ${\bf Y}$, respectively. In each model we'll be interested in investigating a linear relationship between $x$ and $y$. These $n$ data points will not fall exactly on a straight line but, rather, we're interested in summarizing the sample information by fitting a line to the observed data points. Given a data set $\{(x_i, y_i)\}_{i=1}^n$, we can define the following quantities of interest

\begin{align}
    \textnormal{The sample means } \begin{array}{c}
        \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i    \\
        \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i,   
    \end{array} & \textnormal{ and the sums of squares} \begin{array}{c}
       S_{xx} = \sum_{i=1}^{n} (x_i - \bar{x})^2  \\
       S_{yy} = \sum_{i=1}^{n} (y_i - \bar{y})^2   \\
       S_{xy} = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
    \end{array}.
\end{align}

Then the most common estimates of $\alpha$ and $\beta$ are 

\begin{align*}
    \hat{\beta} = \frac{S_{xy}}{S_{xx}} \textnormal{ and } \hat{\alpha} = \bar{y} - \hat{\beta} \bar{x}.
\end{align*}

\subsection{Least squares: A mathematical solution}

Our first derivation of estimates for $\alpha$ and $\beta$ makes no statistical assumptions about the observations  $\{(x_i, y_i)\}_{i=1}^n$. Simply consider a scatterplot of the dataset, for any line $y=c+dx$, the \textbf{residual sum of squares} (RSS) is defined to be 

\begin{equation}
    \textnormal{RSS } = \sum_{i=1}^{n} \left(y_i - (c+dx_i)^2\right), 
\end{equation}

which measures the vertical distance from each data point to the line $c+dx$, then summing the squares of these distances. The \textbf{least squares estimates} of $\alpha$ and $\beta$, 
$\hat{\alpha}$ and $\hat{\beta}$, are defined to be those values such that the line $\hat{\alpha} + \hat{\beta} x$ minimizes the RSS. That is, the least squares estimates satisfy

\begin{equation}
    \min_{c, \blank d \blank \in \R} \sum_{i=1}^{n} (y_i - (c+dx_i))^2 = \sum_{i=1}^{n} (y_i - (\hat{\alpha}+\hat{\beta}x_i))^2.
\end{equation}

The analytic minimization can be done in the following way. For any fixed value $d \in \R$, the value of $c$ which gives the minimum value can be found by writing 

$$
\sum_{i=1}^{n} (y_i - (c+dx_i))^2 = \sum_{i=1}^{n} ((y_i - dx_i) -c)^2.
$$

Then the minimizing value of $c$ is 

\begin{equation}
    c = \frac{1}{n} \sum_{i=1}^{n} (y_i - dx_i) = \bar{y} - d \bar{x}.
    \label{least squares 2}
\end{equation}

Thus, for a given value of $d$, the minimum value of RSS is 

\begin{equation}
    \sum_{i=1}^{n} \bigg((y_i - dx_i) - (\bar{y} - d \bar{x})\bigg)^2 = \sum_{i=1}^{n} \bigg((y_i - \bar{y}) - d(x_i - \bar{x})\bigg)^2 = S_{yy} - 2dS_{xy} + d^2 S_{xx}.
\end{equation}

The value of $d$ which gives the overall minimum value of RSS is obtained by setting the derivative of this quadratic function of $d$ equal to $0$. The minimizing value is then 

\begin{align}
    \frac{d}{d d}  S_{yy} - 2dS_{xy} + d^2 S_{xx} &= 0 \Rightarrow -2 S_{xy} + 2d S_{xx} = 0 \\
    \Rightarrow d = \frac{S_{xy}}{S_{xx}}.
    \label{least squares 1}
\end{align}

This value is, indeed, a minimum since the coefficient of $d^2$ is positive. Therefore, \eqref{least squares 2} and \eqref{least squares 1}, are indeed the values of $c$ and $d$ that minimize the residual sum of the squares. \\

The RSS is only one of the many reasonable ways of measuring the distance from the line $c+dx$ to the data points. For example, rather than using vertical distances we could use horizontal distances. This is equivalent to graphing the $y$ variable on the horizontal axis and the $x$ variabble on the vertical axis and using vertical distances as we did above. Using \eqref{least squares 2} and \eqref{least squares 1}, interchanging the roles of $x$ and $y$, we find the least squares lines is $\hat{x} = \hat{\alpha}' + \hat{\beta}'y$ where 

\begin{align*}
   \hat{\beta}' = \frac{S_{xy}}{S_{yy}} \textnormal{ and } \hat{\alpha}' = \bar{x} - \hat{\beta}'\bar{y},
\end{align*}

so that when reexpressing the line so that $y$ is a function of $x$, we obtain $\hat{y} = -\frac{\hat{\alpha}'}{\hat{\beta}'} + \frac{1}{\hat{\beta}' x}$. \\

It should be noted that least squares, as a method, is not statistical inference, since there's no way to construct confidence intervals nor testing hypothesis. In this context, the estimates are simply solutions of a minimization problem. It will turn out that, in some statistical models, these solutions have optimality properties. \\

\subsection{Best Linear Unbiased Estimators: A statistical solution}

In this section, we'll show that the estimates obtained in \eqref{least squares 2} and \eqref{least squares 1}, are optimal in the class of linear unbiased estimates under a general statistical model. Assume that the values $\{x_i\}_{i=1}^{n}$ are known, fixed values. The values $\{y_i\}_{i=1}^{n}$ are observed values of uncorrelated random variables $\{{\bf Y}_i\}_{i=1}^{n}$. The linear relationship assumed is given by \eqref{linear regression 2}, where we'll also asume $\var({\bf Y}_i) = \sigma^2, \blank \forall i \in \mathds{N}_{[1,n]}$. This is called the homoscedastic condition. The previous two assumptions are the only requirements for our model to work, eg. we need not any assumption on the probability distribution on the response variables. \\

To derive estimators for the parameters $\alpha$ and $\beta$, we restrict our attention to the class of linear estimators, ie. estimators which can be written as 

$$
\exists \{d_i\}_{i=1}^{n} \subset \mathds{R} \blank | \blank \sum_{i=1}^{n} d_i {\bf Y}_i.
$$

In particular, we'll restrict ourselves only to a subclass of linear estimators, the unbiased linear estimators. \\

An unbiased estimator of the slope $\beta$ must satisfy then 

$$
\exists \{d_i\}_{i=1}^{n} \subset \mathds{R} \blank | \blank  \E \bigg[\sum_{i=1}^{n} d_i {\bf Y}_i\bigg] = \hat{\beta},
$$

therefore 

\begin{align*}
    \hat{\beta} &= \E \bigg[\sum_{i=1}^{n} d_i {\bf Y}_i\bigg]  = \sum_{i=1}^{n} d_i \E({\bf Y}_i) = \sum_{i=1}^{n} d_i (\alpha + \beta x_i) \\
    &= \alpha \bigg(\sum_{i=1}^{n} d_i\bigg) + \beta \bigg(\sum_{i=1}^{n} d_i x_i\bigg).
\end{align*}

This equality is true for all $\alpha$ and $\beta$ if and only if 

\begin{align}
    \sum_{i=1}^{n} d_i = 0 \textnormal{ and } \sum_{i=1}^{n} d_i x_i = 1.
    \label{blue conditions}
\end{align}

Then, $\{d_i\}_{i=1}^{n} \in \R$ must satisfy \eqref{blue conditions} in order for the estimator $\hat{\beta}$ to be an unbiased estimator of $\beta$. We say a given estimator is the "best" if it has the smallest variance among all unbiased estimators. Similarly, an estimator is the \textbf{best linear unbiased estimator} (BLUE) if it is the linear unbiased estimator with the smallest variance. Since the  $\{{\bf Y}_i\}_{i=1}^{n}$ are uncorrelated with equal variance $\sigma^2$, the variance of any linear estimator if given by 

\begin{align*}
    \var \bigg[\sum_{i=1}^{n} d_i {\bf Y}_i\bigg]  = \sum_{i=1}^{n} d_i^2 \var({\bf Y}_i) = \sum_{i=1}^{n} d_i^2 \sigma^2 = \sigma^2 \sum_{i=1}^{n} d_i^2. 
\end{align*}

The BLUE of $\beta$ is, therefore, defined by a set of constants  $\{d_i\}_{i=1}^{n} \in \R$ which satisfy \eqref{blue conditions} and have the minimum value of $\sum_{i=1}^{n} d_i^2$, with $\sigma^2$ being an overall multiplicative constant. Let 

$$
k = n, \blank v_i = x_i, \blank c_i = 1 \textnormal{ and } a_i = d_i,
$$

which imply $\bar{v}_c = \bar{x}$. If $d_i$ is of the form 

\begin{equation}
    d_i = Kc_i ( v_i - \bar{v}_c) = K (x_i - \bar{x}) \textnormal{ with } i = 1, \cdots, n,
    \label{gram schmidt}
\end{equation}


then the set $\{d_i\}_{i=1}^{n} \in \R$ maximizes 

\begin{equation}
    \frac{\bigg(\sum_{i=1}^{n} d_i x_i\bigg)^2}{\sum_{i=1} d_i^2} \blank \forall \{d_i\}_{i=1}^{n} \in \R \blank | \blank \sum_{i} d_i = 0.
    \label{maximize functional}
\end{equation}

Furthermore, since 

$$
\{\{d_i\}_{i=1}^{n} \in \R: \blank \sum_{i}\ d_i \textnormal{ and } \sum_{i=1} d_i x_i = 1 \} \subset \{\{d_i\}_{i=1}^{n} \in \R : \blank \sum_{i} d_i = 0\},
$$

if the $d_i$s of the form \eqref{gram schmidt}, satisfy the \eqref{blue conditions} as well, then they certainly maximize 
\eqref{maximize functional} among the set of all real finite-sequences with the sum of its elements being zero: $\{\{d_i\}_{i=1}^{n} \in \R : \blank \sum_{i} d_i = 0\}$. Now, using \eqref{gram schmidt}, we have 

\begin{equation}
    \sum_{i=1}^{n} d_i x_i = \sum_{i=1}^{n} K(x_i - \bar{x}) x_i = K S_{xx}.
\end{equation}

Now, \eqref{blue conditions}'s second constraint is satisfied if $K = \frac{1}{K_{xx}}$. Therefore, we can force the first constraint by setting the $d_i$s as 

\begin{equation}
  \{d_i\}_{i=1}^{n} \in \R \Rightarrow d_i = \frac{x_i - \bar{x}}{S_{xx}} \blank \forall i = 1, \cdots, n,
  \label{contrasts estimators}
\end{equation}

which then produce a maximum. Note that for all finite subsets of real numbers which satisfy \eqref{blue conditions}, 

$$
\frac{\bigg(\sum_{i=1}^{n} d_i x_i\bigg)^2}{\sum_{i=1} d_i^2} = \frac{1}{\sum_{i=1} d_i^2}.
$$

Thus, for all $\{\{d_i\}_{i=1}^{n} \in \R : \blank \sum_{i} d_i = 0\}$ which satisfy \eqref{blue conditions}, maximization of \eqref{maximize functional} is equivalent to minimization of $\sum_{i=1}^{n} d_i^2$. Hence, we can conclude that the $d_i$s defined in \eqref{contrasts estimators}, give the minimum value of $\sum_{i=1}^{n} d_i^2$ among all $d_i$ which satisfy \eqref{blue conditions}, with the linear unbiased estimator defined by these $d_i$s, namely,

\begin{equation}
    \hat{\beta} = \sum_{i=1}^{n} \frac{x_i - \bar{x}}{S_{xx}} y_i = \frac{S_{xy}}{S_{yy}}.
\end{equation}

The variance of $\beta$ is 

\begin{equation}
    \var \beta = \sigma^2 \sum_{i=1}^{n} d_i^2 = \frac{\sigma^2}{S_{xx}} = \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2},
\end{equation}

note that, since the values $\{x_i\}_{i=1}^{n}$ are chosen by the experimenter, they can be chosen to make $S_{xx}$ large and the variance of the estimator small. That is, the experiment can be designed to make the estimator more precise. For example, if all the $\{x_i\}_{i=1}^{n}$ must be chosen so that they lie in the $\R_{[a,b]}$ and if $n$ is even, the choice of the $\{x_i\}_{i=1}^{n}$ which makes $S_{xx}$ as large as posible is to take half of the $x_i$s equal to $a$ and half equal to $b$. This would be the best design in that it would give the most precise estimate of the slope $\beta$. In practice, however, this design is seldom used since an experimenter is hardly ever certain of the model and this two-point design only gives information about the value of $\E ({\bf Y}_i\blank|\blank{\bf X}_i)$ at only two values. \\

A similar analysis will show that $\hat{\alpha}$ is the BLUE of the intercept $\alpha$, which must be written in terms of
$\{\{d_i\}_{i=1}^{n} \in \R$ which must satisfy 

\begin{align}
    \sum_{i=1}^{n} d_i = 1 \textnormal{ and } \sum_{i=1}^{n} d_i x_i = 0.
\end{align}

The fact that least squares estimators are BLUEs holds in other linear models as well. This result is called the Gauss-Markov theorem. \\

\subsection{Gauss-Markov Theorem}
Let 

\begin{equation}
    {\bf Y} = {\bf X} \bm{\beta} + \bm{\epsilon}, \textnormal{ where } \begin{array}{c}
         {\bf Y}, \bm{\epsilon} \in \R^{n}, \\
         \bm{\beta} \in \mathds{R}^{K}, \\
         {\bf X} \in \R^{n \times K}
    \end{array},
\end{equation}

where the $\beta_j$ are non-random but unobservable parameters, the $X_{ij}$ are non-random and observable (the explanatory variables), the $\epsilon_i$ are random (the errors), and thus the $Y_i$ are also random. Note that, should we desire to include a constant in the model above, one can choose to introduce the constant as a $\beta_{K+1}$ variable with a newly introduced last column of ${\bf X}$ being unity ie. ${\bf X}_{i(K+1)} = 1, \blank \forall i$\footnote{Note that although the ${\bf Y}_i$, as sample responses, are observable, the theorem holds only under the condition of knowing $\X_{ij}$, but not the $\Y_{ij}$.}. The Gauss-Markov assumptions concern a set of error random variables ${\bm \epsilon}$ with 

\begin{itemize}
    \item mean zero: $\E({\epsilon}_i) = 0$, 
    \item homoscedastic-compatible, having the same finite variance $\var({\epsilon}_i) = \sigma^2 < \infty, \blank \forall i$,
    \item and with distinct error terms being uncorrelated $\cov({\epsilon}_i, {\epsilon}_j) \propto \delta_{ij}$.
\end{itemize}

Then a linear estimator of $\beta_j$ is a linear combination 

$$
\hat{\beta_j} = \sum_{k = 1}^{n} c_{kj} {Y}_k = {\bf c}^{\textnormal{T}} {\bf Y},
$$

in which the coefficients $c_{ij}$ are not allowed to depend on the underlying coefficients $\beta_j$, since those are not observable, but are allowed to depend on the values $X_{ij}$, since these data are observable\footnote{Note that the dependence on the coefficients on each $X_{ij}$ is typically nonlinear, the estimator is linear in each $Y_i$, and hence in each random $\epsilon$.}. The estimator is said to be unbiased if and only if 
$$
\E(\hat{\beta_j}) = {\beta_j},
$$

regardless of the values of ${X}_{ij}$. Now, let $\sum_{j=1}^{K} \lambda_{j} \beta_{j}$ be some linear combination of the coefficients. Then the mean squared error of the corresponding estimation is 

$$
\E\bigg[\bigg(\sum_{j=1}^{K} \lambda_j (\hat{\beta_j} - {\beta_j})\bigg)^2\bigg].
$$

In other words, it's the expectation of the square of the weighted sum (across parameters) of the differences between the estimators and the corresponding parameters to be estimated. Note that since we're considering the case in which all the parameter estimates are unbiased, this mean squared error is the same as the variance of the linear combination. Then, the best linear unbiased estimator BLUE of the vector ${\bm \beta}$, of parameters $\beta_j$, is the one with the smallest mean squared error for every vector $\lambda \in \mathds{R}$ of linear combination parameters. This is equivalent to the condition that

$$
\var(\Tilde{{\bm \beta}}) - \var({{\bm \beta}}),
$$

is a positive semi-definite matrix for every other linear unbiased estimator $\Tilde{{\bm \beta}}$. Then, the \textbf{ordinary least squares estimator (OLS)} is then a matrix function 

$$
\hat{{\bm \beta}} = ({\bf X}^{\textnormal{T}} {\bf X})^{-1} {\bf X}^{\textnormal{T}} {\bf Y},
$$

which minimizes the sum of squares of residuals 

$$
\sum_{i=1}^{n} (Y_{i} - \hat{Y}_i) = \sum_{i=1}^{n} \bigg(Y_i - \sum_{j=1}^{K} \hat{\beta}_j X_{ij} \bigg)^2.
$$

The theorem now states that the OLS estimator is a BLUE. The main point behind the proof is tha the least-squares estimator is uncorrelated with every linear unbiased estimator of zero ie. a linear combination of the $Y_i$s whose coefficients do not depend upon the unobservable $\bm \beta$ but whose expected value is always zero.

\subsubsection{Proof of the Gauss-Markov theorem} TODO

\subsection{Models and Distribution Assumptions}

In this section, we'll treat two models for the paired data $\{(x_i, y_i)\}_{i=1}^n$ that are called simple linear regression models. Up until now, the least squares estimates were not obtained via statistical considerations but via a minimization problem. Therefore, we cannot make any statistical inference on the results. In order to do exactly that, we present two complete probabilistic models.

\paragraph{\textbf{Conditional Normal Model}} \\

The conditional normal model is the most common simple linear regression model and the most straightforward to analyze. The observed data are the $n$ pairs $\{(x_i, y_i)\}_{i=1}^n$. The values of the predictor variable, $\{x_i\}_{i=1}^n$ are assumed to be known, fixed constants, chosen by the experimenter. The values of the response variable $\{y_i\}_{i=1}^n$ are observed values of random variables, $\{{\bf Y}_i\}_{i=1}^n$, which are assumed to be independent. Furthermore, we assume the distribution of the ${\bf Y}_i$s to be normal, specifically

\begin{equation}
    {\bf Y}_i \sim \N(\alpha+\beta x_i, \sigma^2), \blank i = 1, \cdots, n.
\end{equation}

Thus the population regression function is a linear function of $x$, that is $\E ({\bf Y}_i\blank|\blank{\bf X}_i = x_i) = \alpha + \beta x_i, $ and all the ${\bf Y}_i$s have the same variance, $\sigma^2$. The (complete) conditional normal model is then

\begin{equation}
    {\bf Y} = \bm \alpha J_n + \bm \beta {\bf X} + \bm \epsilon,
\end{equation}

where $(J_n)_k = 1, \blank 1 \leq k \leq n$, where the errors ${\bm \epsilon}$ are assumed to be $\N(0,\sigma^2)$ random variables. 

\end{document}
