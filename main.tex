\documentclass{homework}
\author{Tomás Pérez}
\class{Mathematical Biostatistics Boot Camp 1}
\date{\today}
\title{Homework Assignments and Quizzes}

\graphicspath{{./media/}}

\begin{document} \maketitle

\section{Homework 1 - Week 1}

\begin{tcolorbox}[title=Question 1]
$P(B\cap A^C)$ is always equal to?
\end{tcolorbox}

Solution: Let $U$ denote the universe such that $A, B \subset U$, then ${P}(B) = P(B\cap A)+P(B\cap A^c)$. This is, the probability of B happening is the probability of both B and A happening plus the probability of B happening and not A. Rearranging, we get that $P(B \cap A^c) = P(B) - P(B \cap A)$.\\

\begin{tcolorbox}[title=Question 2]
When does $P(A \cup B) = P(A) + P(B)$? 
\end{tcolorbox}

Solution: Let $U$ denote the universe such that $A, B \subset U$. Then, as a direct result of Kolmogorov's principles, it's true that ${\displaystyle P(A\cup B)=P(A)+P(B)-P(A\cap B)}.$ The only case in which the probability of either event A happening or event B happening is the sum of the probabilities occurs only when said events are mutually exclusive \textit{ie.} $A\cap B = \emptyset$. Thus $P(A\cap B)=0$ since the empty set has null measure. \\

\begin{tcolorbox}[title=Question 3]
What is the probability of getting at least one head on three coin flips expressed as a percentage to one decimal place?
\end{tcolorbox}

A coin flip is a dichotomic experiment (ruled by a Bernoulli random variable) \textit{ie} having only two outcomes, head or tail. Since the coin flips, as experiments, are independent of each other, let A be the event of getting at least one head. Then $A^c$  is the probability of getting no heads (three tails) with an associated probability of $1/8$. The probability of $A$ is then $1 - \frac{1}{8} = \frac{7}{8} = 87.5$ \\

\begin{tcolorbox}[title=Question 4]
Suppose a random variable, $X$, follows a uniform distribution. That is, having a density that is a constant 1 between 0 and 1. What is the probability that $X$ is between .1 and .7 expressed as an integer percentage (i.e. no decimal places)?
\end{tcolorbox}

A uniformly distributed random variable has the following pdf

$$
f(x)= \left\{ \begin{array}{lcc}
             1 &  \textnormal{ if } x \in \mathds{R}_{[0,1])}  \\
             1 &  \textnormal{ if } x \notin \mathds{R}_{[0,1])}
             \end{array}
   \right.
$$

Thus, the probability of $X$ being in the (.1,.7)-interval is the area covered by the pdf in said interval. Namely, $P_{[.1,.7]} = \int_{.1}^{.7} dx = .6$. \\

\begin{tcolorbox}[title=Question 5]
Suppose a random variable follows a density $cx$ for$ 0\leq x \leq 1$. What is $c$?
\end{tcolorbox}

All probability density functions describing any random variable are to be normalized, thus $1=\int_{0}^{1} cxdx$ which implies $c=2$. \\

\begin{tcolorbox}[title=Question 6]
Suppose that a density is of the form $3x^2$ for 
$x$ between 0 and 1. What is the distribution function associated with this density?
\end{tcolorbox}

The cumulative distribution function is the integral of the random probability's pdf, thus: $F(x)=\int_{0}^{x} 3t^2 dt=x^3.$ \\

\begin{tcolorbox}[title=Question 7]
Suppose that the time in days until hospital discharge for a certain patient population follows a density $f(x) = \frac{1}{10}\exp(-x/10)$ for $x > 0$.  Calculate the probability that a person takes longer than 11 days to be discharged. Express it as an integer percentage with no digits after the decimal.
\end{tcolorbox}

The answer is straightforward, 

$$
100 \int_{11}^{\infty}\frac{1}{10}\exp\left(-\frac{x}{10}\right)dx =100\exp \left(-\frac{11}{10}\right)\approx 33.
$$ \\

\begin{tcolorbox}[title=Question 8]
Suppose $h$ is a real valued function such that $h \geq 0$ and $\int_{-\infty}^\infty h(x)dx > 0$ Then $c \times h$ is a valid density when $c$ is equal to?
\end{tcolorbox}

According to Kolmogorovian's axioms, all random variables' pdfs are to be normalized. Then the only valid answer is $ \left( \int_{-\infty}^\infty h(x)dx \right)^{-1}$. \\

\begin{tcolorbox}[title=Question 9]
Consider a health care worker without the flu. Suppose that they have a p=.01 probability of getting infected after an examination of an infected patient. His chance of getting infected after the $i^{th}$ interaction is assumed to be 

$$p(1-p)^{i-1}$$ 

for $i=1, 2, \ldots$. What is the probability of his getting infected after 3 or more interactions expressed as to the nearest integer percentage? (That is, no decimal places.)

\end{tcolorbox}

Solution: All pdfs are normalized to one, so let 
$X$ be the number of interactions until he is infected. Thus, $P(X \geq 3) + P(X < 3) = 1$. Rearranging we have $ P(X \geq 3) = 1 - P(X = 2) - P(X = 1) = 1 - .01 \times .99 - .01 \approx 98\%$. \\ 

\begin{tcolorbox}[title=Question 10]
Consider a geometric random variable, $X$ which has mass function

$$p(1-p)^{x-1}$$ 

for $x=1,2,\ldots$. (You can assume this sums to 1.) What is the probability $X > 5$? 
\end{tcolorbox}

Solution: We have that 

\begin{align*}
P(X>5)=P(X\geq6)=\sum_{x=6}^{\infty} p(1-p)^{x-1} 
= p(1-p)^{5} \sum_{x=6}^{\infty} p(1-p)^{x-5-1} \\= p(1-p)^{5} \sum_{x=1}^{\infty} p(1-p)^{x-1} =p(1-p)^{5}.
\end{align*} \\

\begin{tcolorbox}[title=Question 11]
Let $g(x) = \pi f_1(x) + (1 - \pi)f_2(x)$(x) where $f_1$ and $f_2$ are densities with means $\mu_1$ and $\mu_2$ and associated variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Here $0 \leq \pi \leq 1$. Note that $g$ is a valid density. What is $E[X^2]$ where $X$ is a random variable having density $g$?
\end{tcolorbox}

Solution: From it's very definition we have $Var(X)+E[X]^2=E[X^2]$ we get 

\begin{align*}
\int_{-\infty}^{\infty} dx x^2 f_{1}(x), \\
\int_{-\infty}^{\infty} dx x^2 f_{2}(x),
\end{align*}

Therefore 

\begin{align*}
    E[X^2]= \pi \int_{\mathds{R}} dx x^2f_{1}(x)+(1-\pi) \int_{\mathds{R}} dx x^2f_{2}(x) = \pi(\sigma_1^2+\mu_1^2) + (1-\pi)(\sigma_2^2+\mu_2^2)
\end{align*}. \\

\begin{tcolorbox}[title=Question 12]
Suppose that a density is of the form $(k+1)x^k$
for some constant $k > 1$ and $0 \leq x \leq 1$. What is $E[X^n]$ where $n$ is an integer and $X$ is a random variable from this density?
\end{tcolorbox}

We are to compute the following integral

\begin{align*}
    E[X^n]=\int_{0}^{1} dx (k+1)x^{k+n}= \frac{k+n+1}{k+1}.
\end{align*} \\


\begin{tcolorbox}[title=Question 13]
You are playing a game with a friend where you flip a coin and if it comes up heads you give her two dollars and if it comes up tails she gives you one dollar. You play the game ten times. What is the expected total earnings for you?
\end{tcolorbox}

Let $X_1,\cdots,X_{10}$ be random variables such that said random variables

\begin{itemize}
    \item take the value -2 with .5 probability
    \item and take the value 1 with .5 probability,
\end{itemize}

since the coin is assumed fair. Thus, in order to calculate the net expected earnings we compute the expected value of the sum of the ten random variables

\begin{align*}
    E\bigg[\sum_{j=1}^{10}X_j\bigg] =  \sum_{j=1}^{10} E\bigg[X_j\bigg] = 10 \times (-2 \times .5 + 1 \times .5) = -5.
\end{align*}

This means that this game is not favourable to play. \\

\begin{tcolorbox}[title=Question 14]
When at the free-throw line for two shots, a basketball player makes at least one free throw 90\% of the time. 80\% of the time, the player makes the first shot, while 70\% of the time she makes both shots. Does it appear that the player's second shot success is independent of the first?
\end{tcolorbox}

Let $A$ denote the event in which the player makes the first shot and let $B$ denote the event in which he makes the second shot. Then, we are given the following data:

\begin{itemize}
    \item $P(A \cup B) = .9$ - the probability of making at least one free throw -,
    \item $P(A) = .8$ - the probability of making the first shot -, 
    \item $P(A\cap B) = .7$ - the probability of making the second shot -,
\end{itemize}

Then, the probability of making the second shot is $P(B)= P(A \cup B) - P(A) + P(A \cap B) = .8$. Should the events A and B be independent, then $P(A\cap B) = P(A) \times P(B)$. This is not case so the events are not independent.\\

\begin{tcolorbox}[title=Question 15]
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. What is the variance of $X/\sigma + 10$?
\end{tcolorbox}

In general, if $X$ is a random variable with mean $\mu$ and variance $\sigma^2$ then $Var[aX+b]=a^2Var[X]$. Thus

\begin{align*}
    Var\left[\frac{X}{\sigma}+10\right] = \frac{1}{\sigma^2} Var[X] = 1. 
\end{align*}\\

\begin{tcolorbox}[title=Question 16]
Let $X_1,\ldots, X_{n_1}$ be random variables independent of $Y_1,\ldots, Y_{n_2}$ , where both groups are iid with associated population means $\mu_1$ and $\mu_2$ and population variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Let $\bar X$ and $\bar Y$ be their sample means. What is the mean of $\bar X - \bar Y$ ?
\end{tcolorbox}

The expectation value of a sum(difference) of random variables is linear on said random variables, noting that the sample mean of a random variable is in and of itself a random variable.  Thus $E[\bar X - \bar Y] = E[\bar X] - E[\bar Y] = \mu_1 - \mu_2$. \\

\begin{tcolorbox}[title=Question 17]
The Poisson mass function is given by $P(X = x) = \frac{e^{-\lambda} \lambda^x}{x!}$ for $x=0,1,2,3,\ldots$ and $\lambda >0$. What is $E[X(X-1)]$ where $X$ is a Poisson random variable?
\end{tcolorbox}

Since this is a discrete random variable we are to compute the following expression:

\begin{align*}
    E[X(X-1)]=\sum_{x=0}^{\infty} x(x-1) \frac{e^{-\lambda} \lambda^x}{x\!} = \sum_{x=2}^{\infty} \frac{e^{-\lambda} \lambda^x}{(x-2)\!} = \lambda^2  \sum_{x=2}^\infty \frac{e^{-\lambda} \lambda^{x-2}}{(x-2)!} = \lambda^2 \sum_{x=0}^\infty \frac{e^{-\lambda} \lambda^{x}}{(x)!} = \lambda^2.
\end{align*}

\begin{tcolorbox}[title=Question 18]
Let $f$ be a density such that $f(x) = f(-x)$. What is the associated mean of this density (assuming that it has a mean)?
\end{tcolorbox}

It has to be zero. Indeed, 

$$\int_{-\infty}^{\infty} dx xf(x)
= \int_{0}^{\infty} dx xf(x) + \int_{-\infty}^0 dx xf(x) = \int_{0}^\infty x f(x) dx - \int_{0}^\infty t f(t) dt = 0$$

\begin{tcolorbox}[title=Question 19]
For a Bernoulli coin flip with probability of  head $p$, what is the value of $p$ that yields the largest variance?
\end{tcolorbox}

The variance is $p(1-p)$. Taking a derivative yields $1-2p$, solving for 0 yields $p=.5$. The second derivative is $-2$ suggesting that the function is concave and that $p=.5$ is, in fact, the maximum.\\

\begin{tcolorbox}[title=Question 20]
Let $f$ be a mean $0$ variance $1$ density. Let $g(x) = f\{ (x - \mu) / \sigma \} / \sigma$. Argue to yourself that $g$ is a valid density. What is the variance associated with $g$
\end{tcolorbox}

We are to compute the following:

\begin{align*}
    \int_{-\infty}^{\infty} dx f\{ (x - \mu) / \sigma \} / \sigma = \int_{\mathds{R}} dz (z\sigma+\mu)f(z) \textnormal{ where } z=(x-\mu) / \sigma  \\
    = \sigma  \int_{\mathds{R}} dz z f(z) + \mu  \int_{\mathds{R}} dz f(z) = \mu.
\end{align*}

So the mean of $g$ is $\mu$. The calculation of the variance is straightforward:

\begin{align*}
    \int_{\mathds{R}} dx (x-\mu)^2f\{ (x - \mu) / \sigma \} = \int_{\mathds{R}} dz \sigma^2 z^2f(z) = \sigma^2. 
\end{align*}

\clearpage

\section{Quiz 1 - Week 1}

\begin{tcolorbox}[title=Question 1]
What is $P(A\cup B)$ always equal to?
\end{tcolorbox}

Using De Morgan's law we have that $P(A\cup B) = 1 - P(A^c \cap B^c).$\\

\begin{tcolorbox}[title=Question 2]
Which of the following are always true about $P(\cup_{i=1}^n E_i)$?
\end{tcolorbox}

N/A\\

\begin{tcolorbox}[title=Question 3]
Consider influenza epidemics for two parent heterosexual families. Suppose that the probability is 17\% that at least one of the parents has contracted the disease. The probability that the father has contracted influenza is 12\% while the probability that both the mother and father have contracted the disease is 6\%. What is the probability that the mother has contracted influenza?
\end{tcolorbox}

Let A denote the event in which the mother has contracted the disease and let B denote the event in which the father has contracted the disease, we are given the following information

\begin{itemize}
    \item The probability that at least one of them has contracted the disease is 17\%, $P (A\cup B) = 0.17$.
    \item The probability that the father has contracted the disease is 12\%, thus $P (A\cap B) = 0.12$
    \item and the probability that both of them have contracted is 6\%, $P(B)=0.06$
\end{itemize}

Since we know $P(A\cup B)=P(A)+P(B)-P(AnB)$, therefore $P(A)=P(A\cup B)-P(B)+P(A\cap B)=.11$ or 11\%. This calculation can be computed in \texttt{R}, with the following code snippet. 

\begin{lstlisting}[language=R]
# Quiz 1 - Ex. 3
P_AvB <- 0.17
P_B <- 0.12
P_AyB <- 0.06

P_A <- P_AvB - P_B + P_AyB
P_A
\end{lstlisting}

\begin{tcolorbox}[title=Question 4]
A random variable, $X$ is uniform, so that it's density is $f(x) = 1$ for $0\leq x \leq 1$. What is it's 75th percentile? Express your answer to two decimal places.
\end{tcolorbox}

Since the pdf is constant, the point at which the area below the pdf is 0.75 is also 0.75. In \texttt{R} we can compute it as follows:

\begin{lstlisting}[language=R]
# Quiz 1 - Ex. 3
# This density is a box of inferior lenght 1. The point so that the area below it is  
qunif(0.75)
\end{lstlisting}

\begin{tcolorbox}[title=Question 5]
A Pareto density is $\frac{1}{x^2}$
for $1 < x < \infty$. What is the distribution function associated with this density for $1 < x < \infty$?
\end{tcolorbox}

The cumulative distribution function of a given continuous random variable $X$, ruled by a probability density function $f_X(x)$ such that $\int_{\mathds{R}}dx f_X(x) = 1$, is:

$$
F_{X}(x)= P(X\geq x) = \int_{1}^{x} dt f_X(t) = 1 - \frac{1}{x}.
$$\\

\begin{tcolorbox}[title=Question 6]
What is the quantile $p$ from the density $e^{-x}(1 + e^{-x})^{-2}$?
\end{tcolorbox}

First, note this pdf has support over $\mathds{R}$. Thus we are to compute the following equation

\begin{align*}
    \int_{-\infty}^{x_p} dx \cdot e^{-x}(1+e^{-x})^{-2} = p \\
    \frac{1}{1+ e^{-x}}\bigg |_{-\infty}^{x_p} = \frac{e^{x_p}}{1+e^{x_p}} = p \longrightarrow e^{x_p}=p+pe^{x_p} \longrightarrow e^{x_p} (1-p) = p \longrightarrow \ln (e^{x_p} (1-p)) = \ln p \\ \longrightarrow x_p + \ln (1-p) = \ln p \longrightarrow x_p = \ln p - \ln (1-p) \Longrightarrow x_p = \ln \frac{p}{1-p}.
\end{align*} \\

\begin{tcolorbox}[title=Question 7]
Suppose that a density is of the form $cx^k$ for some constant $k > 1$ and $0 < x < 1$. What is the value of $c$?
\end{tcolorbox}

The calculation is straightforward.

\begin{align*}
    \int_{\mathds{R}_{[0,1]}}dx cx^{k} = 1 \longrightarrow c = k+1.
\end{align*} \\

\begin{tcolorbox}[title=Question 8]
Suppose that the time in days until hospital discharge for a certain patient population follows a density $f(x) = \frac{1}{2} \exp(-x/2)$ for $x > 0$.  What is the median discharge time in days?
\end{tcolorbox}

Let $X$ be a random variable which describes the hospital discharge time, in days, of a certain patient population. The pdf is given by 

$$
f_X(x)= \frac{1}{2} \exp(-x/2) \textnormal{ for } x > 0,
$$

which has support in $\mathds{R}_{+}$. Thus we are to find it's expected value

\begin{align*}
    E[X] = \int_{\mathds{R}_{+}} dx \cdot x f_{X}(x) =  \int_{\mathds{R}_{+}} dx \cdot x \cdot \frac{1}{2} \exp(-x/2) = 2. 
\end{align*} \\


\begin{tcolorbox}[title=Question 9]
Consider the density given by $2x \exp\left(-x^2 \right)$ for $x > 0$. What is the median?
\end{tcolorbox}

We are to find the value $\alpha_{\frac{1}{2}}$ such that 

\begin{align*}
    \int_{0}^{\alpha_{\frac{1}{2}}} dx \cdot 2x \exp\left(-x^2 \right) = \frac{1}{2}.
\end{align*}

Then

\begin{align*}
    \int_{0}^{\alpha_{\frac{1}{2}}} dx \cdot 2x \exp\left(-x^2 \right) = \mathrm{e}^{\alpha_{\frac{1}{2}}^2}\cdot\left(\mathrm{e}^{\alpha_{\frac{1}{2}}^2}-1\right) = \frac{1}{2} \\
    1 - e^{-\alpha_{\frac{1}{2}}^{2}} = \frac{1}{2} \\
    \frac{1}{2} = e^{-\alpha_{\frac{1}{2}}^{2}} = \frac{1}{2} \longrightarrow \alpha_{\frac{1}{2}} = \sqrt{\ln 2}
\end{align*} \\

\begin{tcolorbox}[title=Question 10]
Suppose $h(x)$ is such that $\infty > h(x) > 0$ for $x=1,2,\ldots, I$. Then $c\times h(x)$ is a valid pmf when $c$ is equal to what?
\end{tcolorbox}

Here we are dealing with a discrete random variable (reason for which a pmf is used rather than a pdf). It must be normalized to one, so $h(x)$ is a valid pmf if and only if $c= 
\left(\sum_{x=1}^I h(x) \right)^{-1}$. \\

\begin{tcolorbox}[title=Question 11]
Let $g(x) = \pi f_1(x) + (1 - \pi)f_2(x)$ where $f_1$ and $f_2$ are densities with means $\mu_1$ and $\mu_2$, respectively. Here $0 \leq \pi \leq 1$. Note that $g$ is a valid density. What is it's associated mean?
\end{tcolorbox}

The expected value of a linear combination of random variables is the linear combination of the expected values of said random variables \textit{ie.}

$$
E[aX+by] = a E[x]+bE[y].
$$

Thus, in our particular case, we have

\begin{align*}
    E[\pi f_1(x) + (1 - \pi)f_2(x)] = \pi E[f_1(x)] + (1 - \pi) E[f_2(x)] = \pi \mu_1 + (1-\pi) \mu_2. 
\end{align*}\\

\begin{tcolorbox}[title=Question 12]
Suppose that a density is of the form $(k + 1)x^k$
for some constant $k > 1$ and $0 \leq x \leq 1$. What is the mean associated with this density?
\end{tcolorbox}

Given a random variable $X$ with an associated pdf $(k+1)x^k$, with support in $\mathds{R}_{[0,1]}$, we are to compute the following integral

\begin{align*}
E[X] = \int_{\mathds{R}_{[0,1]}} dx \cdot x \cdot(k + 1)x^k = \frac{k+1}{k+2}.     
\end{align*}

This integral can be computed with Sympy, running the following code snippet 

\begin{lstlisting}[language=Python]
# Quiz 1 - Week 1 - Ex 12
## Suppose that a density is of the form $(k + 1)x^k$ for some constant $k > 1$ and $0 \leq x \leq 1$. 
## What is the mean associated with this density?

from sympy import *
from numpy import *
from math import *
from sympy.solvers import solve

x = Symbol('x')

print('The pdf is normalized', integrate((k+1)*x**k,(x,0,1)))
print('The expected value is', integrate(x*(k+1)*x**k, (x,0,1)))
\end{lstlisting}

\begin{tcolorbox}[title=Question 13]
You are playing a game with a friend where you flip a coin and if it comes up heads you give her $X$ dollars and if it comes up tails she gives you $Y$ dollars. The probability that the coin is heads in $p$ (some number between 0 and 1.)  What has to be true about $X$ and $Y$ to make so that both of your expected total earnings is 0. (The game would then be called "fair".)
\end{tcolorbox}

Here we are presented with a dichotomic experiment in which, in a given coin flip, if a coin comes up heads we have a net loss of $X$ dollars and if it comes up tails we have a net gain of $Y$ dollars. The coin flip is a random variable $\mathbf{X}$, a Bernoulli random variable, with $p$ being the probability of getting a head in a coin flip and $1-p$ being the probability of getting a tail.

Thus, in order for the net gain to be zero, the expected losses should be equal to the expected earnings where 

\begin{itemize}
    \item $pX$ is the expected loss, roughly the probability of the coin coming up a head times the loss of $X$ dollars
    \item whereas $(1-p)Y$ is the expected earnings, roughly the probability of the coin coming up a tail times the gain of $Y$ dollars.
\end{itemize}

Then, the balance equation can be written as

$$
pX = (1-p)Y \longrightarrow \frac{Y}{X} = \frac{p}{1-p}.
$$\\

\begin{tcolorbox}[title=Question 14]
You are playing a game with a friend where you flip a coin and if it comes up heads you give her 1 dollar and if it comes up tails she gives you one dollar.  If you play the game 10 times what would be the variance of your earnings?
\end{tcolorbox}

Here we have $X_{1},\cdots,X_{10}$ iid Bernoulli random variables, describing each coin flip; with the implicit assumption that the coin is fair. Their pmf take -1 and +1 as a value with .5 probability each. Let $X = \sum_{j=1}^{10} X_j$ be the net earnings. Since the coin is fair and according to the results obtained to Question 14, $E[X] =0$. Thus $Var[X_j] = .5(-1)^2+.5(1)^2=1$, then $Var[X]=10$ since the variance of the sum is the sum of the variances. \\

\begin{tcolorbox}[title=Question 15]
When at the free-throw line for two shots, a basketball player makes at least one free throw 90\% of the time. 80\% of the time, the player makes the first shot, while 70\% of the time she makes both shots. Which number is closest to the conditional probability that the player makes the second shot given that she missed the first?
\end{tcolorbox}

Let $A$ be the event in which the player makes the first shot and let $B$ denote the event in which said player makes the second shot. We are given the following information.

\begin{itemize}
    \item $P(A) = .8$, the probability for the player to make the first shot,
    \item $P(A \cup B) = .9$, the probability for the player to make at least one shot
    \item and $P(A \cap B) = .7$, the probability for the player to make both shots.
\end{itemize}

Then $P(B) = P(A \cup B) + P(A \cap B) - P(A) = .8$. We need $P(B|A^{c}) = \frac{P(B \cap A^{c})}{P(A^{c})} = \frac{P(B) - P(B \cap A)}{1-P(A)} = \frac{.8-.7}{.2}=.5$. \\

\begin{tcolorbox}[title=Question 16]
Let $X_1,\ldots, X_{n_1}$ be random variables independent of $Y_1,\ldots, Y_{n_2}$, where both groups are iid with associated population means $\mu_1$ and $\mu_2$ and population variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Let $\bar X$ and $\bar Y $ be their sample means. What is the variance of $\bar X - \bar Y$ ?
\end{tcolorbox}

The variance of the sum/difference of two random variables is the sum of said random variables' variances. Thus

$$
Var(\bar X - \bar Y) = Var(\bar X)+Var(\bar Y) = \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}.
$$ \\

\begin{tcolorbox}[title=Question 17]
Quality control experts estimate that the time (in years) until a specific electronic part from an assembly line fails follows (a specific instance of) the Pareto density, $f(x) = \frac{3}{x^4}$  for $1 < x < \infty$. Which option is closest to the mean failure time?
\end{tcolorbox}

This pdf has support over $\mathds{R}_{[1,\infty)}$. We calculate the following expression

\begin{align*}
    E[X] = \int_{[1,\infty)} dx \cdot \frac{3}{x^4} = \frac{3}{2}.
\end{align*}

This integral can be calculated using the Sympy package for Python, by running the following code snippet

\begin{lstlisting}[language=Python]
# Quiz 1 - Week 1 - Ex 17
##Quality control experts estimate that the time (in years) until a specific electronic part from an assembly line fails follows (a specific instance of) the Pareto density,
## $f(x) = \frac{3}{x^4}$  for $1 < x < \infty$. Which option is closest to the mean failure time?

from sympy import *
from numpy import *
from math import *

pos_infinity=math.inf
neg_infinity=-math.inf

x = Symbol('x')

print('The pdf is normalized since', integrate(3*x**-4, (x, 1, pos_infinity)))
print('The expected value is', integrate(3*x*x**-4, (x, 1, pos_infinity)))
\end{lstlisting} 

\begin{tcolorbox}[title=Question 18]
Let $f$ be a continuous density having a finite mean and $\mu$ be any number. Suppose that $f(x)= f(-x)$ (i.e. $f$ is symmetric about 0). Convince yourself that $f(x-\mu)$ is a valid density. What is its associated mean?
\end{tcolorbox}

Note that $g(x)=f(x-\mu)$ is symmetric about $\mu$. Thus

$$
E[X_f] = \int_{\mathds{R}} dx \cdot x f(x-\mu) =_{y= x-\mu}\int_{\mathds{R}} dy (y+\mu) f(y) = \cancel{\int_{\mathds{R}} dy y f(y)} + \mu = \mu
$$.

\begin{tcolorbox}[title=Question 19]
Suppose that $f$ be a mean 0 density having variance 1. What is the variance associated with the density $g(x) = f( x  / \sigma) / \sigma$
\end{tcolorbox}

The mean asociated with $g(x) = f(x/\sigma)/\sigma$ is zero (to complete). Then the variance is 

$$
\int_{\mathds{R}} dx \cdot x^2f( x  / \sigma) / \sigma =_{y = x/\sigma} \int_{\mathds{R}} dy \cdot (\sigma y)^2f( y) = \sigma^2 \int_{\mathds{R}} dy \cdot y^2 f(y) = \sigma^2.
$$

\begin{tcolorbox}[title=Question 20]
If $X$ and $Y$ are mean 0, variance 1 independent random variables, what is $E[X^2Y^2]$?
\end{tcolorbox}
Since they are independent random variables, the expected value of the product of random variables is the product of said random variables' expected values, thus $E[X^2Y^2]=1$.

\clearpage
\section{Homework 2 - Week 2}

\begin{tcolorbox}[title=Question 1]
A web site (www.medicine.ox.ac.uk/bandolier/band64/b64-7.html) for home pregnancy tests cites the following: "When the subjects using the test were women who collected and tested their own samples, the overall sensitivity was 75\%. Specificity was also low, in the range 52\% to 75\%." Suppose a subject has a negative test. Assume the lower bound for the specificity. What number is closest to the multiplier of the pre-test odds of pregnancy to obtain the post-test odds of pregnancy given a negative test result?
\end{tcolorbox}

We are given the following parameters,

\begin{itemize}
    \item Overall sensitivity of the test: .75. This is probability of a positive test, conditioned on truly being positive.
    \item Specificity of the test: .52. This is the probability of a negative test, conditioned on truly being negative.
\end{itemize}

The definitions for test sensitivity and test specificity are as follows

\begin{align*}
    {\displaystyle {\begin{aligned}{\text{sensitivity}}&={\frac {\text{number of true positives}}{{\text{number of true positives}}+{\text{number of false negatives}}}}\\[8pt]&={\frac {\text{number of true positives}}{\text{total number of sick individuals in population}}}\\[8pt]&={\text{probability of a positive test given that the patient has the disease}}\end{aligned}}}
\end{align*}
\begin{align*}
    {\displaystyle {\begin{aligned}{\text{specificity}}&={\frac {\text{number of true negatives}}{{\text{number of true negatives}}+{\text{number of false positives}}}}\\[8pt]&={\frac {\text{number of true negatives}}{\text{total number of well individuals in population}}}\\[8pt]&={\text{probability of a negative test given that the patient is well}}\end{aligned}}}
\end{align*}

A negative result in a test with high sensitivity is useful for \textbf{ruling out disease}. A test with 100\% sensitivity will recognize all patients with the disease by testing positive. \underline{A negative test result would} \underline{definitively rule out presence of the disease in a patient}. However, a positive result in a test with high sensitivity is not necessarily useful for ruling in disease. 

A positive result in a test with high specificity is useful for \textbf{ruling in disease}. The test rarely gives positive results in healthy patients. A positive result signifies a high probability of the presence of disease. \underline{A test with 100\% specificity} \underline{will recognize all patients without the disease by testing negative}, so a positive test result would definitely \textbf{rule in} the presence of the disease.

Here we have to calculate the DLR for a negative test result. The DLR for a negative and positive test result are given by

\begin{align*}
    {\displaystyle {\text{DLR}}+={\frac {\text{sensitivity}}{1-{\text{specificity}}}}} = {\displaystyle ={\frac {\Pr({T+}\mid D+)}{\Pr({T+}\mid D-)}}} \\ 
    {\displaystyle {\text{DLR}}-={\frac {1-{\text{sensitivity}}}{\text{specificity}}}} = {\displaystyle ={\frac {\Pr({T-}\mid D+)}{\Pr({T-}\mid D-)}}}
\end{align*}

Here $T+$ or $T-$ denote the result of the test being positive or negative and $D+$ or $D-$ denote the disease is present or absent, respectively. 

\begin{itemize}
    \item So "true positives" are those that test positive $T+$ and have the disease $D+$,
    \item and "false positives" are those that test positive $T+$ but do not have the disease $D-$.
\end{itemize}

Then, we calculate the $DLR_- = \frac{1-.75}{.52} \approx .481$.\\

\begin{tcolorbox}[title=Question 2]
Question 2
A web site (www.medicine.ox.ac.uk/bandolier/band64/b64-7.html) for home pregnancy tests cites the following: "When the subjects using the test were women who collected and tested their own samples, the overall sensitivity was 75\%. Specificity was also low, in the range 52\% to 75\%." Assume the lower value for the specificity. Suppose a subject has a negative test and that 30\% of women taking pregnancy tests are actually pregnant. What number is closest to the probability of pregnancy given a negative test?
\end{tcolorbox}

We are given the following information

\begin{itemize}
    \item Sensitivy: .75,
    \item Specificity between .52 and .75. We assume the lower bound.
    \item Only .30 of women taking pregnancy tests are actually pregnant. 
\end{itemize}

Thus, Let A be the event of positive test and let B the event of a woman being actually pregnant, then we have

\begin{itemize}
    \item $P(B) = .3$, the probability of the patient being pregnant. 
    \item Sensitivity: $.75 = P(A|B) = \frac{P(A\cap B)}{P(B)}$, the probability of a positive test given the patient is pregnant.
    \item Specificity: $.52 = P(A^c | B^c) = \frac{P(A^c\cap B^c)}{P(B^c)}$, the probability of a negative test given that the patient is not pregnant. 
\end{itemize}

Then, accordding to Bayes' theorem, 

\begin{align*}
    P(B|A) = \frac{P(A \cap B)}{P(B)} = \frac{P(A|B) P(A)}{P(B)P(A|B)+P(B^c)P(A|B^c)} \\
    = \frac{P(A|B)P(B)}{P(B)P(A|B)+P(B^c)(1-P(A^c|B^c))} = \frac{.75 \times .3}{.3 \times .75 + (1-.3)\times (1-.52)} \approx 0.401.
\end{align*}\\

\begin{tcolorbox}[title=Question 3]
Suppose that hospital infection counts are models as Poisson with mean $\mu$. Recall the Poisson mass function with mean $\mu$ is

$$ 
\frac{\mu^x e^{-\mu}}{x!} 
$$ 

for $x = 0,1,\ldots$. Three independent hospitals are observed for one year and their infection counts where 5, 4 and 6, respectively. What is the ML estimate for $\mu$?
\end{tcolorbox}

Let $X_1, X_2, X_3$ be the observed random variables, each one associated with a given hospital, which describe their infection counts. We are give $K=15$ data points. Then the likelihood is obtained as the product of three iid Poisson pdfs, 

\begin{align*}
    \prod_{j=1}^{3} \frac{\mu^x_j e^{-\mu}}{x_j!} \propto \prod_{j=1}^{3} {\mu^x_j e^{-\mu}} \propto \mu^{\sum_{j=1}^{K}x_j} e^{-3\mu} = \mu^{15}e^{-3\mu}. 
\end{align*}
In order to compute the ML estimate $\hat \mu = \arg \max_{\mu} L(\mu; x_k)$ it's convenient to calculate first (and maximize) the log-likelihood function, given $\log x$ is a monotone function on $x$. Then, the log-likelihood function is 

\begin{align*}
    \log x^{15}e^{-3\mu} = 15 \log \mu - 3 \mu
\end{align*}

Differentiating with respect to $\mu$ yields

\begin{align*}
    \frac{d}{d\mu} \log L(\mu;x_k)\bigg|_{\mu = \hat{\mu}} = 0 \\
    \frac{15}{\hat{\mu}} - 3 = 0 \longrightarrow \hat{\mu} = \frac{15}{3} = 5. 
\end{align*} \\

\begin{tcolorbox}[title=Question 4]
Let $X_1,\ldots, X_n$ be iid exponential$(\beta)$. That is, having density
$$\frac{1}{\beta} e^{-x/\beta} $$
for $x > 0$. What is the ML estimate for $\beta$?
\end{tcolorbox}

We are given $n$ iid exponential random variables, their pdfs having support over $\mathds{R}_{+}$. Thus, the likelihood function is

\begin{align*}
    L(\beta; x_1, \cdots, x_k) = \prod_{k=1}^{n} f_{X}(\beta; x_k) 
    = \prod_{k=1}^{n} \frac{1}{\beta} e^{-x/\beta} \\
    = \beta^{-n} \exp\left(-\sum_{k=1}^{n} x_k/\beta\right)  
\end{align*}

Thus, the log-likelihood function is 

\begin{align*}
    I(\beta, x_1, \cdot, x_k) = \log L(\beta; x_1, \cdots, x_k) \\
    = \log \bigg( \beta^{-n} \exp(-\sum_{k=1}^{n} x_k/\beta) \bigg)\\
    = \log\left(\beta^{-n}\right) + \log\left( \exp(-\sum_{k=1}^{n} x_k/\beta)\right) \\
    = -n \log(\beta) - \frac{1}{\beta} \sum_{k=1}^{n} x_k.
\end{align*}

Then, we can find the ML estimate:

\begin{align*}
    \hat{\beta} = \arg \max_{\beta} I(\beta, x_1, \cdots, x_k).
\end{align*}

Thus

\begin{align*}
    \frac{d}{d\beta} I(\beta, x_1, \cdot, x_k)\bigg|_{\beta= \hat{\beta}} = 0 \\
    = -\frac{n}{\hat{\beta}} + \frac{\sum_{k=1}^{n} x_k}{\hat{\beta}} \longrightarrow \hat{\beta} = \frac{\sum_{k=1}^{n} x_k}{n}.
\end{align*} \\

\begin{tcolorbox}[title=Question 5]
Let $X$ be a geometric random variable. That is $X$ counts the number of coin flips until one obtains the first head. The mass function is 

$$
 P(X = x) = p(1 - p)^{x-1}. 
$$

for $x=1,2,\ldots$. What is the maximum likelihood estimate for $p$ if one observes a geometric random variable?
\end{tcolorbox}

Note that said pmf has support over $\mathds{Z}_{+}$. Let $ {X}_{1}, {X}_{2}, {X}_{3}.....{X}_{n} $ be a random sample from the geometric distribution with p.d.f.

$$ f_{X}\left(x;p \right)=p{\left(1-p \right)}^{x-1}\textnormal{, } x \in \mathds{Z}_{+}$$

The likelihood function $L(p;x_1, \cdot, x_n)$ is given by:

$$ L(p;x_1, \cdot, x_n)=\prod_{k=1}^{n} f_{X_{k}}(p;x_k)={p}^{n}{\left(1-p \right)}^{\sum_{i=1}^{n}{x}_{i}-n} $$

Taking log,

$$ I(p, x_1, \cdot, x_n) = \log L(p;x_1, \cdot, x_n)= n\log{p}+\left(\sum_{i=1}^{n}{x}_{i}-n \right)\log{\left(1-p \right)} $$

Differentiating and equating to zero, we get,

$$ \frac{dI(p, x_1, \cdot, x_n)}{dp}\bigg|_{p=\hat{p}}=\frac{n}{\hat{p}} -\frac{\left(\sum_{i=1}^{n}{x}_{i}-n \right)}{\left(1-\hat{p} \right)}=0 $$

Therefore,

$$ \hat{p}=\frac{n}{\left(\sum_{i=1}^{n}{x}_{i} \right)} $$

So, the maximum likelihood estimator $\hat{p}$ is:

$$ \hat{p}=\frac{n}{\left(\sum_{1}^{n}{X}_{i} \right)}=\frac{1}{X} $$

This agrees with the intuition because, in n observations of a geometric random variable, there are $n$ successes in the $\sum_{i=1}^{n}{X}_{i} $ trials. Thus the estimate of p is the number of successes divided by the total number of trials. \\

\begin{tcolorbox}[title=Question 6]
Let $X$ be a Poisson count with mean $\mu$. Recall the Poisson mass function with mean $\mu$ is

$$ \frac{\mu^x e^{-\mu}}{x!}  $$

for $x = 0,1,\ldots$. What is the maximum likelihood estimate for $\mu$?
\end{tcolorbox}

\begin{tcolorbox}[title=Question 7]
Suppose that diastolic blood pressures (DBPs) for men aged 35-44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10. What is the probability that a random 35-44 year old has a  DBP greater than 100 (mm Hg)?
\end{tcolorbox}

\begin{tcolorbox}[title=Question 8]
Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. About what brain volume representes the 10th percentile?
\end{tcolorbox}

\begin{tcolorbox}[title=Question 9]
Return to the previous question. Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. Consider the sample mean of 144 random adult women from this population. Around what is the 10th percentile of the distribution of the distribution of sample means of 144 women?
\end{tcolorbox}

\begin{tcolorbox}[title=Question 10]
Question 10
Consider two sample means, $\bar X_1$ and $\bar X_2$ from a the same normal population having mean $\mu$ and standard deviation $\sigma$.  The first sample mean is based on $n_1 = 10^{k}$  observations while the second is based on $n_2 = 10^{k+2}n $ observations for some positive integer $k$. Take any percentile, say 100 $\times \alpha$, from the distribution of the means for each sample size excluding the median. What is the ratio of the distance of the two percentiles from the $\mu$ dividing distance 1 by distance 2?
\end{tcolorbox}

\begin{tcolorbox}[title=Question 11]
You flip a fair coin 6 times, which count of heads is most likely (0, 1, 2, 3, 4, 5, 6)?
\end{tcolorbox}

\begin{tcolorbox}[title=Question 12]
The respiratory disturbance index (RDI), a measure of sleep disturbance, for a specific population has a mean of 1515 (sleep events per hour) and a standard deviation of 1010. They are not normally distributed. Give your best estimate of the probability that a sample mean RDI of 100100 people is above 1717.
\end{tcolorbox}

\begin{tcolorbox}[title=Question 13]
Consider a standard uniform density. The mean for this density is .5 and the variance is 1 / 12. You sample 1,000 sample means where each sample mean is comprised of 100 observations. You take the mean of the 1,000 sample means. About what number would you expect it to be?
\end{tcolorbox}

According to the central limit theorem and given that the sample mean is unbiased, the mean of the thousand sample means is about 0.5.

\end{document}
