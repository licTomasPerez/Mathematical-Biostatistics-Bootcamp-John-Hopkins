\documentclass{homework}
\author{Tomás Pérez}
\class{Mathematical Biostatistics Boot Camp 1}
\date{\today}
\title{Homework Assignments and Quizzes}

\graphicspath{{./media/}}

\begin{document} \maketitle

\section{Homework 1 - Week 1}

\begin{tcolorbox}[title=Question 1]
$P(B\cap A^C)$ is always equal to?
\end{tcolorbox}

Solution: Let $U$ denote the universe such that $A, B \subset U$, then ${P}(B) = P(B\cap A)+P(B\cap A^c)$. This is, the probability of B happening is the probability of both B and A happening plus the probability of B happening and not A. Rearranging, we get that $P(B \cap A^c) = P(B) - P(B \cap A)$.\\

\begin{tcolorbox}[title=Question 2]
When does $P(A \cup B) = P(A) + P(B)$? 
\end{tcolorbox}

Solution: Let $U$ denote the universe such that $A, B \subset U$. Then, as a direct result of Kolmogorov's principles, it's true that ${\displaystyle P(A\cup B)=P(A)+P(B)-P(A\cap B)}.$ The only case in which the probability of either event A happening or event B happening is the sum of the probabilities occurs only when said events are mutually exclusive \textit{ie.} $A\cap B = \emptyset$. Thus $P(A\cap B)=0$ since the empty set has null measure. \\

\begin{tcolorbox}[title=Question 3]
What is the probability of getting at least one head on three coin flips expressed as a percentage to one decimal place?
\end{tcolorbox}

A coin flip is a dichotomic experiment (ruled by a Bernoulli random variable) \textit{ie} having only two outcomes, head or tail. Since the coin flips, as experiments, are independent of each other, let A be the event of getting at least one head. Then $A^c$  is the probability of getting no heads (three tails) with an associated probability of $1/8$. The probability of $A$ is then $1 - \frac{1}{8} = \frac{7}{8} = 87.5$ \\

\begin{tcolorbox}[title=Question 4]
Suppose a random variable, $X$, follows a uniform distribution. That is, having a density that is a constant 1 between 0 and 1. What is the probability that $X$ is between .1 and .7 expressed as an integer percentage (i.e. no decimal places)?
\end{tcolorbox}

A uniformly distributed random variable has the following pdf

$$
f(x)= \left\{ \begin{array}{lcc}
             1 &  \textnormal{ if } x \in \mathds{R}_{[0,1])}  \\
             1 &  \textnormal{ if } x \notin \mathds{R}_{[0,1])}
             \end{array}
   \right.
$$

Thus, the probability of $X$ being in the (.1,.7)-interval is the area covered by the pdf in said interval. Namely, $P_{[.1,.7]} = \int_{.1}^{.7} dx = .6$. \\

\begin{tcolorbox}[title=Question 5]
Suppose a random variable follows a density $cx$ for$ 0\leq x \leq 1$. What is $c$?
\end{tcolorbox}

All probability density functions describing any random variable are to be normalized, thus $1=\int_{0}^{1} cxdx$ which implies $c=2$. \\

\begin{tcolorbox}[title=Question 6]
Suppose that a density is of the form $3x^2$ for 
$x$ between 0 and 1. What is the distribution function associated with this density?
\end{tcolorbox}

The cumulative distribution function is the integral of the random probability's pdf, thus: $F(x)=\int_{0}^{x} 3t^2 dt=x^3.$ \\

\begin{tcolorbox}[title=Question 7]
Suppose that the time in days until hospital discharge for a certain patient population follows a density $f(x) = \frac{1}{10}\exp(-x/10)$ for $x > 0$.  Calculate the probability that a person takes longer than 11 days to be discharged. Express it as an integer percentage with no digits after the decimal.
\end{tcolorbox}

The answer is straightforward, 

$$
100 \int_{11}^{\infty}\frac{1}{10}\exp\left(-\frac{x}{10}\right)dx =100\exp \left(-\frac{11}{10}\right)\approx 33.
$$ \\

\begin{tcolorbox}[title=Question 8]
Suppose $h$ is a real valued function such that $h \geq 0$ and $\int_{-\infty}^\infty h(x)dx > 0$ Then $c \times h$ is a valid density when $c$ is equal to?
\end{tcolorbox}

According to Kolmogorovian's axioms, all random variables' pdfs are to be normalized. Then the only valid answer is $ \left( \int_{-\infty}^\infty h(x)dx \right)^{-1}$. \\

\begin{tcolorbox}[title=Question 9]
Consider a health care worker without the flu. Suppose that they have a p=.01 probability of getting infected after an examination of an infected patient. His chance of getting infected after the $i^{th}$ interaction is assumed to be 

$$p(1-p)^{i-1}$$ 

for $i=1, 2, \ldots$. What is the probability of his getting infected after 3 or more interactions expressed as to the nearest integer percentage? (That is, no decimal places.)

\end{tcolorbox}

Solution: All pdfs are normalized to one, so let 
$X$ be the number of interactions until he is infected. Thus, $P(X \geq 3) + P(X < 3) = 1$. Rearranging we have $ P(X \geq 3) = 1 - P(X = 2) - P(X = 1) = 1 - .01 \times .99 - .01 \approx 98\%$. \\ 

\begin{tcolorbox}[title=Question 10]
Consider a geometric random variable, $X$ which has mass function

$$p(1-p)^{x-1}$$ 

for $x=1,2,\ldots$. (You can assume this sums to 1.) What is the probability $X > 5$? 
\end{tcolorbox}

Solution: We have that 

\begin{align*}
P(X>5)=P(X\geq6)=\sum_{x=6}^{\infty} p(1-p)^{x-1} 
= p(1-p)^{5} \sum_{x=6}^{\infty} p(1-p)^{x-5-1} \\= p(1-p)^{5} \sum_{x=1}^{\infty} p(1-p)^{x-1} =p(1-p)^{5}.
\end{align*} \\

\begin{tcolorbox}[title=Question 11]
Let $g(x) = \pi f_1(x) + (1 - \pi)f_2(x)$(x) where $f_1$ and $f_2$ are densities with means $\mu_1$ and $\mu_2$ and associated variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Here $0 \leq \pi \leq 1$. Note that $g$ is a valid density. What is $E[X^2]$ where $X$ is a random variable having density $g$?
\end{tcolorbox}

Solution: From it's very definition we have $Var(X)+E[X]^2=E[X^2]$ we get 

\begin{align*}
\int_{-\infty}^{\infty} dx x^2 f_{1}(x), \\
\int_{-\infty}^{\infty} dx x^2 f_{2}(x),
\end{align*}

Therefore 

\begin{align*}
    E[X^2]= \pi \int_{\mathds{R}} dx x^2f_{1}(x)+(1-\pi) \int_{\mathds{R}} dx x^2f_{2}(x) = \pi(\sigma_1^2+\mu_1^2) + (1-\pi)(\sigma_2^2+\mu_2^2)
\end{align*}. \\

\begin{tcolorbox}[title=Question 12]
Suppose that a density is of the form $(k+1)x^k$
for some constant $k > 1$ and $0 \leq x \leq 1$. What is $E[X^n]$ where $n$ is an integer and $X$ is a random variable from this density?
\end{tcolorbox}

We are to compute the following integral

\begin{align*}
    E[X^n]=\int_{0}^{1} dx (k+1)x^{k+n}= \frac{k+n+1}{k+1}.
\end{align*} \\


\begin{tcolorbox}[title=Question 13]
You are playing a game with a friend where you flip a coin and if it comes up heads you give her two dollars and if it comes up tails she gives you one dollar. You play the game ten times. What is the expected total earnings for you?
\end{tcolorbox}

Let $X_1,\cdots,X_{10}$ be random variables such that said random variables

\begin{itemize}
    \item take the value -2 with .5 probability
    \item and take the value 1 with .5 probability,
\end{itemize}

since the coin is assumed fair. Thus, in order to calculate the net expected earnings we compute the expected value of the sum of the ten random variables

\begin{align*}
    E\bigg[\sum_{j=1}^{10}X_j\bigg] =  \sum_{j=1}^{10} E\bigg[X_j\bigg] = 10 \times (-2 \times .5 + 1 \times .5) = -5.
\end{align*}

This means that this game is not favourable to play. \\

\begin{tcolorbox}[title=Question 14]
When at the free-throw line for two shots, a basketball player makes at least one free throw 90\% of the time. 80\% of the time, the player makes the first shot, while 70\% of the time she makes both shots. Does it appear that the player's second shot success is independent of the first?
\end{tcolorbox}

Let $A$ denote the event in which the player makes the first shot and let $B$ denote the event in which he makes the second shot. Then, we are given the following data:

\begin{itemize}
    \item $P(A \cup B) = .9$ - the probability of making at least one free throw -,
    \item $P(A) = .8$ - the probability of making the first shot -, 
    \item $P(A\cap B) = .7$ - the probability of making the second shot -,
\end{itemize}

Then, the probability of making the second shot is $P(B)= P(A \cup B) - P(A) + P(A \cap B) = .8$. Should the events A and B be independent, then $P(A\cap B) = P(A) \times P(B)$. This is not case so the events are not independent.\\

\begin{tcolorbox}[title=Question 15]
Let $X$ be a random variable with mean $\mu$ and variance $\sigma^2$. What is the variance of $X/\sigma + 10$?
\end{tcolorbox}

In general, if $X$ is a random variable with mean $\mu$ and variance $\sigma^2$ then $Var[aX+b]=a^2Var[X]$. Thus

\begin{align*}
    Var\left[\frac{X}{\sigma}+10\right] = \frac{1}{\sigma^2} Var[X] = 1. 
\end{align*}\\

\begin{tcolorbox}[title=Question 16]
Let $X_1,\ldots, X_{n_1}$ be random variables independent of $Y_1,\ldots, Y_{n_2}$ , where both groups are iid with associated population means $\mu_1$ and $\mu_2$ and population variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Let $\bar X$ and $\bar Y$ be their sample means. What is the mean of $\bar X - \bar Y$ ?
\end{tcolorbox}

The expectation value of a sum(difference) of random variables is linear on said random variables, noting that the sample mean of a random variable is in and of itself a random variable.  Thus $E[\bar X - \bar Y] = E[\bar X] - E[\bar Y] = \mu_1 - \mu_2$. \\

\begin{tcolorbox}[title=Question 17]
The Poisson mass function is given by $P(X = x) = \frac{e^{-\lambda} \lambda^x}{x!}$ for $x=0,1,2,3,\ldots$ and $\lambda >0$. What is $E[X(X-1)]$ where $X$ is a Poisson random variable?
\end{tcolorbox}

Since this is a discrete random variable we are to compute the following expression:

\begin{align*}
    E[X(X-1)]=\sum_{x=0}^{\infty} x(x-1) \frac{e^{-\lambda} \lambda^x}{x\!} = \sum_{x=2}^{\infty} \frac{e^{-\lambda} \lambda^x}{(x-2)\!} = \lambda^2  \sum_{x=2}^\infty \frac{e^{-\lambda} \lambda^{x-2}}{(x-2)!} = \lambda^2 \sum_{x=0}^\infty \frac{e^{-\lambda} \lambda^{x}}{(x)!} = \lambda^2.
\end{align*}

\begin{tcolorbox}[title=Question 18]
Let $f$ be a density such that $f(x) = f(-x)$. What is the associated mean of this density (assuming that it has a mean)?
\end{tcolorbox}

It has to be zero. Indeed, 

$$\int_{-\infty}^{\infty} dx xf(x)
= \int_{0}^{\infty} dx xf(x) + \int_{-\infty}^0 dx xf(x) = \int_{0}^\infty x f(x) dx - \int_{0}^\infty t f(t) dt = 0$$

\begin{tcolorbox}[title=Question 19]
For a Bernoulli coin flip with probability of  head $p$, what is the value of $p$ that yields the largest variance?
\end{tcolorbox}

The variance is $p(1-p)$. Taking a derivative yields $1-2p$, solving for 0 yields $p=.5$. The second derivative is $-2$ suggesting that the function is concave and that $p=.5$ is, in fact, the maximum.\\

\begin{tcolorbox}[title=Question 20]
Let $f$ be a mean $0$ variance $1$ density. Let $g(x) = f\{ (x - \mu) / \sigma \} / \sigma$. Argue to yourself that $g$ is a valid density. What is the variance associated with $g$
\end{tcolorbox}

We are to compute the following:

\begin{align*}
    \int_{-\infty}^{\infty} dx f\{ (x - \mu) / \sigma \} / \sigma = \int_{\mathds{R}} dz (z\sigma+\mu)f(z) \textnormal{ where } z=(x-\mu) / \sigma  \\
    = \sigma  \int_{\mathds{R}} dz z f(z) + \mu  \int_{\mathds{R}} dz f(z) = \mu.
\end{align*}

So the mean of $g$ is $\mu$. The calculation of the variance is straightforward:

\begin{align*}
    \int_{\mathds{R}} dx (x-\mu)^2f\{ (x - \mu) / \sigma \} = \int_{\mathds{R}} dz \sigma^2 z^2f(z) = \sigma^2. 
\end{align*}

\clearpage

\section{Quiz 1 - Week 1}

\begin{tcolorbox}[title=Question 1]
What is $P(A\cup B)$ always equal to?
\end{tcolorbox}

Using De Morgan's law we have that $P(A\cup B) = 1 - P(A^c \cap B^c).$\\

\begin{tcolorbox}[title=Question 2]
Which of the following are always true about $P(\cup_{i=1}^n E_i)$?
\end{tcolorbox}

N/A\\

\begin{tcolorbox}[title=Question 3]
Consider influenza epidemics for two parent heterosexual families. Suppose that the probability is 17\% that at least one of the parents has contracted the disease. The probability that the father has contracted influenza is 12\% while the probability that both the mother and father have contracted the disease is 6\%. What is the probability that the mother has contracted influenza?
\end{tcolorbox}

Let A denote the event in which the mother has contracted the disease and let B denote the event in which the father has contracted the disease, we are given the following information

\begin{itemize}
    \item The probability that at least one of them has contracted the disease is 17\%, $P (A\cup B) = 0.17$.
    \item The probability that the father has contracted the disease is 12\%, thus $P (A\cap B) = 0.12$
    \item and the probability that both of them have contracted is 6\%, $P(B)=0.06$
\end{itemize}

Since we know $P(A\cup B)=P(A)+P(B)-P(AnB)$, therefore $P(A)=P(A\cup B)-P(B)+P(A\cap B)=.11$ or 11\%. This calculation can be computed in \texttt{R}, with the following code snippet. 

\begin{lstlisting}[language=R]
# Quiz 1 - Ex. 3
P_AvB <- 0.17
P_B <- 0.12
P_AyB <- 0.06

P_A <- P_AvB - P_B + P_AyB
P_A
\end{lstlisting}

\begin{tcolorbox}[title=Question 4]
A random variable, $X$ is uniform, so that it's density is $f(x) = 1$ for $0\leq x \leq 1$. What is it's 75th percentile? Express your answer to two decimal places.
\end{tcolorbox}

Since the pdf is constant, the point at which the area below the pdf is 0.75 is also 0.75. In \texttt{R} we can compute it as follows:

\begin{lstlisting}[language=R]
# Quiz 1 - Ex. 3
# This density is a box of inferior lenght 1. The point so that the area below it is  
qunif(0.75)
\end{lstlisting}

\begin{tcolorbox}[title=Question 5]
A Pareto density is $\frac{1}{x^2}$
for $1 < x < \infty$. What is the distribution function associated with this density for $1 < x < \infty$?
\end{tcolorbox}

The cumulative distribution function of a given continuous random variable $X$, ruled by a probability density function $f_X(x)$ such that $\int_{\mathds{R}}dx f_X(x) = 1$, is:

$$
F_{X}(x)= P(X\geq x) = \int_{1}^{x} dt f_X(t) = 1 - \frac{1}{x}.
$$\\

\begin{tcolorbox}[title=Question 6]
What is the quantile $p$ from the density $e^{-x}(1 + e^{-x})^{-2}$?
\end{tcolorbox}

First, note this pdf has support over $\mathds{R}$. Thus we are to compute the following equation

\begin{align*}
    \int_{-\infty}^{x_p} dx \cdot e^{-x}(1+e^{-x})^{-2} = p \\
    \frac{1}{1+ e^{-x}}\bigg |_{-\infty}^{x_p} = \frac{e^{x_p}}{1+e^{x_p}} = p \longrightarrow e^{x_p}=p+pe^{x_p} \longrightarrow e^{x_p} (1-p) = p \longrightarrow \ln (e^{x_p} (1-p)) = \ln p \\ \longrightarrow x_p + \ln (1-p) = \ln p \longrightarrow x_p = \ln p - \ln (1-p) \Longrightarrow x_p = \ln \frac{p}{1-p}.
\end{align*} \\

\begin{tcolorbox}[title=Question 7]
Suppose that a density is of the form $cx^k$ for some constant $k > 1$ and $0 < x < 1$. What is the value of $c$?
\end{tcolorbox}

The calculation is straightforward.

\begin{align*}
    \int_{\mathds{R}_{[0,1]}}dx cx^{k} = 1 \longrightarrow c = k+1.
\end{align*} \\

\begin{tcolorbox}[title=Question 8]
Suppose that the time in days until hospital discharge for a certain patient population follows a density $f(x) = \frac{1}{2} \exp(-x/2)$ for $x > 0$.  What is the median discharge time in days?
\end{tcolorbox}

Let $X$ be a random variable which describes the hospital discharge time, in days, of a certain patient population. The pdf is given by 

$$
f_X(x)= \frac{1}{2} \exp(-x/2) \textnormal{ for } x > 0,
$$

which has support in $\mathds{R}_{+}$. Thus we are to find it's expected value

\begin{align*}
    E[X] = \int_{\mathds{R}_{+}} dx \cdot x f_{X}(x) =  \int_{\mathds{R}_{+}} dx \cdot x \cdot \frac{1}{2} \exp(-x/2) = 2. 
\end{align*} \\


\begin{tcolorbox}[title=Question 9]
Consider the density given by $2x \exp\left(-x^2 \right)$ for $x > 0$. What is the median?
\end{tcolorbox}

We are to find the value $\alpha_{\frac{1}{2}}$ such that 

\begin{align*}
    \int_{0}^{\alpha_{\frac{1}{2}}} dx \cdot 2x \exp\left(-x^2 \right) = \frac{1}{2}.
\end{align*}

Then

\begin{align*}
    \int_{0}^{\alpha_{\frac{1}{2}}} dx \cdot 2x \exp\left(-x^2 \right) = \mathrm{e}^{\alpha_{\frac{1}{2}}^2}\cdot\left(\mathrm{e}^{\alpha_{\frac{1}{2}}^2}-1\right) = \frac{1}{2} \\
    1 - e^{-\alpha_{\frac{1}{2}}^{2}} = \frac{1}{2} \\
    \frac{1}{2} = e^{-\alpha_{\frac{1}{2}}^{2}} = \frac{1}{2} \longrightarrow \alpha_{\frac{1}{2}} = \sqrt{\ln 2}
\end{align*} \\

\begin{tcolorbox}[title=Question 10]
Suppose $h(x)$ is such that $\infty > h(x) > 0$ for $x=1,2,\ldots, I$. Then $c\times h(x)$ is a valid pmf when $c$ is equal to what?
\end{tcolorbox}

Here we are dealing with a discrete random variable (reason for which a pmf is used rather than a pdf). It must be normalized to one, so $h(x)$ is a valid pmf if and only if $c= 
\left(\sum_{x=1}^I h(x) \right)^{-1}$. \\

\begin{tcolorbox}[title=Question 11]
Let $g(x) = \pi f_1(x) + (1 - \pi)f_2(x)$ where $f_1$ and $f_2$ are densities with means $\mu_1$ and $\mu_2$, respectively. Here $0 \leq \pi \leq 1$. Note that $g$ is a valid density. What is it's associated mean?
\end{tcolorbox}

The expected value of a linear combination of random variables is the linear combination of the expected values of said random variables \textit{ie.}

$$
E[aX+by] = a E[x]+bE[y].
$$

Thus, in our particular case, we have

\begin{align*}
    E[\pi f_1(x) + (1 - \pi)f_2(x)] = \pi E[f_1(x)] + (1 - \pi) E[f_2(x)] = \pi \mu_1 + (1-\pi) \mu_2. 
\end{align*}\\

\begin{tcolorbox}[title=Question 12]
Suppose that a density is of the form $(k + 1)x^k$
for some constant $k > 1$ and $0 \leq x \leq 1$. What is the mean associated with this density?
\end{tcolorbox}

Given a random variable $X$ with an associated pdf $(k+1)x^k$, with support in $\mathds{R}_{[0,1]}$, we are to compute the following integral

\begin{align*}
E[X] = \int_{\mathds{R}_{[0,1]}} dx \cdot x \cdot(k + 1)x^k = \frac{k+1}{k+2}.     
\end{align*}

This integral can be computed with Sympy, running the following code snippet 

\begin{lstlisting}[language=Python]
# Quiz 1 - Week 1 - Ex 12
## Suppose that a density is of the form $(k + 1)x^k$ for some constant $k > 1$ and $0 \leq x \leq 1$. 
## What is the mean associated with this density?

from sympy import *
from numpy import *
from math import *
from sympy.solvers import solve

x = Symbol('x')

print('The pdf is normalized', integrate((k+1)*x**k,(x,0,1)))
print('The expected value is', integrate(x*(k+1)*x**k, (x,0,1)))
\end{lstlisting}

\begin{tcolorbox}[title=Question 13]
You are playing a game with a friend where you flip a coin and if it comes up heads you give her $X$ dollars and if it comes up tails she gives you $Y$ dollars. The probability that the coin is heads in $p$ (some number between 0 and 1.)  What has to be true about $X$ and $Y$ to make so that both of your expected total earnings is 0. (The game would then be called "fair".)
\end{tcolorbox}

Here we are presented with a dichotomic experiment in which, in a given coin flip, if a coin comes up heads we have a net loss of $X$ dollars and if it comes up tails we have a net gain of $Y$ dollars. The coin flip is a random variable $\mathbf{X}$, a Bernoulli random variable, with $p$ being the probability of getting a head in a coin flip and $1-p$ being the probability of getting a tail.

Thus, in order for the net gain to be zero, the expected losses should be equal to the expected earnings where 

\begin{itemize}
    \item $pX$ is the expected loss, roughly the probability of the coin coming up a head times the loss of $X$ dollars
    \item whereas $(1-p)Y$ is the expected earnings, roughly the probability of the coin coming up a tail times the gain of $Y$ dollars.
\end{itemize}

Then, let $G$ be the random variable describing the net gains, the balance equation can be written as

$$ E[G] = 0 \longrightarrow
pX = (1-p)Y \longrightarrow \frac{Y}{X} = \frac{p}{1-p}.
$$\\

\begin{tcolorbox}[title=Question 14]
You are playing a game with a friend where you flip a coin and if it comes up heads you give her 1 dollar and if it comes up tails she gives you one dollar.  If you play the game 10 times what would be the variance of your earnings?
\end{tcolorbox}

Here we have $X_{1},\cdots,X_{10}$ iid Bernoulli random variables, describing each coin flip; with the implicit assumption that the coin is fair. Their pmf take -1 and +1 as a value with .5 probability each. Let $X = \sum_{j=1}^{10} X_j$ be the net earnings. Since the coin is fair and according to the results obtained to Question 14, $E[X] =0$. Thus $Var[X_j] = .5(-1)^2+.5(1)^2=1$, then $Var[X]=10$ since the variance of the sum is the sum of the variances. \\

\begin{tcolorbox}[title=Question 15]
When at the free-throw line for two shots, a basketball player makes at least one free throw 90\% of the time. 80\% of the time, the player makes the first shot, while 70\% of the time she makes both shots. Which number is closest to the conditional probability that the player makes the second shot given that she missed the first?
\end{tcolorbox}

Let $A$ be the event in which the player makes the first shot and let $B$ denote the event in which said player makes the second shot. We are given the following information.

\begin{itemize}
    \item $P(A) = .8$, the probability for the player to make the first shot,
    \item $P(A \cup B) = .9$, the probability for the player to make at least one shot
    \item and $P(A \cap B) = .7$, the probability for the player to make both shots.
\end{itemize}

Then $P(B) = P(A \cup B) + P(A \cap B) - P(A) = .8$. We need $P(B|A^{c}) = \frac{P(B \cap A^{c})}{P(A^{c})} = \frac{P(B) - P(B \cap A)}{1-P(A)} = \frac{.8-.7}{.2}=.5$. \\

\begin{tcolorbox}[title=Question 16]
Let $X_1,\ldots, X_{n_1}$ be random variables independent of $Y_1,\ldots, Y_{n_2}$, where both groups are iid with associated population means $\mu_1$ and $\mu_2$ and population variances $\sigma_1^2$ and $\sigma_2^2$, respectively. Let $\bar X$ and $\bar Y $ be their sample means. What is the variance of $\bar X - \bar Y$ ?
\end{tcolorbox}

The variance of the sum/difference of two random variables is the sum of said random variables' variances. Thus

$$
Var(\bar X - \bar Y) = Var(\bar X)+Var(\bar Y) = \frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}.
$$ \\

\begin{tcolorbox}[title=Question 17]
Quality control experts estimate that the time (in years) until a specific electronic part from an assembly line fails follows (a specific instance of) the Pareto density, $f(x) = \frac{3}{x^4}$  for $1 < x < \infty$. Which option is closest to the mean failure time?
\end{tcolorbox}

This pdf has support over $\mathds{R}_{[1,\infty)}$. We calculate the following expression

\begin{align*}
    E[X] = \int_{[1,\infty)} dx \cdot \frac{3}{x^4} = \frac{3}{2}.
\end{align*}

This integral can be calculated using the Sympy package for Python, by running the following code snippet

\begin{lstlisting}[language=Python]
# Quiz 1 - Week 1 - Ex 17
##Quality control experts estimate that the time (in years) until a specific electronic part from an assembly line fails follows (a specific instance of) the Pareto density,
## $f(x) = \frac{3}{x^4}$  for $1 < x < \infty$. Which option is closest to the mean failure time?

from sympy import *
from numpy import *
from math import *

pos_infinity=math.inf
neg_infinity=-math.inf

x = Symbol('x')

print('The pdf is normalized since', integrate(3*x**-4, (x, 1, pos_infinity)))
print('The expected value is', integrate(3*x*x**-4, (x, 1, pos_infinity)))
\end{lstlisting} 

\begin{tcolorbox}[title=Question 18]
Let $f$ be a continuous density having a finite mean and $\mu$ be any number. Suppose that $f(x)= f(-x)$ (i.e. $f$ is symmetric about 0). Convince yourself that $f(x-\mu)$ is a valid density. What is its associated mean?
\end{tcolorbox}

Note that $g(x)=f(x-\mu)$ is symmetric about $\mu$. Thus

$$
E[X_f] = \int_{\mathds{R}} dx \cdot x f(x-\mu) =_{y= x-\mu}\int_{\mathds{R}} dy (y+\mu) f(y) = \cancel{\int_{\mathds{R}} dy y f(y)} + \mu = \mu
$$.

\begin{tcolorbox}[title=Question 19]
Suppose that $f$ be a mean 0 density having variance 1. What is the variance associated with the density $g(x) = f( x  / \sigma) / \sigma$
\end{tcolorbox}

The mean asociated with $g(x) = f(x/\sigma)/\sigma$ is zero (to complete). Then the variance is 

$$
\int_{\mathds{R}} dx \cdot x^2f( x  / \sigma) / \sigma =_{y = x/\sigma} \int_{\mathds{R}} dy \cdot (\sigma y)^2f( y) = \sigma^2 \int_{\mathds{R}} dy \cdot y^2 f(y) = \sigma^2.
$$

\begin{tcolorbox}[title=Question 20]
If $X$ and $Y$ are mean 0, variance 1 independent random variables, what is $E[X^2Y^2]$?
\end{tcolorbox}
Since they are independent random variables, the expected value of the product of random variables is the product of said random variables' expected values, thus $E[X^2Y^2]=1$.

\clearpage
\section{Homework 2 - Week 2}

\begin{tcolorbox}[title=Question 1]
A web site (www.medicine.ox.ac.uk/bandolier/band64/b64-7.html) for home pregnancy tests cites the following: "When the subjects using the test were women who collected and tested their own samples, the overall sensitivity was 75\%. Specificity was also low, in the range 52\% to 75\%." Suppose a subject has a negative test. Assume the lower bound for the specificity. What number is closest to the multiplier of the pre-test odds of pregnancy to obtain the post-test odds of pregnancy given a negative test result?
\end{tcolorbox}

We are given the following parameters,

\begin{itemize}
    \item Overall sensitivity of the test: .75. This is probability of a positive test, conditioned on truly being positive.
    \item Specificity of the test: .52. This is the probability of a negative test, conditioned on truly being negative.
\end{itemize}

The definitions for test sensitivity and test specificity are as follows

\begin{align*}
    {\displaystyle {\begin{aligned}{\text{sensitivity}}&={\frac {\text{number of true positives}}{{\text{number of true positives}}+{\text{number of false negatives}}}}\\[8pt]&={\frac {\text{number of true positives}}{\text{total number of sick individuals in population}}}\\[8pt]&={\text{probability of a positive test given that the patient has the disease}}\end{aligned}}}
\end{align*}
\begin{align*}
    {\displaystyle {\begin{aligned}{\text{specificity}}&={\frac {\text{number of true negatives}}{{\text{number of true negatives}}+{\text{number of false positives}}}}\\[8pt]&={\frac {\text{number of true negatives}}{\text{total number of well individuals in population}}}\\[8pt]&={\text{probability of a negative test given that the patient is well}}\end{aligned}}}
\end{align*}

A negative result in a test with high sensitivity is useful for \textbf{ruling out disease}. A test with 100\% sensitivity will recognize all patients with the disease by testing positive. \underline{A negative test result would} \underline{definitively rule out presence of the disease in a patient}. However, a positive result in a test with high sensitivity is not necessarily useful for ruling in disease. 

A positive result in a test with high specificity is useful for \textbf{ruling in disease}. The test rarely gives positive results in healthy patients. A positive result signifies a high probability of the presence of disease. \underline{A test with 100\% specificity} \underline{will recognize all patients without the disease by testing negative}, so a positive test result would definitely \textbf{rule in} the presence of the disease.

Here we have to calculate the DLR for a negative test result. The DLR for a negative and positive test result are given by

\begin{align*}
    {\displaystyle {\text{DLR}}+={\frac {\text{sensitivity}}{1-{\text{specificity}}}}} = {\displaystyle ={\frac {\Pr({T+}\mid D+)}{\Pr({T+}\mid D-)}}} \\ 
    {\displaystyle {\text{DLR}}-={\frac {1-{\text{sensitivity}}}{\text{specificity}}}} = {\displaystyle ={\frac {\Pr({T-}\mid D+)}{\Pr({T-}\mid D-)}}}
\end{align*}

Here $T+$ or $T-$ denote the result of the test being positive or negative and $D+$ or $D-$ denote the disease is present or absent, respectively. 

\begin{itemize}
    \item So "true positives" are those that test positive $T+$ and have the disease $D+$,
    \item and "false positives" are those that test positive $T+$ but do not have the disease $D-$.
\end{itemize}

Then, we calculate the $DLR_- = \frac{1-.75}{.52} \approx .481$.\\

\begin{tcolorbox}[title=Question 2]
Question 2
A web site (www.medicine.ox.ac.uk/bandolier/band64/b64-7.html) for home pregnancy tests cites the following: "When the subjects using the test were women who collected and tested their own samples, the overall sensitivity was 75\%. Specificity was also low, in the range 52\% to 75\%." Assume the lower value for the specificity. Suppose a subject has a negative test and that 30\% of women taking pregnancy tests are actually pregnant. What number is closest to the probability of pregnancy given a negative test?
\end{tcolorbox}

We are given the following information

\begin{itemize}
    \item Sensitivy: .75,
    \item Specificity between .52 and .75. We assume the lower bound.
    \item Only .30 of women taking pregnancy tests are actually pregnant. 
\end{itemize}

Thus, Let A be the event of positive test and let B the event of a woman being actually pregnant, then we have

\begin{itemize}
    \item $P(B) = .3$, the probability of the patient being pregnant. 
    \item Sensitivity: $.75 = P(A|B) = \frac{P(A\cap B)}{P(B)}$, the probability of a positive test given the patient is pregnant.
    \item Specificity: $.52 = P(A^c | B^c) = \frac{P(A^c\cap B^c)}{P(B^c)}$, the probability of a negative test given that the patient is not pregnant. 
\end{itemize}

Then, accordding to Bayes' theorem, 

\begin{align*}
    P(B|A) = \frac{P(A \cap B)}{P(B)} = \frac{P(A|B) P(A)}{P(B)P(A|B)+P(B^c)P(A|B^c)} \\
    = \frac{P(A|B)P(B)}{P(B)P(A|B)+P(B^c)(1-P(A^c|B^c))} = \frac{.75 \times .3}{.3 \times .75 + (1-.3)\times (1-.52)} \approx 0.401.
\end{align*}\\

\begin{tcolorbox}[title=Question 3]
Suppose that hospital infection counts are models as Poisson with mean $\mu$. Recall the Poisson mass function with mean $\mu$ is

$$ 
\frac{\mu^x e^{-\mu}}{x!} 
$$ 

for $x = 0,1,\ldots$. Three independent hospitals are observed for one year and their infection counts where 5, 4 and 6, respectively. What is the ML estimate for $\mu$?
\end{tcolorbox}

Let $X_1, X_2, X_3$ be the observed random variables, each one associated with a given hospital, which describe their infection counts. We are give $K=15$ data points. Then the likelihood is obtained as the product of three iid Poisson pdfs, 

\begin{align*}
    \prod_{j=1}^{3} \frac{\mu^x_j e^{-\mu}}{x_j!} \propto \prod_{j=1}^{3} {\mu^x_j e^{-\mu}} \propto \mu^{\sum_{j=1}^{K}x_j} e^{-3\mu} = \mu^{15}e^{-3\mu}. 
\end{align*}
In order to compute the ML estimate $\hat \mu = \arg \max_{\mu} L(\mu; x_k)$ it's convenient to calculate first (and maximize) the log-likelihood function, given $\log x$ is a monotone function on $x$. Then, the log-likelihood function is 

\begin{align*}
    \log x^{15}e^{-3\mu} = 15 \log \mu - 3 \mu
\end{align*}

Differentiating with respect to $\mu$ yields

\begin{align*}
    \frac{d}{d\mu} \log L(\mu;x_k)\bigg|_{\mu = \hat{\mu}} = 0 \\
    \frac{15}{\hat{\mu}} - 3 = 0 \longrightarrow \hat{\mu} = \frac{15}{3} = 5. 
\end{align*} \\

\begin{tcolorbox}[title=Question 4]
Let $X_1,\ldots, X_n$ be iid exponential$(\beta)$. That is, having density
$$\frac{1}{\beta} e^{-x/\beta} $$
for $x \mbox{\textgreater} 0$. What is the ML estimate for $\beta$?
\end{tcolorbox}

We are given $n$ iid exponential random variables, their pdfs having support over $\mathds{R}_{+}$. Thus, the likelihood function is

\begin{align*}
    L(\beta; x_1, \cdots, x_k) = \prod_{k=1}^{n} f_{X}(\beta; x_k) 
    = \prod_{k=1}^{n} \frac{1}{\beta} e^{-x/\beta} \\
    = \beta^{-n} \exp\left(-\sum_{k=1}^{n} x_k/\beta\right)  
\end{align*}

Thus, the log-likelihood function is 

\begin{align*}
    I(\beta, x_1, \cdot, x_k) = \log L(\beta; x_1, \cdots, x_k) \\
    = \log \bigg( \beta^{-n} \exp(-\sum_{k=1}^{n} x_k/\beta) \bigg)\\
    = \log\left(\beta^{-n}\right) + \log\left( \exp(-\sum_{k=1}^{n} x_k/\beta)\right) \\
    = -n \log(\beta) - \frac{1}{\beta} \sum_{k=1}^{n} x_k.
\end{align*}

Then, we can find the ML estimate:

\begin{align*}
    \hat{\beta} = \arg \max_{\beta} I(\beta, x_1, \cdots, x_k).
\end{align*}

Thus

\begin{align*}
    \frac{d}{d\beta} I(\beta, x_1, \cdot, x_k)\bigg|_{\beta= \hat{\beta}} = 0 \\
    = -\frac{n}{\hat{\beta}} + \frac{\sum_{k=1}^{n} x_k}{\hat{\beta}} \longrightarrow \hat{\beta} = \frac{\sum_{k=1}^{n} x_k}{n}.
\end{align*} \\

\begin{tcolorbox}[title=Question 5]
Let $X$ be a geometric random variable. That is $X$ counts the number of coin flips until one obtains the first head. The mass function is 

$$
 P(X = x) = p(1 - p)^{x-1}. 
$$

for $x=1,2,\ldots$. What is the maximum likelihood estimate for $p$ if one observes a geometric random variable?
\end{tcolorbox}

Note that said pmf has support over $\mathds{Z}_{+}$. Let $ {X}_{1}, {X}_{2}, {X}_{3}.....{X}_{n} $ be a random sample from the geometric distribution with p.d.f.

$$ f_{X}\left(x;p \right)=p{\left(1-p \right)}^{x-1}\textnormal{, } x \in \mathds{Z}_{+}$$

The likelihood function $L(p;x_1, \cdot, x_n)$ is given by:

$$ L(p;x_1, \cdot, x_n)=\prod_{k=1}^{n} f_{X_{k}}(p;x_k)={p}^{n}{\left(1-p \right)}^{\sum_{i=1}^{n}{x}_{i}-n} $$

Taking log,

$$ I(p, x_1, \cdot, x_n) = \log L(p;x_1, \cdot, x_n)= n\log{p}+\left(\sum_{i=1}^{n}{x}_{i}-n \right)\log{\left(1-p \right)} $$

Differentiating and equating to zero, we get,

$$ \frac{dI(p, x_1, \cdot, x_n)}{dp}\bigg|_{p=\hat{p}}=\frac{n}{\hat{p}} -\frac{\left(\sum_{i=1}^{n}{x}_{i}-n \right)}{\left(1-\hat{p} \right)}=0 $$

Therefore,

$$ \hat{p}=\frac{n}{\left(\sum_{i=1}^{n}{x}_{i} \right)} $$

So, the maximum likelihood estimator $\hat{p}$ is:

$$ \hat{p}=\frac{n}{\left(\sum_{1}^{n}{X}_{i} \right)}=\frac{1}{X} $$

This agrees with the intuition because, in n observations of a geometric random variable, there are $n$ successes in the $\sum_{i=1}^{n}{X}_{i} $ trials. Thus the estimate of $p$ is the number of successes divided by the total number of trials. \\

\begin{tcolorbox}[title=Question 6]
Let $X$ be a Poisson count with mean $\mu$. Recall the Poisson mass function with mean $\mu$ is

$$ \frac{\mu^x e^{-\mu}}{x!}  $$

for $x = 0,1,\ldots$. What is the maximum likelihood estimate for $\mu$?
\end{tcolorbox}

Note that $f_{X}(\mu;x)$, as a function, has support over $\mathds{Z}_{+}$. First and foremost, the likelihood function is 

\begin{align*}
    L(\mu;x_1,\cdots,x_n) = \prod_{j=1}^{n} f_{X_j}(\mu;x_j) = \prod_{j=1}^{n} \frac{\mu^x_j e^{-\mu}}{x_j!} 
\end{align*}

Then the log-likelihood function is 

\begin{dmath*}
    I(\mu;x_1,\cdots,x_n) = \log (L(\mu;x_1,\cdots,x_n) ) = \log \left(\prod_{j=1}^{n} \frac{\mu^x_j e^{-\mu}}{x_j!}\right) 
    = \sum_{j=1}^{n} \log \left(\frac{\mu^x_j e^{-\mu}}{x_j!} \right) \\
    = \sum_{j=1}^{n}  \left[\log \mu^x_j + \log e^{-\mu} - \log x_j! \right] \\
    = \sum_{j=1}^{n}  \left[\log \mu x_j - \mu - \log x_j!\right] \\
    = -n \mu + \log \mu \sum_{j=1}^{n} x_j - \sum_{j=1}^{n} \log x_j!.
\end{dmath*}

The maximum likelihood estimate $\hat{\mu}$ for $\mu$ is found as $\hat{\mu} = \arg \max_{\mu}  I(\mu;x_1,\cdots,x_n)$, then

\begin{dmath*}
    \frac{d}{d\mu}I(\mu;x_1,\cdots,x_n)\bigg|_{\mu\rightarrow\hat{\mu}}=0\\
    = -n + \frac{1}{\hat{\mu}} \sum_{j=1}^{n} x_j = 0.
\end{dmath*}

Thus $\hat{\mu} = \frac{1}{n} \sum_{j=1}^{n} x_j.  $ is the ML estimate for $\mu$. \\

\begin{tcolorbox}[title=Question 7]
Suppose that diastolic blood pressures (DBPs) for men aged 35-44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10. What is the probability that a random 35-44 year old has a  DBP greater than 100 (mm Hg)?
\end{tcolorbox}

Let $X$ be the random variable which describes the DBPs, such that $X \sim \mathcal{N}(80\textnormal{ mmHg},(10\textnormal{ mmHg})^2)$. We are interested in $P(x \mbox{\textgreater} 100 \textnormal{ mmHg}) = 1-P(x \mbox{\textless} 100 \textnormal{ mmHg})$, which can be computed both numerically and analytically. In R, the desired routine is
\begin{lstlisting}[language=R]
# Homework 2 - Week 2 - Ex 7
## Suppose that diastolic blood pressures (DBPs) for men aged 35-44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10. What is the probability that a random 35-44 year old has a  DBP greater than 100 (mm Hg)?

#One way to calculate this probability is 

targetDBP <- 100
mu <- 80
sigma <- 10
percentage <- (1-pnorm(targetDBP, mean = mu, sd = sigma)) * 100
percentage

[1] 2.275013%

# while a more succinct option is simply

pnorm(100, mean = 80, sd = 10, lower.tail = FALSE)

[1] 0.02275013
\end{lstlisting}

As previously stated, it can also be determined analytically. Let $Z=\frac{X-\mu}{\sigma} \sim \mathcal{N}(0,1)$ be the standard normal distribution, then 

\begin{align*}
P(x \mbox{\textgreater} 100 \textnormal{ mmHg}) = P\left(\frac{x-\mu}{\sigma} \mbox{\textgreater} \frac{100-\mu}{\sigma}\right) = P(z\mbox{\textgreater}2) = 1 - P(z\mbox{\textless}2).
\end{align*} 

This last probability, $P(z\mbox{\textless}2)$, can be found in tables and it's numerical value is $.97725$.

Thus $P(x \mbox{\textgreater} 100 \textnormal{ mmHg}) = 1-.97725 = 0.02275,$ which agrees with the numerical result obtained in R. \\

\begin{tcolorbox}[title=Question 8]
Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. About what brain volume representes the 10th percentile?
\end{tcolorbox}

Let $X$ be the random variable which describes the brain volume for adult women, such that $X\sim \mathcal{N}(1100 \textnormal{ cc}, (75 \textnormal{ cc})^2)$. We are interested in the 10th quantile, this is we want $x_{10}$ such that $F(x_{10}) = 10$ where 

$$
F(x) = \int_{-\infty}^{x}dt\cdot f_{X}(x)
$$

and where $f_{X}(x)$ is a Gaussian with mean $\mu =1100 \textnormal{ cc}$ and standard deviation $\sigma^2 = (75 \textnormal{ cc})^2)$. This can be computed numerically, with the following routine

\begin{lstlisting}[language=R]
# Homework 2 - Week 2 - Ex 8
## Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. About what brain volume representes the 10th percentile?
qtile <- .10
mu <- 1100
sigma <- 75

targetVolume <- qnorm(qtile, mean = mu, sd = sigma)
targetVolume

[1] 1003.884
\end{lstlisting}

Also, note that $z_{10} = -1.28 = \frac{x_{10} - \mu}{\sigma}$ represents the 10th percentile from the standard normal $Z =\frac{X-\mu}{\sigma}$. Thus the answer is $x_{10}=\mu +\sigma z_{10} = 1100 - 1.28 * 75$ which is around 1000 cc.  \\

\begin{tcolorbox}[title=Question 9]
Return to the previous question. Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. Consider the sample mean of 144 random adult women from this population. Around what is the 10th percentile of the distribution of the distribution of sample means of 144 women?
\end{tcolorbox}

Here we have a population sample of $144$ random adult women, \textit{ie}. $X_1, \cdots, X_{144}$ describe each women's brain volume. Then the population mean and the standard deaviation for the sample mean are, respectively,

$$
\mu_{X} = \mu \textnormal{ and } s_N = \frac{\sigma}{\sqrt{N}} = 
{\displaystyle {\sqrt {{\frac {1}{N}}\sum _{i=1}^{N}\left(x_{i}-{\bar {x}}\right)^{2}}},}
$$

where ${\displaystyle \{x_{1},\,x_{2},\,\ldots ,\,x_{N}\}}$ are the observed values of the sample items, and ${\displaystyle {\bar {x}}}$ is the mean value of these observations, while the denominator $N$ stands for the size of the sample: this is the square root of the sample variance, which is the average of the squared deviations about the sample mean. Note that $\hat{s}_N$ is a consistent estimator (it converges in probability to the population value as the number of samples goes to infinity), and is the maximum-likelihood estimate when the population is normally distributed.

Then $\mu_X = 1100 \textnormal{ cc}$ and $s_{144} = 6.25 \textnormal{ cc}$. Since $z_{10} = -1.28 = \frac{x_{10} - \mu}{s_N}$ represents the 10th percentile from the standard normal $Z =\frac{X-\mu}{s_N}$, the answer is $x_{10}^{N=144} = \mu + s_N z_{10} = 1100 - 1.28 \times 6.25 = 1092$. Numerically, this can be computed with an R routine

\begin{lstlisting}[language=R]
# Homework 2 - Week 2 - Ex 9
## Return to the previous question. Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. Consider the sample mean of 144 random adult women from this population. Around what is the 10th percentile of the distribution of the distribution of sample means of 144 women?

qtile <- .1
mu <- 1100
sigma <- 75
N <- 144
s_variance <- sigma/sqrt(N)
round(qnorm(qtile, mean = mu, sd = s_variance))
------
[1] 1092
\end{lstlisting}

\begin{tcolorbox}[title=Question 10]
Consider two sample means, $\bar X_1$ and $\bar X_2$ from a the same normal population having mean $\mu$ and standard deviation $\sigma$.  The first sample mean is based on $n_1 = 10^{k}$  observations while the second is based on $n_2 = 10^{k+2} $ observations for some positive integer $k$. Take any percentile, say 100 $\times \alpha$, from the distribution of the means for each sample size excluding the median. What is the ratio of the distance of the two percentiles from the $\mu$ dividing distance 1 by distance 2?
\end{tcolorbox}

A priori, we don't know the distribution of neither $\bar X_1$ nor $\bar X_2$ but according to the central limit theorem we can assume them to be normal distributions, provided $n_1 = 10^{k} \gg 1$ and  $n_2 = 10^{k+2} \gg 1$, this is $\bar X_1 \sim \mathcal{N}(\mu, \sigma^2)$ and $\bar X_2 \sim \mathcal{N}(\mu, \sigma^2)$. Then, let's consider the $100\times\alpha$th percentile. The percentile of the first distribution is $x_{1,\alpha} = \mu + z_{a}\frac{\sigma}{\sqrt{n_1}}$ whilst the percentile for the second distribution is $x_{2,\alpha} = \mu + z_{a}\frac{\sigma}{\sqrt{n_2}}$. Thus, the ratio of percentiles is proportional to $\frac{\sqrt{n_1}}{\sqrt{n_1}}=10$ \textit{ie.} every time you increase your sample size by a factor of one hundred, you shrink the percentiles of the distributions of the sample mean toward the population mean by a factor of ten. For example, if $n_1$ is much bigger than $n_2$ , the percentile for the mean based on $n_1$ observations will be much closer to $\mu$ than that of the mean based on $n_2$ observations.

\begin{tcolorbox}[title=Question 11]
You flip a fair coin 6 times, which count of heads is most likely (0, 1, 2, 3, 4, 5, 6)?
\end{tcolorbox}

A coin flip is a dichotomic Bernoulli random variable. Thus, the probability is $\binom{6}{k} 2^{-6} = 3$.

\begin{tcolorbox}[title=Question 12]
The respiratory disturbance index (RDI), a measure of sleep disturbance, for a specific population has a mean of 15 (sleep events per hour) and a standard deviation of 10. They are not normally distributed. Give your best estimate of the probability that a sample mean RDI of 100 people is above 17.
\end{tcolorbox}

The standard error of the mean is $10 / \sqrt{100}
= 1$. Thus $17$ is two standard deviations above the 15 for the distribution of the sample mean. Using the CLT, we guess this should be about 2.5\%.

\begin{tcolorbox}[title=Question 13]
Consider a standard uniform density. The mean for this density is .5 and the variance is 1 / 12. You sample 1,000 sample means where each sample mean is comprised of 100 observations. You take the mean of the 1,000 sample means. About what number would you expect it to be?
\end{tcolorbox}

According to the central limit theorem and given that the sample mean is unbiased, the mean of the thousand sample means is about 0.5.

\section{Quiz 2 - Week 2}

\begin{tcolorbox}[title=Question 1]
You meet a person at the bus stop and strike up a conversation. In the conversation, the person gives the strange answer that at least one of his two children is a girl. Thinking on this, you decide to do a probability calculation. Assuming only that genders are iid with 50\% probability each, what is the chance of a two child family having two girls given the information that at least one is a girl?
\end{tcolorbox}

This is the boy and girl paradox. In a nutshell, the answer is $\frac{1}{3}$.

\begin{tcolorbox}[title=Question 2]
A web site (www.medicine.ox.ac.uk/bandolier/band64/b64-7.html) for home pregnancy tests cites the following: "When the subjects using the test were women who collected and tested their own samples, the overall sensitivity was 75\%. Specificity was also low, in the range 52\% to 75\%." Suppose a subject has a positive test. Assume the lower bound for the specificity. What number is closest to the multiplier of the pre-test odds of pregnancy to obtain the post-test odds of pregnancy given a positive test result?
\end{tcolorbox}

The DLR for a positive test is 

$$
DLR_{+} = \frac{sensitivity}{1-specificity} = \frac{.75}{1-.52} = 1.5625. 
$$ \\

\begin{tcolorbox}[title=Question 3]
A web site (www.medicine.ox.ac.uk/bandolier/band64/b64-7.html) for home pregnancy tests cites the following: "When the subjects using the test were women who collected and tested their own samples, the overall sensitivity was 75\%. Specificity was also low, in the range 52\% to 75\%." Assume the lower value for the specificity. Suppose a subject has a positive test and that 30\% of women taking pregnancy tests are actually pregnant. What number is closest to the probability of pregnancy given the positive test?
\end{tcolorbox}

We are given the following information

\begin{itemize}
    \item Sensitivy: .75,
    \item Specificity between .52 and .75. We assume the lower bound.
    \item Only .30 of women taking pregnancy tests are actually pregnant. 
\end{itemize}

Thus, Let A be the event of positive test and let B the event of a woman being actually pregnant, then we have

\begin{itemize}
    \item $P(B) = .3$, the probability of the patient being pregnant. 
    \item Sensitivity: $.75 = P(A|B) = \frac{P(A\cap B)}{P(B)}$, the probability of a positive test given the patient is pregnant.
    \item Specificity: $.52 = P(A^c | B^c) = \frac{P(A^c\cap B^c)}{P(B^c)}$, the probability of a negative test given that the patient is not pregnant. 
\end{itemize}

Then, according to Bayes' theorem, 

\begin{align*}
    P(B|A) = \frac{P(A \cap B)}{P(B)} = \frac{P(A|B) P(A)}{P(B)P(A|B)+P(B^c)P(A|B^c)} \\
    = \frac{P(A|B)P(B)}{P(B)P(A|B)+P(B^c)(1-P(A^c|B^c))} = \frac{.75 \times .3}{.3 \times .75 + (1-.3)\times (1-.52)} \approx 0.401.
\end{align*}\\

Note that this result (the probability of pregnancy given a positive test given that 30\% of women taking the test are actually pregnant) is the same to that obtained in Homework 2 - Week 2 - Question 2 (where we find the probability of pregnancy given a negative test given that 30\% of women taking the test are actually pregnant).

\begin{tcolorbox}[title=Question 4]
Let $X_1\ldots X_K$ be independent Poisson counts with means $t_i \lambda$ for some known value $t_i$. Recall the Poison mass function with mean $\mu$ is

$$
 \frac{\mu^x e^{-\mu}}{x!}  
$$

for $x = 0,1,\ldots$. What is the maximum likelihood estimate for $\lambda$?
\end{tcolorbox}

Given $X_i \in \{X_{j}\}_{j=1}^{K}$, an independent Poisson count with mean $t_i \lambda$ for some known value $t_i$. Then the likelihood function is
 
\begin{dmath*}
    L(t_i; x_1\ldots x_K) = \prod_{j=1}^{K} f_{X_j}(t_j; x_j) = \prod_{j=1}^{K} \frac{(t_j \lambda)^{x_j}e^{-t_j \lambda}}{x_j!}.
\end{dmath*}

Then, the log-likelihood function is 

\begin{dmath*}
I(t_i; x_1\ldots x_K) = \log L (t_i; x_1\ldots x_K) = \log \prod_{j=1}^{K} \frac{(t_j \lambda)^{x_j}e^{-t_j \lambda}}{x_j!} = \sum_{j=1}^{K} \log \bigg(\frac{(t_j \lambda)^{x_j}e^{-t_j \lambda}}{x_j!}\bigg) = \sum_{j=1}^{K} x_j \log(t_j \lambda) -t_j \lambda - \log{x_j!} = \sum_{j=1}^{K} x_j (\log(t_j)+\log(\lambda)) - \lambda \sum_{j=1}^{K}t_j - \sum_{j=1}^{K}\log{x_j!}.
\end{dmath*}

Thus, the Maximum likelihood estimator $\hat{\lambda} = \arg \max_{\lambda} I(t_i; x_1\ldots x_K)$. Then, taking the derivative and evaluating it to zero we get

\begin{dmath*}
\frac{d}{d\lambda} I(t_i; x_1\ldots x_K)\bigg|_{\lambda=\hat{\lambda}} = \frac{1}{\hat{\lambda}} \sum_{j=1}^{K} x_j -  \sum_{j=1}^{K}t_j = 0
\end{dmath*}

Thus, 

\begin{align*}
\frac{1}{\hat{\lambda}} \sum_{j=1}^{K} x_j = \sum_{j=1}^{K}t_j \longrightarrow \hat{\lambda}=\frac{\sum_{j=1}^{K} x_j}{\sum_{j=1}^{K}t_j}. 
\end{align*}

\begin{tcolorbox}[title=Question 5]
Suppose that a person is flipping  a biased coin with success probability pp. She flips the coin 1010 times yielding 11 head. Consider two possibilities: 1) the person planned on flipping the coin ten times and got one head, 2) the person planned to flip the coin until the first head and it took ten times. What can you say about the likelihood in these two circumstances?
\end{tcolorbox}

The likelihood function is $p^1(1-p)^9$ in either case, should the coin be a geometric or a Bernoulli trial. As far as the likelihood is concerned, there are 10 trials and only one head. The intention of the flipper is irrelevant. \\

\begin{tcolorbox}[title=Question 6]
Let $X$ be a uniform random variable with support of length 1, but where we don't know where it starts. So that the density is

$ f(x) = 1$  

for $x \in (\theta, \theta + 1)$ and $0$ otherwise. We observe a random variable from this distribution, say $x_1$. What does the likelihood look like?
\end{tcolorbox}

The likelihood looks like a flat line since it's non-zero only for a compact interval, 

\begin{equation*}
    L(\theta,x_1,\cdots, x_n) = \Pi f(x_i; \theta) = \left\{ \begin{array}{lcc}
             1 &  \textnormal{ if } \forall i\in \mathds{N}, \theta \leq x_i \leq \theta+1  \\
             0 &  \textnormal{ otherwise } 
             \end{array}
   \right.
\end{equation*}

In terms of $\theta$, this can be written as
$ L(\theta,x_1,\cdots, x_n) = 1$ in $X_{max}-1\leq \theta \leq X_{min}$. Then, in said interval, it looks like a flat line. \\

\begin{tcolorbox}[title=Question 7]
Suppose that diastolic blood pressures (DBPs) for men aged 35-44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10. What is the probability that a random 35-44 year old has a  DBP less than 70?
\end{tcolorbox}

We are interested in calculating $P(X \geq 70)$ given that $X \sim \mathcal{N}(80 \textnormal{ mmHg}, (10\textnormal{ mmHg})^2)$. Then, analytically,

\begin{align*}
    P(X \leq 70) = P \bigg(\frac{X-\mu}{\sigma} \leq \frac{70-\mu}{\sigma}\bigg) = P\left(Z \leq -1 \right) = .16  
\end{align*}

Or numerically

\begin{lstlisting}[language=R]
## Quiz 2 - Week 2 - Exercise 7
##Suppose that diastolic blood pressures (DBPs) for men aged 35-44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10. What is the probability that a random 35-44 year old has a  DBP less than 70?

pnorm(70,mean=80,sd=10,lower.tail = TRUE)

[1] 0.1586553
\end{lstlisting}

\begin{tcolorbox}[title=Question 8]
Brain volume for adult women is normally distributed with a mean of about 1,100 cc for women with a standard deviation of 75 cc. About what brain volume represents the 95th percentile? 
\end{tcolorbox}

Let $X$ be the random variable which describes the brain volume for adult women, then 

\noindent $X \sim \mathcal{N}(1100 \textnormal{ cc}, (750\textnormal{ cc})^2)$. We need to calculate the quantile corresponding to a probability of 0.95. This can be computed numerically with the following routine

\begin{lstlisting}[language=R]
# Quiz 2 - Week 2 - Exercise 8
#Brain volume for adult women is normally distributed with a mean of about 1,100 cc for women with a standard deviation of 75 cc. About what brain volume represents the 95th percentile?

qnorm(0.95,mean=1100,sd=75,lower.tail = TRUE)

[1] 1223.364
\end{lstlisting}

\begin{tcolorbox}[title=Question 9]
Return to the previous question. Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. Consider the sample mean of 100 random adult women from this population. Around what is the 95th percentile of the distribution of that sample mean?
\end{tcolorbox}

According to the central limit theorem and since the sample size is large, we can consider the sample mean as a normal distribution $\mathcal{N}(\mu,\sigma^2/n)$, where $\mu$=1100, $\sigma$=75 and $n$=100. Then,

\begin{lstlisting}[language=R]
# Quiz 2 - Week 2 - Exercise 9
## Return to the previous question. Brain volume for adult women is about 1,100 cc for women with a standard deviation of 75 cc. Consider the sample mean of 100 random adult women from this population. Around what is the 95th percentile of the distribution of that sample mean?

qnorm(0.95,mean=1100,sd=75/10,lower.tail = TRUE)
[1] 1112.336
\end{lstlisting}

\begin{tcolorbox}[title=Question 10]
Recall from an earlier question. Suppose that diastolic blood pressures (DBPs) for men aged 35-44 are normally distributed with a mean of 80 (mm Hg) and a standard deviation of 10.  What's the probability that in a random sample of 5 subjects, 4 or more have DBPs more than 90?
\end{tcolorbox}



\begin{tcolorbox}[title=Question 11]
Consider two sample means, $\bar X_1$ and $\bar X_2$ from a the same normal population having mean zero and standard deviation $\sigma$.  The first sample mean is based on $n_1$ observations while the second is based on $n_2$ observations. Take any percentile, say $100 \times \alpha$,  for the distribution of sample means based on the two sample sizes excluding the median. What is the ratio of the two percentiles where it is the percentile of the first group divided by the second?
\end{tcolorbox}

\begin{tcolorbox}[title=Question 12]
You flip a fair coin 5 times, what's the probability of getting 4 or 5 heads?
\end{tcolorbox}

As the coin is fair, the probability of getting 1 head at each flip is 0.5. The probability of getting at least 4 heads after 5 flips can be computed using the binomial law 

$$
p=\mathcal{C}_{4}^{5}.(0.5)^4(1-0.5)+\mathcal{C}_{5}^{5}.(0.5)^5
$$.

In R, 

\begin{lstlisting}[language=R]
## Quiz 2 - Week 2 - Exercise 12

pbinom(3,size=5,prob=0.5, lower.tail = FALSE)
## [1] 0.1875
\end{lstlisting}

\begin{tcolorbox}[title=Question 13]
The respiratory disturbance index (RDI), a measure of sleep disturbance, for a specific population has a mean of 15 (sleep events per hour) and a standard deviation of 10. They are not normally distributed. Give your best estimate of the probability that a sample mean RDI of 100 people is between 14 and 16 events per hour?
\end{tcolorbox}

The standard error of the mean is $\frac{\sigma}{n}$, where $\sigma$=10 and $n$=100. So the value is 1. We then want to measure the probability of the RDI being inside 1 standard deviation around the mean \textit{ie.} should be around 68\%.

Indeed, the Central Limit Theorem (CLT) states that for a large enough sample size $n$, the distribution of the sample mean $\bar x$ will approach a normal distribution.
\begin{lstlisting}[language=R]
# Quiz 2 - Week 2 - Exercise 13.
# The respiratory disturbance index (RDI), a measure of sleep disturbance, for a specific population has a mean of 15 (sleep events per hour) and a standard deviation of 10. They are not normally distributed. Give your best estimate of the probability that a sample mean RDI of 100 people is between 14 and 16 events per hour?
mu <- 15
sigma <- 10
n <- 100
SE <- sigma/sqrt(n)

left <- 14
right <- 16

percentageLeft <- pnorm(left, mean = mu, sd = SE) * 100
percentageRight <- pnorm(right, mean = mu, sd = SE) * 100

probPercentage <- round(percentageRight - percentageLeft)
probPercentage

[1] 68
\end{lstlisting}

\begin{tcolorbox}[title=Question 14]
Consider a standard uniform density. The mean for this density is .5 and the variance is 1 / 12. You sample 1,000 observations from this distribution and take the sample mean, what value would you expect it to be near?
\end{tcolorbox}

Using the Law of Large Numbers, the value should be approximately 0.5.

\begin{tcolorbox}[title=Question 15]
Consider a standard uniform density. The mean for this density is .5 and the variance is 1 / 12. You sample 1,000 sample means where each sample mean is comprised of 100 observations. You take the standard deviation of the 1,000 sample means. About what number would you expect it to be?
\end{tcolorbox}

The standard deviation of the sample mean $\mu$
is $s_{X} = \frac{\sigma}{n}$, where $n$ is the number of observations per sample. Thus $s_X = \frac{1}{\sqrt{12 \times 100}}$.

\begin{tcolorbox}[title=Question 16]
Consider a standard uniform density. The mean for this density is .5 and the variance is 1 / 12. You sample 1,000 sample variances where each sample variance is comprised of 100 observations. You take the average of the 1,000 sample variances. What number would you expect that to be near?
\end{tcolorbox}

Yet again, according to the Law of Large Numbers, the answer is $\frac{1}{12}$.

\section{Homework 3 - Week 3}

\begin{tcolorbox}[title=Question 1]
Researchers are studying the relative concentration of blood lead for factory workers. They took the natural logarithm of ratio of blood lead concentration for 8 factory workers and 8 control subjects. The measurements resulted in a mean log concentration of 6 (log parts per volume) for the factory workers and 4 for the control subjects. The sample variance in the factory workers was 3 while it was 5 in the control group. Assuming equal variances, create a 95\% confidence interval for the difference in the population means of log blood lead concentration between factory workers and controls (Factory works - Controls).
\end{tcolorbox}

Let $X$ be the random variable describing the relative concentration of blood lead for the first group, the eight factory workers, and let $Y$ be the random variable describing the blood lead concentration for the control subjects. We assume $X \sim \mathcal{N}(\mu_X, \sigma^2)$ and $Y \sim \mathcal{N}(\mu_X, \sigma^2)$, assumption justified by the Central Limit Theorem. Let $\bar X, \bar Y, S_X, S_Y$ be their means and standard deviations, the pooled variance estimator can be found as

$$
S_p^2 = \frac{(n_X-1)S_x^2 + (n_Y-1)S_y^2 }{n_X+n_Y-2} = \pi S_X^2 + (1-\pi) S_Y^2 \textnormal{  where } \pi = \frac{n_x-1}{n_x+n_y-2}.
$$

and is a good (unbiased) estimator for the variance $\sigma^2$. It's a mixture of the group variances, placing greater weight on whichever has a larger sample size. Should the sample sizes be equal, the pooled variance estimate is the average of the group variances. 

We know the sum of two independent Chi-squared random variables is a Chi-squared random variable with its degrees of freedom equal to the sum of the degrees of freedom of the summands, \textit{ie.} 

\begin{dmath*}
(n_X+n_Y-2)\frac{S_p^2}{\sigma^2} = {n_X-1} \frac{S_X^2}{\sigma^2} +  (n_y-1) \frac{S_Y^2}{\sigma^2} =\chi^2_{n_X-1}+\chi^2_{n_Y-1} = \chi^2_{n_X-n_Y-2}
\end{dmath*}

The $t$-statistic is 

$$
\frac{\frac{\bar{Y}-\bar{X} - (\mu_Y-\mu_X) }{\sigma\left(\frac{1}{n_X}+\frac{1}{n_Y}\right)}}{\sqrt{\frac{(n_X+n_Y-2)S_p^2}{n_X+n_Y-2\sigma^2}}} = \frac{\bar{Y}-\bar{X}- (\mu_Y-\mu_X)}{S_p \left(\frac{1}{n_X}+\frac{1}{n_Y}\right)^{1/2}},
$$

a standard normal (since $\bar{Y}-\bar{X} \sim \mathcal{N}\left(\mu_{Y}-\mu_{X},\sigma^2 \left(\frac{1}{n_X}+\frac{1}{n_Y}\right)\right)$) divided by  the square root of an independent Chi-squared divided by its degrees of freedom. Thus, this statistic follows a Gosset's $t$ distribution with $n_X+n_Y-2$ degrees of freedom.

Therefore a $(1-\alpha)\times 100\%$-confidence interval for $\mu_Y-\mu_X$ can be written as follows

$$
\bar{Y}-\bar{X}\pm t_{n_{X}+n_{Y}-2,1-\frac{\alpha}{2}}S_p \sqrt{\frac{1}{n_X}+\frac{1}{n_Y}}.
$$

In our particular case, $\pi = \frac{1}{2}$ and $S_p = \sqrt{\frac{S_X^2}{n_x}+\frac{S_Y^2}{n_y}} = \sqrt{\frac{3}{2}+\frac{5}{2}} = 2.$ and
we are interested in a central 95\% confidence interval (\textit{ie.} $\alpha=2.5$), with $t_{14,.975} = 2.145$. Then, the confidence interval is 

$$
6-4 \pm 2.145 \times 2 \times \sqrt{\frac{1}{8}+\frac{1}{8}} = [-.145,4.145].
$$.

We can compute this in R with the following routine

\begin{lstlisting}[language=R]
# Homework 3 - Week 3 - Exercise 1
## Researchers are studying the relative concentration of blood lead for factory workers. They took the natural logarithm of ratio of blood lead concentration for 8 factory workers and 8 control subjects. The measurements resulted in a mean log concentration of 6 (log parts per volume) for the factory workers and 4 for the control subjects. The sample variance in the factory workers was 3 while it was 5 in the control group. Assuming equal variances, create a 95\% confidence interval for the difference in the population means of log blood lead concentration between factory workers and controls (Factory works - Controls).
barX <- 4
barY <- 6
nX <- 8
nY <- 8
SE1 <- 3
SE2 <- 5
inv <-sqrt((1/nX)+(1/nY))
Sp <- sqrt((nX-1)/(nX+nY-2)*SE1 + (1-(nX-1)/(nX+nY-2))*SE2)
cat('The pooled sample variance is', Sp)

qtileC95 <- qt(c(.975), df = nX+nY-2)
cat('\n The 95th t-quantile for a t-distribution with 14 dof is',qtileC95)

lowerBound <- barY - barX - qtileC95 * Sp * inv
higherBound <- barY - barX + qtileC95 * Sp * inv
cat('\n The 95th interval is [',lowerBound,higherBound,']')

In[1]: The pooled sample variance is 2
In [2]: The 95th t-quantile for a t-distribution with 14 dof is 2.144787
In [3]: The 95th interval is [ -0.1447867 4.144787 ]
[Execution complete with exit code 0]
\end{lstlisting}

\begin{tcolorbox}[title=Question 2]
Consider the previous question. Suppose that the interval was [-0.2, 3.0] log parts per volume. What would this say about factory workers versus controls?
\end{tcolorbox}

Since 0 is in the interval, at a 95\% confidence range, we can not rule out the possibility that the means are equal.

\begin{tcolorbox}[title=Question 3]
Let $S_1^2$, $S_2^2$ and $S_3^2$ be sample variances from random samples of size $n_1$, $n_2$ and $n_3$ respectively. The populations have means $\mu_1$, $\mu_2$ and $\mu_3$ respectively, and a common variance $\sigma^2$. When is $\pi_1 S_1^2  + \pi_2 S_2^2 + \pi_3 S_3^2$ unbiased?
\end{tcolorbox}

We compute the expected value of this weightened sum, $E[\pi_1 S_1^2  + \pi_2 S_2^2 + \pi_3 S_3^2]=\sigma^2(\pi_1+ \pi_2 + \pi_3).$ Note that they technically do not have to be positive for this estimate to be unbiased. However, if there is a negative one, the resulting variance estimate has a non-zero probability of being negative.\\

\begin{tcolorbox}[title=Question 4]
In an effort to improve efficiency, hospital administrators are evaluating a new triage system for their emergency room. In an validation study of the system, 5 randomly selected out of 10 patients were tracked in a mock ER the old triage system while the remaining ones were tracked under the new. The waiting times were recorded for all ten patients. Would it be better to use an independent group or paired T confidence interval in this setting?
\end{tcolorbox}

Since the groups were independent, it's preferable to use an independent group interval. \\

\begin{tcolorbox}[title=Question 5]
Suppose that you create a 95\% confidence interval. Later, you decide to create a 99\% interval. What can be said about the new interval with respect to the 95\% interval?
\end{tcolorbox}

The new interval will be narrower since $t_{df,.99}\mbox{\textgreater}t_{df,.95}$.

\begin{tcolorbox}[title=Question 6]
You have three data points {1, 3, 7}. What is the exact bootstrap distribution of the sample median? (Note - specifically - the bootstrap distribution of the median.)
\end{tcolorbox}

There are $3^3=27$ possible resampled data sets, 

\begin{itemize}
  \item $|111\rangle$ (Median = 1), $|113\rangle$ (Median = 1), $|117\rangle$ (Median = 1), 
  \item $|131\rangle$ (Median = 1), $|133\rangle$ (Median = 3), $|137\rangle$ (Median = 3),
  \item $|171\rangle$ (Median = 1), $|173\rangle$ (Median = 3), $|177\rangle$ (Median = 7), 
  \item $|311\rangle$ (Median = 1), $|313\rangle$ (Median = 3), $|317\rangle$ (Median = 3), 
  \item $|331\rangle$ (Median = 3), $|333\rangle$ (Median = 3), $|337\rangle$ (Median = 3),
  \item $|371\rangle$ (Median = 3), $|373\rangle$ (Median = 3), $|377\rangle$ (Median = 7),
  \item $|711\rangle$ (Median = 1), $|713\rangle$ (Median = 3), $|717\rangle$ (Median = 7),
  \item $|731\rangle$ (Median = 3), $|733\rangle$ (Median = 3), $|737\rangle$ (Median = 7),
  \item $|771\rangle$ (Median = 7), $|773\rangle$ (Median = 7), $|777\rangle$ (Median = 7). 
\end{itemize}

In terms of the medians, there are seven 1's, thirteen 3's and seven 7's and each row is equally likely. The R routine is 

\begin{lstlisting}[language=R]
# Homework 3 - Week 3 - Exercise 6
## You have three data points {1, 3, 7}. What is the exact bootstrap distribution of the sample median? (Note - specifically - the bootstrap distribution of the median.)
temp <- expand.grid(c(1, 3, 7), c(1, 3, 7), c(1, 3, 7)) 

table(apply(temp, 1, median))

In [1]: 
1  3  7 
7 13  7 
\end{lstlisting}

\begin{tcolorbox}[title=Question 7]
Suppose we simulated a large amount of random uniform numbers and a large amount of exponential(1) numbers. What would a plot of the quantiles of one versus the other look like? (Uniform on the horizontal axis and the exponential on the vertical axis.)
\end{tcolorbox}

Let $X$ be a uniform random variable then the $\alpha^{\textnormal{th}}$  quantile is $\alpha$. For an exponential it is $-\log(1 - \alpha)$. \\

\begin{tcolorbox}[title=Question 8]
Consider any distribution function F with associated mean 0 and variance 1. Notice that if $X$ is a random variable from FF, then $Y = \mu + \sigma X$ has mean $\mu$ and variance $\sigma^2$. What is the distribution function associated with $Y$? (Call it $G$.)
\end{tcolorbox}

\begin{align*}
    G(y) = P \left(Y \leq y\right) = P\left(\mu + \sigma X\leq y\right) = P\left(X \leq \frac{y-\mu}{\sigma}\right) = F\left(\frac{y-\mu}{\sigma}\right)
\end{align*} \\

\begin{tcolorbox}[title=Question 9]
Let $F$ and $G$ be distribution functions. If you do a quantile-quantile (QQ) plot with quantiles of $F$ on the horizontal axis and quantiles of $G$ on the vertical axis, what must it look like?
\end{tcolorbox}

We want the curve $\{F^{-1}(\alpha), G^{-1}(\alpha)\}$ for $0\leq \alpha \leq 1$.  This is the function $H(x) = G^{-1}\{F(x)\} $. \\

\begin{tcolorbox}[title=Question 10]
Consider the points in a QQ plot, going from left to right in the plot, can a point ever be lower than the previous point?
\end{tcolorbox} 

No, both the empirical and theoretical quantiles must be non-decreasing. 

\clearpage

\section{Quiz 3 - Week 3}

\begin{tcolorbox}[title=Question 1]
In a new population, a sample of 9 men yielded a sample average brain volume of 1,100cc and a standard deviation of 30cc. What is a 95\% Student's T confidence interval for the mean brain volume in this new population?
\end{tcolorbox}

Let $\{X_i\}_{i=1}^{n=9}$ be the iid Gaussian variables describing the men's brain volume, drawn from the $\mathcal{N}(\mu, \sigma^2)$ distribution with $\mu = 1100cc$ and $\sigma = 30cc$. Let 

$$
\bar X = \frac{1}{n}\sum_{i=1}^{n} X_i
$$
be the sample mean and let

$${\displaystyle S^{2}={\frac {1}{n-1}}\sum _{i=1}^{n}(X_{i}-{\bar {X}})^{2}}$$

be the (Bessel-corrected) sample variance. Then the random variable

\begin{align*}
{\frac {{\bar {X}}-\mu }{\sigma /{\sqrt {n}}}} \sim & {\mathcal{N}(0,1)}, & \frac{\bar X - \mu}{S / \sqrt{n}}\ \sim t_{n-1}    
\end{align*}

where $t_{n-1}$ is the Gosset's distribution for $n-1$ degrees of freedom. We are interested in a central 95\% confidence interval (ie. given Student's distribution's two tails, we are interested in the 97.5th percentile of said Student's distribution at the upper tail). For unknown mean and known standard deviation, a confidence interval for the population mean, based on a simple random sample of size $n$ is 

$$
CI = \mu \pm t_{\textnormal{0.975, } n-1}\frac{S}{\sqrt{n}}.
$$

In our case, $\mu = 1100cc$, $S=30cc$, $n=9$ dofs, then $t_{0.975, 8} = 2.306004$. Thus our confidence interval is 

$$
CI= [1076.94cc, 1123.06 cc].
$$

This CI can be computed with the following R routine

\begin{lstlisting}[language=R]
# Quiz 3 - Week 3 - Exercise 1
## In a new population, a sample of 9 men yielded a sample average brain volume of 1,100cc and a standard deviation of 30cc. What is a 95\% Student's T confidence interval for the mean brain volume in this new population?

mean <- 1100
sd <- 30
n <- 9

p <- .95 + (1-.95)/2
error <- qt(p, df=n-1)*sd/sqrt(n)
ci <- mean + c(-1,1)*error
ci

In [1]: 
1076.94 1123.06
\end{lstlisting}

\begin{tcolorbox}[title=Question 2]
A diet pill is given to 9 subjects over six weeks. The average difference in weight (follow up - baseline) is -2 pounds. What would the standard deviation have to be for the 95\% T confidence interval to lie entirely below 0?
\end{tcolorbox}

As previously stated, the Student's 95th CI is

$$
CI = \mu \pm t_{\textnormal{0.975, } n-1}\frac{S}{\sqrt{n}},
$$

which in our case is zero. So $-2 + t_{\textnormal{0.975, } 8}\frac{\sigma}{\sqrt{n}} = 0 \rightarrow \sigma = \frac{-(-2) \times \sqrt{n}}{t_{\textnormal{0.975, } 8}} = 2.601903$. This can be computed with the following R routine

\begin{lstlisting}[language=R]
# Quiz 3 - Week 3 - Exercise 2
## A diet pill is given to 9 subjects over six weeks. The average difference in weight (follow up - baseline) is -2 pounds. What would the standard deviation have to be for the 95\% T confidence interval to lie entirely below 0?

mu <- -2
n <- 9
sigma <- -mu*sqrt(n)/qt(0.975,df=n-1)
sigma

In [1] 
2.601903
\end{lstlisting}

\begin{tcolorbox}[title=Question 3]
Refer to the previous question. The interval would up being [-3.5, -0.5] pounds. What can be said about the population mean weight loss at 95\% confidence? 
\end{tcolorbox}

Since both the lower and higher bounds lie below zero, there is support at 95\% confidence of mean weight loss.\\

\begin{tcolorbox}[title=Question 4]
In an effort to improve efficiency, hospital administrators are evaluating a new triage system for their emergency room. In an validation study of the system, 5 patients were tracked in a mock ER under both the new and old triage system. Their waiting times were recorded. Would it be better to use an independent group or paired T confidence interval in this setting?
\end{tcolorbox}

N/A \\

\begin{tcolorbox}[title=Question 5]
Refer to the setting of the previous question. To further test the system, administrators selected 20 nights and randomly assigned the new triage system to be used on 10 nights and the standard system on the remaining 10 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 3 hours with a variance of 0.60 while the average MWT for the old system was 5 hours with a variance of 0.68. Give a 95\% confidence interval estimate for the differences of the mean MWT associated with the new system. Assume a constant variance.
\end{tcolorbox}

Let $X$ be the random variable describing the
nightly median waiting times using the old triage system and let $Y$ be the random variable the
nightly median waiting times using the new triage system. We assume $X \sim \mathcal{N}(\mu_X, \sigma^2)$ and $Y \sim \mathcal{N}(\mu_X, \sigma^2)$, assumptions justified by the Central Limit Theorem. Let $\bar X, \bar Y, S_X, S_Y$ be their means and standard deviations, the pooled variance estimator can be found as

$$
S_p^2 = \frac{(n_X-1)S_x^2 + (n_Y-1)S_y^2 }{n_X+n_Y-2} = \pi S_X^2 + (1-\pi) S_Y^2 \textnormal{  where } \pi = \frac{n_x-1}{n_x+n_y-2}.
$$

and is a good (unbiased) estimator for the variance $\sigma^2$. It's a mixture of the group variances, placing greater weight on whichever has a larger sample size. Should the sample sizes be equal, the pooled variance estimate is the average of the group variances. 

We know the sum of two independent Chi-squared random variables is a Chi-squared random variable with its degrees of freedom equal to the sum of the degrees of freedom of the summands, \textit{ie.} 

\begin{dmath*}
(n_X+n_Y-2)\frac{S_p^2}{\sigma^2} = {n_X-1} \frac{S_X^2}{\sigma^2} +  (n_y-1) \frac{S_Y^2}{\sigma^2} =\chi^2_{n_X-1}+\chi^2_{n_Y-1} = \chi^2_{n_X-n_Y-2}
\end{dmath*}

The $t$-statistic is 

$$
\frac{\frac{\bar{Y}-\bar{X} - (\mu_Y-\mu_X) }{\sigma\left(\frac{1}{n_X}+\frac{1}{n_Y}\right)}}{\sqrt{\frac{(n_X+n_Y-2)S_p^2}{n_X+n_Y-2\sigma^2}}} = \frac{\bar{Y}-\bar{X}- (\mu_Y-\mu_X)}{S_p \left(\frac{1}{n_X}+\frac{1}{n_Y}\right)^{1/2}},
$$

a standard normal (since $\bar{Y}-\bar{X} \sim \mathcal{N}\left(\mu_{Y}-\mu_{X},\sigma^2 \left(\frac{1}{n_X}+\frac{1}{n_Y}\right)\right)$) divided by  the square root of an independent Chi-squared divided by its degrees of freedom. Thus, this statistic follows a Gosset's $t$ distribution with $n_X+n_Y-2$ degrees of freedom.

Therefore a $(1-\alpha)\times 100\%$-confidence interval for $\mu_Y-\mu_X$ can be written as follows

$$
\bar{Y}-\bar{X}\pm t_{n_{X}+n_{Y}-2,1-\frac{\alpha}{2}}S_p \sqrt{\frac{1}{n_X}+\frac{1}{n_Y}}.
$$

In our case we have $\mu_X = 5 hs$, $\sigma^2_X = 0.68 hs^2$, $\mu_Y = 3hs$, $\sigma^2_Y = 0.60hs^2$, $t_{0.975, n_X + n_Y -2} = 2.100922$. Then the CI is $CI = [-2.75,-1.25].$ The previous calculations can be implemented with the following R routine 

\begin{lstlisting}[language=R]
# Quiz 3 - Week 3 - Exercise 5 
## Refer to the setting of the previous question. To further test the system, administrators selected 20 nights and randomly assigned the new triage system to be used on 10 nights and the standard system on the remaining 10 nights. They calculated the nightly median waiting time (MWT) to see a physician. The average MWT for the new system was 3 hours with a variance of 0.60 while the average MWT for the old system was 5 hours with a variance of 0.68. Give a 95\% confidence interval estimate for the differences of the mean MWT associated with the new system. Assume a constant variance.

old_mean <- 5 
old_var <- .68
old_n <- 10

new_mean <- 3
new_var <- .60
new_n <- 10

p <- .95

pooled_variance <- ((new_n-1)*new_var+(old_n-1)*old_var)/(old_n+new_n-2)
ci <- (new_mean - old_mean) + c(-1,1) * qt(p + (1-p)/2, (old_n+new_n-2)) * sqrt(pooled_variance) * sqrt(1/old_n+1/new_n)
ci 

In [1] 
-2.751649 -1.248351
\end{lstlisting}

\begin{tcolorbox}[title=Question 6]
Suppose that you create a 95\% T confidence interval. You then create a 90\% interval using the same data. What can be said about the 90\% interval with respect to the 95\% interval?
\end{tcolorbox}

The interval will be narrower since $t_{df, 95} > t_{df, 90}$. (To be confirmed) \\

\begin{tcolorbox}[title=Question 7]
Let distribution 1 be $\mathcal{N}(\mu_1, \sigma_1^2)$ and distribution 2 be $\mathcal{N}(\mu_2,\sigma_2^2)$. Let $x_{1,\alpha}$ and $x_{2,\alpha}$ be the $\alpha^{\textnormal{th}}$ quantile from the two distributions, respectively. How are the two mathematically related?
\end{tcolorbox}

Let $X$ be a random variable with a cumulative distribution function $F_{X}(x) = \int_{-\infty}^{x}dt f_{X}(t)$. Then the function $Q_{X}(\alpha):\mathds{R}_{[0,1]}\rightarrow\mathds{R}$, which is the inverse CDF, is the quantile function (QF) of $X$ \textit{ie.} the QF is the function that, for a given quantile $\alpha \in \mathds{R}_{[0,1]}$ returns the smallest $x$ for which $F_{X}(x)=\alpha$:

$$
Q_{X}(\alpha) = \min \{x \in \mathds{R} | F_{X}(x) = \alpha\}.
$$

In particular, let $X$ be a random variable following a normal distribution $X \sim \mathcal{N}(\mu, \sigma^2)$. Then its CDF is 

$$
F_{X}(x) = \Phi\left(\frac{x-\mu}{\sigma}\right) = \frac{1}{2}\left[1+\textnormal{erf}\left(\frac{x-\mu}{\sqrt{2}\sigma}\right)\right]
$$

where $\Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x} dt e^{-\frac{t^2}{2}}$ is the CDF of the standard normal distribution $\mathcal{N}(0,1)$ and where $\textnormal{erf}(x) = \frac{2}{\sqrt{\pi}} \int_{0}^{x}dt e^{-t^2}$ is the error function, which gives the probability of a random variable, with normal distribution of mean 0 and variance $\frac{1}{2}$, falling in the range $[-x,x]$. Given that the CDF is strictly monotonically increasing, the QF is equal to the inverse of the CDF:

$$
Q_{X}(\alpha) = F_{X}^{-1}(x).
$$

Thus, 

\begin{align*}
    \alpha =  \frac{1}{2}\left[1+\textnormal{erf}\left(\frac{x-\mu}{\sqrt{2}\sigma}\right)\right] \rightarrow 2\alpha - 1 = \textnormal{erf}\left(\frac{x-\mu}{\sqrt{2}\sigma}\right) \\
    \textnormal{erf}^{-1}(2\alpha-1) = \frac{x-\mu}{\sqrt{2}\sigma} \rightarrow x = \sqrt{2}\sigma \textnormal{erf}^{-1}(2\alpha-1) + \mu. 
\end{align*}

Now, let $X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)$ be two iid random variables and let $x_{1,\alpha}$ and $x_{2,\alpha}$ be the $\alpha^{\textnormal{th}}$ quantile from the two distributions, respectively. Then

\begin{align*}
    x_{1,\alpha} & = \sqrt{2}\sigma_1 \textnormal{erf}^{-1}(2\alpha-1) + \mu_1, &  x_{2,\alpha} & = \sqrt{2}\sigma_2 \textnormal{erf}^{-1}(2\alpha-1) + \mu_2.
\end{align*}

Consequently

\begin{equation*}
\begin{gathered}
    \frac{\sigma_2}{\sigma_1}x_{1,\alpha}  =  \frac{\sigma_2}{\sigma_1} \bigg[ \sqrt{2}\sigma_1 \textnormal{erf}^{-1}(2\alpha-1) + \mu_1 \bigg] \\
    \mu_2 + \frac{\sigma_2}{\sigma_1}x_{1,\alpha} = \mu_2 + \frac{\sigma_2}{\sigma_1} \bigg[ \sqrt{2}\sigma_1 \textnormal{erf}^{-1}(2\alpha-1) + \mu_1 \bigg] \\
    \mu_2 - \frac{\sigma_2}{\sigma_1} \mu_1 + \frac{\sigma_2}{\sigma_1}x_{1,\alpha} = \mu_2 - \frac{\sigma_2}{\sigma_1} \mu_1 + \frac{\sigma_2}{\sigma_1} \bigg[ \sqrt{2}\sigma_1 \textnormal{erf}^{-1}(2\alpha-1) + \mu_1 \bigg] \\
    = \mu_2 + \sqrt{2} \sigma_2 \cancel{ \frac{\sigma_1}{\sigma_1}} \textnormal{erf}^{-1}(2\alpha-1) + \cancel{ \frac{\sigma_2}{\sigma_1} \mu_1 - \frac{\sigma_2}{\sigma_1} \mu_1} = \sqrt{2} \sigma_2 \textnormal{erf}^{-1}(2\alpha-1) + \mu_2 = x_{2,\alpha}. 
\end{gathered}
\end{equation*}

The final result being\footnote{Relevant links: \href{https://statproofbook.github.io/D/qf}{Definition: Quantile function
} and
\href{https://statproofbook.github.io/P/norm-qf.html}{Proof: Quantile function of the normal distribution}. Retrieved on May 8th, 2022.} that the $\alpha^{\textnormal{th}}$ quantiles of two normal random variables are linearly related 

$$
x_{2,\alpha} = \mu_{2} - \frac{\sigma_2}{\sigma_1}\mu_1 + \frac{\sigma_2}{\sigma_1}x_{1,\alpha}.  
$$

\begin{tcolorbox}[title=Question 8]
Consider data points $x_1,\ldots, x_n$. Imagine a probability mass function, so that a random variable X from this distribution has
$$ 
P(X = x_i) = p(x_i) = \frac{1}{n}
$$
This is the so-called bootstrap distribution. What is the mean of the bootstrap distribution?
\end{tcolorbox}

The bootstrap distribution associated with an iid sample $\{X_{k}\}_{k=1}^{m}$ is defined as 

$$
\hat{F}_{m}(x) = \frac{1}{m} \sum_{j=1}^{m} \mathds{1}_{X_j \leq x} \textnormal{ such that } X_j \overset{\textnormal{iid}}{\sim} F(x).
$$

Consequently, the mean of the bootstrap distribution $\hat{F}_{m}(x)$ (conditional on the iid sample $\{X_{k}\}_{k=1}^{m}$) is 

$$
\mathds{E}_{\hat{F}_{m}(x)}[X] = \frac{1}{m} \sum_{j=1}^{m} X_j = \bar{X}_m.
$$

In other words, the mean of the bootstrap distribution is precisely the sample mean. In effect, let $\{z_{k}\}_{k=1}^{m} \in \mathds{R}$, then the arithmetic mean of said set of numbers is $\frac{1}{m}(z_{1} + \cdots + z_m)$. Let $\{Z_k\}_{k=1}^{m}$ be random variables, then $\forall \alpha_{j}, \alpha_{j} \in  \{\alpha_{k}\}_{k=1}^{m} \in \mathds{R}$ the expectation value is a linear operator. Now, let $B$ be a sample $(B_1, \cdots, B_k)$ obtained from a dataset $x=(x_{1}, \cdots, x_n)$ by taking $k$ elements uniformly from $x$ with replacement. Then $m(B)$ is the aritmetic mean of B, a random variable, thus 

$$
\mathds{E}(m(B)) = \mathds{E}\bigg(\frac{1}{k}(B_1 + \cdots + B_k)\bigg) = \frac{1}{k} (\mathds{E}(B_1)+\cdots+\mathds{E}(B_k)) $$

which follows by linearity of expectation. Since all the elements of $B$ are obtained in the same fashion, they all have the same expectation value $b$: $\mathds{E}(B_1) = \cdots = \mathds{E}(B_k) = b$ \textit{ie. } $\mathds{E}(m(B)) = \frac{1}{k}(kb) = \mathds{E}(B_1) = \bar{x}$, the arithmetic mean of the data. 

The previous argument states that if one uses the data mean $\bar x$ to estimate the population eman, then the bootstrap mean (which is the case $k=n$) also equals $\bar x$, and therefore is identical as an estimator of the population mean. 

\begin{tcolorbox}[title=Question 9]
Suppose we were to simulate a large number of standard normal random variables and a large number of exponential random variables. What would a plot of the exponential quantiles (horizontal axis) versus the standard normal quantiles (vertical axis) look like? Let $\Phi$ be the standard normal distribution function.
\end{tcolorbox}

Let $X \sim \textnormal{Exp}(1)$, then its CDF is 

$$
F_{\textnormal{Exp}(1)}(x)= \int_{\mathds{R}_{[0,x]}} dt  f_{X}(t) = \int_{\mathds{R}_{[0,x]}} dt e^{-t} = (1-e^{-x})\Theta(x).
$$

Now, let $Y \sim \mathds{N}(0,1)$ be a standard normal distribution. Then its quantile function are given by the error function, $\Phi^{-1}(p) = \sqrt{2}\textnormal{erf}^{-1}(2p-1)$ with $p \in \mathds{R}_{[0,1)}$. Then a plot of standard normal quantiles v. exponential quantiles will follow a $\Phi^{-1} \circ F_{\textnormal{Exp}(1)}(x) \propto \Phi^{-1}(1-e^{-x})$ distribution.

\footnote{Note that the quantile function for the exponential distribution is the inverse of the CDF, then 

\begin{equation*}
    \begin{gathered}
        p = F_{X}(x) \textnormal{ for p } \in \mathds{R}_{[0,1)} \\ 
        p = 1-e^{-x} \\
        x = -\ln(1-p).
    \end{gathered}
\end{equation*}}

\begin{tcolorbox}[title=Question 10]
Let $F(x)$ be a distribution function. Notice that $G(x) = F(a^2x + b)$ is also a distribution function for $a \neq 0$. If you were to take large samples from $F$ and $G$, what must the QQ plot look like without knowing the specific values of $a$ and $b$?
\end{tcolorbox}

The $G$ distribution is just a 1D-translated $F$ distribution \textit{ie.} the arguments are mapped as follows, $x \mapsto a^2x+b$. Then the QQ plot will be a line. 

\begin{tcolorbox}[title=Question 11]
Let your data be the two points $\{1, 3\}$.  What is the bootstrap distribution of the sample mean?
\end{tcolorbox}

Our chosen sample, taken from the data distribution $\{1, 3\}$, can be 

\begin{itemize}
    \item {1,1} with mean 1
    \item {1,3} or {3,1} with mean 2
    \item {3,3} with mean 3
\end{itemize}

Then the boostrap distribution for the sample mean is $\frac{1}{4}$ for mean 1, $\frac{1}{2}$ for mean 2 and $\frac{1}{4}$ for mean 3.

\end{document}
